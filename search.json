[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome! Here I will be documenting my journey through PIC 16B.\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/HW2 final/index.html",
    "href": "posts/HW2 final/index.html",
    "title": "Web Scraping",
    "section": "",
    "text": "Let’s start of today’s blog post by asking the following question: what movie or TV shows share actors with your favorite movie or show?\nHave you ever finished a great movie just to immediately look up every behind the scenes detail on the internet? I know that I definitely have. Often times, I find that if I enjoy one movie I am more likely to enjoy a second movie created by the same director or starring the same actor. Rather than going down a tedious “Google rabbit hole”, we can create a program that takes a movie and shows us which projects we can see the same actors in.\nWe will complete this project using the Python package called scrapy. This package allows us to extract data from a specific website and use it in our own way. We must start our project by importing all the necessary Python packages.\n\nimport scrapy\nimport pandas as pd\nimport numpy as py\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nScrapy allows us to webscrape and gather data from an online database. Pandas and Numpy enables us to create out own data frame with the information we extracted through webscraping and manipulate them. With Plotly, we can create visualizations using the elements of our dataframe. For our demo, we will use the 2020 movie “Emma.” starring Anya Joy Taylor. This personal favorite of mine stars very notable actors and actresses and is widely known as it is based off a book by Jane Austen. Many of these actors have overlapping projects with one another. Moreover, we can assume that someone who enjoys the movie “Emma.” will enjoy movies with a similar cast. This leads us to our question! The best way to answer our question and make some excellent movie recommendations will be to create a data frame and a visualization.\n\n\n\nWe created our scraper using two seperate Python files and the Scrapy package. The ‘settings.py’ file was autofilled with all of the necessary lines of code to enable our scraper. In this file, it is very important to change the user agent in order to avoid encountering errors while interacting with different websites.\nIn the ‘tmdb_scraper.py’ file we start by importing scrapy and defining our class. Within our class, we will have three functions. These functions will all work together to extract specific information from our online database.\nThe first function ‘def init’ is set. The only thing that changes in this function is the exact url for our specific movie.\nOur next function is ‘def parse’. In this function, we attach more elements to our original url so that it now links to the cast page from the movie overview page. We pass the result of this function to teh next function so that we may start extracting cast information from our website. Our first function is listed in the following code cell.\n\ndef parse(self, response):\n        cast_page_url = response.url + '/cast'\n        yield scrapy.Request(cast_page_url, callback=self.parse_full_credits)\n\nNext, we will define our ‘parse_full_credits’ function. We will use the call “response.css” along with specific source code from the website itself to identify each member of our movie’s cast. We will then use the url for each actor to pull information from their personal page and determine which movies each actor has worked in. The code for this function is written in the cell below.\n\ndef parse_full_credits(self, response):\n        actors = response.css(\"ol.people.credits\")[0].css(\"a::attr(href)\").extract() \n        for actor_url in actors:\n            yield scrapy.Request(response.urljoin(actor_url), callback=self.parse_actor_page)\n\nFinally, we will create one more function called ‘def parse_actor_page’. This function will actually pull the movie and TV show names from the list of credits for each actor. It is important that we select the correct part of the website to use in the ‘response.css’. After implementing the code from the cell below, we will be able to create our CSV file.\n\ndef parse_actor_page(self, response):\n        actor_name = response.css('a::text').extract()[35]\n        movie_or_TV_names = response.css('bdi::text').extract()\n        for movie_or_TV_name in movie_or_TV_names:\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name}\n\nNow that we have created all of our functions, we will now be able to create a CSV file that contains both actor names and movie/show names. Run the following line in the terminal: #### scrapy crawl tmdb_spider -o results3.csv -a subdir=556678-emma We will now have a CSV file in our ‘TMDB_scraper’ file. After completing all of the above steps, we can now use our extracted data to create a dataframe and a visualization.\n\n\n\nNow that we have gathered our data using our webscraper, we will now organzie it into a database. We will start by simporting our csv file that we filled using our scraper. Please ensure that this CSV file is saved in the same location as your Jupyter notebook.\n\n#creating our initial database using out CSV file\nfilename = \"results.csv\"\ndf = pd.read_csv(filename)\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nAnya Taylor-Joy\nThe Gorge\n\n\n1\nSuzie Toase\nEmma.\n\n\n2\nSuzie Toase\nNational Theatre Live: Present Laughter\n\n\n3\nSuzie Toase\nNational Theatre Live: One Man, Two Guvnors\n\n\n4\nSuzie Toase\nHarry Potter and the Deathly Hallows: Part 2\n\n\n...\n...\n...\n\n\n923\nHannah Stokely\nThe Falling\n\n\n924\nHannah Stokely\nSkyfall\n\n\n925\nHannah Stokely\nVexed\n\n\n926\nHannah Stokely\nThe Duchess\n\n\n927\nHannah Stokely\nChromophobia\n\n\n\n\n928 rows × 2 columns\n\n\n\nAfter printing our dataframe, we can see that it looks extremely similar to the table in our csv file. This is great! However, in order to find movies that share the same actors, we must know which of the films in our “movie_or_TV_name” column have multiple actors from “Emma.”. We can do this by creating a sorted list and adding a new column to our dataframe. We can implement this using the code below.\n\n#identifies the movies that have more that stars more than one of our 50 actors in \"Emma.\":\nactor_counts_per_movie = df.groupby('movie_or_TV_name')['actor'].nunique()\nsorted_actor_counts = actor_counts_per_movie.sort_values(ascending=False)\n\n#here we will make our above list a column within our dataframe: \ndf['actor_counts_per_movie'] = df['movie_or_TV_name'].map(actor_counts_per_movie)\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nactor_counts_per_movie\n\n\n\n\n0\nAnya Taylor-Joy\nThe Gorge\n1\n\n\n1\nSuzie Toase\nEmma.\n49\n\n\n2\nSuzie Toase\nNational Theatre Live: Present Laughter\n1\n\n\n3\nSuzie Toase\nNational Theatre Live: One Man, Two Guvnors\n2\n\n\n4\nSuzie Toase\nHarry Potter and the Deathly Hallows: Part 2\n1\n\n\n...\n...\n...\n...\n\n\n923\nHannah Stokely\nThe Falling\n1\n\n\n924\nHannah Stokely\nSkyfall\n1\n\n\n925\nHannah Stokely\nVexed\n1\n\n\n926\nHannah Stokely\nThe Duchess\n1\n\n\n927\nHannah Stokely\nChromophobia\n1\n\n\n\n\n928 rows × 3 columns\n\n\n\nNow we can better use our data to make recommendations. After creating our second column, let’s test if everything worked. After doing some exploring the Movie Database, I noticed that three members of the “Emma.” cast also had roles in the hit show “Peaky Blinders”. However, our database has thousands of rows of movies. We will use the following cell of code to ensure that our database was created correctly.\n\n#we will pick a random movie that we know multiple actors have starred in \n#for this demo, we will use \"Peaky Blinders\"\nspecific_movie = \"Peaky Blinders\"\n#we will run the line to determine the number of actors using a specific movie title\nactor_count = sorted_actor_counts.get(specific_movie)\n\n#we will now use loops to determine the number of actors who are in \"Emma.\" and \"Peaky Blinders\"\nif actor_count is not None:\n    print(f\"The number of actors in '{specific_movie}' is {actor_count}.\")\nelse:\n    print(\"Error!\")\n\nThe number of actors in 'Peaky Blinders' is 3.\n\n\nOur test worked! Therefore, we can confirm that we have successfully created a dataframe that contains actor names, movies and shows for each actor, and the number of actors from our list that are in each project.\nWe need to make a few more adjustments to our dataset before we can create a readable graph. The first step is to our sort our dataframe. In order to make my dataset and my graph easier to read, I am going to put the rows in descending order by starting with the movies that share the most actors and ending with the ones that share the least.\n\n#creating our sorted dataframe:\nsorted_df = df.sort_values(by='actor_counts_per_movie', ascending=False)\n\n#as every actor in this film is in \"Emma.\", this movie should be in the first few rows\n#display our updated dataframe:\nsorted_df\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nactor_counts_per_movie\n\n\n\n\n771\nJohnny Flynn\nEmma.\n49\n\n\n249\nEdmund George Taylor\nEmma.\n49\n\n\n261\nAlastair Postlethwaite\nEmma.\n49\n\n\n228\nAngus Imrie\nEmma.\n49\n\n\n401\nAmber Anderson\nEmma.\n49\n\n\n...\n...\n...\n...\n\n\n386\nTanya Reynolds\nBreeders\n1\n\n\n388\nTanya Reynolds\nThe Mallorca Files\n1\n\n\n389\nTanya Reynolds\nLily Meets Charlie\n1\n\n\n390\nTanya Reynolds\nFor Love or Money\n1\n\n\n927\nHannah Stokely\nChromophobia\n1\n\n\n\n\n928 rows × 3 columns\n\n\n\nAs we are assuming that the best recommendation has the greatest number of actors from “Emma.”, the last couple rows (the movies with only one actor from our movie) are not great recommendations for our users. Thus, we can make our plot simpler and easier to read by only plotting the the best recommendations. In the next cell, I will create a new dataframe that only includes the top 150 rows of sorted data frame.\n\n#selecting our top choices with the most shared actors\ntop_movies = sorted_df.head(150)\ntop_movies\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nactor_counts_per_movie\n\n\n\n\n771\nJohnny Flynn\nEmma.\n49\n\n\n249\nEdmund George Taylor\nEmma.\n49\n\n\n261\nAlastair Postlethwaite\nEmma.\n49\n\n\n228\nAngus Imrie\nEmma.\n49\n\n\n401\nAmber Anderson\nEmma.\n49\n\n\n...\n...\n...\n...\n\n\n289\nOliver Chris\nBeauty and the Beast: A Comic Relief Pantomime...\n2\n\n\n285\nChloe Pirrie\nThe Victim\n2\n\n\n739\nBill Nighy\nHarry Potter and the Deathly Hallows: Part 1\n2\n\n\n204\nMyra McFadyen\nJonathan Creek\n2\n\n\n281\nChloe Pirrie\nThe Queen's Gambit\n2\n\n\n\n\n150 rows × 3 columns\n\n\n\nFinally, we can create our visualization and determine the best recommendations. The best way to model the answer to our question is through creating a bar graph. We will plot the movie names and the number of actors from “Emma.” in each film. The tall the bar, the better fit the film will be for fans of “Emma.”. As every actor discussed has played a role in “Emma.”, we will expect this film to have the tallest bar that accounts for the entire cast of 49 people. Use Plotly in the cell below to create a graph.\n\n#creating our bar graph with the given conditions\nfig = px.bar(top_movies, \n             x='movie_or_TV_name', \n             y='actor_counts_per_movie', \n             title='Movies with the \"Emma\" Actors')\n\n#make customizations to make our plot more readable\nfig.update_xaxes(title='Movie/TV Show')\nfig.update_yaxes(title='Number of Shared Actors')\nfig.update_layout(yaxis=dict(range=[0, 50]))\nfig.update_layout(title = 'Movie Recommendations for \"Emma.\" Fans')\n\n#output our figure\nfig.show()\n\n\n\n\nNow that we have created out graph, we can make educated recommendations. People who enjoy “Emma.” and its cast will most likely enjoy the shows “The Crown” and “Doctor Who”.\nAs we can see, we can use webscraping to answer a variety of questions and study a variety of topics! I hope that this post helps you use webscraping, data manipulation, and plotting in your assignments. Thanks for reading!"
  },
  {
    "objectID": "posts/HW2 final/index.html#using-scrapy-and-the-film-emma.-2020",
    "href": "posts/HW2 final/index.html#using-scrapy-and-the-film-emma.-2020",
    "title": "Web Scraping",
    "section": "",
    "text": "Let’s start of today’s blog post by asking the following question: what movie or TV shows share actors with your favorite movie or show?\nHave you ever finished a great movie just to immediately look up every behind the scenes detail on the internet? I know that I definitely have. Often times, I find that if I enjoy one movie I am more likely to enjoy a second movie created by the same director or starring the same actor. Rather than going down a tedious “Google rabbit hole”, we can create a program that takes a movie and shows us which projects we can see the same actors in.\nWe will complete this project using the Python package called scrapy. This package allows us to extract data from a specific website and use it in our own way. We must start our project by importing all the necessary Python packages.\n\nimport scrapy\nimport pandas as pd\nimport numpy as py\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nScrapy allows us to webscrape and gather data from an online database. Pandas and Numpy enables us to create out own data frame with the information we extracted through webscraping and manipulate them. With Plotly, we can create visualizations using the elements of our dataframe. For our demo, we will use the 2020 movie “Emma.” starring Anya Joy Taylor. This personal favorite of mine stars very notable actors and actresses and is widely known as it is based off a book by Jane Austen. Many of these actors have overlapping projects with one another. Moreover, we can assume that someone who enjoys the movie “Emma.” will enjoy movies with a similar cast. This leads us to our question! The best way to answer our question and make some excellent movie recommendations will be to create a data frame and a visualization."
  },
  {
    "objectID": "posts/HW2 final/index.html#creating-our-scraper",
    "href": "posts/HW2 final/index.html#creating-our-scraper",
    "title": "Web Scraping",
    "section": "",
    "text": "We created our scraper using two seperate Python files and the Scrapy package. The ‘settings.py’ file was autofilled with all of the necessary lines of code to enable our scraper. In this file, it is very important to change the user agent in order to avoid encountering errors while interacting with different websites.\nIn the ‘tmdb_scraper.py’ file we start by importing scrapy and defining our class. Within our class, we will have three functions. These functions will all work together to extract specific information from our online database.\nThe first function ‘def init’ is set. The only thing that changes in this function is the exact url for our specific movie.\nOur next function is ‘def parse’. In this function, we attach more elements to our original url so that it now links to the cast page from the movie overview page. We pass the result of this function to teh next function so that we may start extracting cast information from our website. Our first function is listed in the following code cell.\n\ndef parse(self, response):\n        cast_page_url = response.url + '/cast'\n        yield scrapy.Request(cast_page_url, callback=self.parse_full_credits)\n\nNext, we will define our ‘parse_full_credits’ function. We will use the call “response.css” along with specific source code from the website itself to identify each member of our movie’s cast. We will then use the url for each actor to pull information from their personal page and determine which movies each actor has worked in. The code for this function is written in the cell below.\n\ndef parse_full_credits(self, response):\n        actors = response.css(\"ol.people.credits\")[0].css(\"a::attr(href)\").extract() \n        for actor_url in actors:\n            yield scrapy.Request(response.urljoin(actor_url), callback=self.parse_actor_page)\n\nFinally, we will create one more function called ‘def parse_actor_page’. This function will actually pull the movie and TV show names from the list of credits for each actor. It is important that we select the correct part of the website to use in the ‘response.css’. After implementing the code from the cell below, we will be able to create our CSV file.\n\ndef parse_actor_page(self, response):\n        actor_name = response.css('a::text').extract()[35]\n        movie_or_TV_names = response.css('bdi::text').extract()\n        for movie_or_TV_name in movie_or_TV_names:\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name}\n\nNow that we have created all of our functions, we will now be able to create a CSV file that contains both actor names and movie/show names. Run the following line in the terminal: #### scrapy crawl tmdb_spider -o results3.csv -a subdir=556678-emma We will now have a CSV file in our ‘TMDB_scraper’ file. After completing all of the above steps, we can now use our extracted data to create a dataframe and a visualization."
  },
  {
    "objectID": "posts/HW2 final/index.html#creating-our-database",
    "href": "posts/HW2 final/index.html#creating-our-database",
    "title": "Web Scraping",
    "section": "",
    "text": "Now that we have gathered our data using our webscraper, we will now organzie it into a database. We will start by simporting our csv file that we filled using our scraper. Please ensure that this CSV file is saved in the same location as your Jupyter notebook.\n\n#creating our initial database using out CSV file\nfilename = \"results.csv\"\ndf = pd.read_csv(filename)\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nAnya Taylor-Joy\nThe Gorge\n\n\n1\nSuzie Toase\nEmma.\n\n\n2\nSuzie Toase\nNational Theatre Live: Present Laughter\n\n\n3\nSuzie Toase\nNational Theatre Live: One Man, Two Guvnors\n\n\n4\nSuzie Toase\nHarry Potter and the Deathly Hallows: Part 2\n\n\n...\n...\n...\n\n\n923\nHannah Stokely\nThe Falling\n\n\n924\nHannah Stokely\nSkyfall\n\n\n925\nHannah Stokely\nVexed\n\n\n926\nHannah Stokely\nThe Duchess\n\n\n927\nHannah Stokely\nChromophobia\n\n\n\n\n928 rows × 2 columns\n\n\n\nAfter printing our dataframe, we can see that it looks extremely similar to the table in our csv file. This is great! However, in order to find movies that share the same actors, we must know which of the films in our “movie_or_TV_name” column have multiple actors from “Emma.”. We can do this by creating a sorted list and adding a new column to our dataframe. We can implement this using the code below.\n\n#identifies the movies that have more that stars more than one of our 50 actors in \"Emma.\":\nactor_counts_per_movie = df.groupby('movie_or_TV_name')['actor'].nunique()\nsorted_actor_counts = actor_counts_per_movie.sort_values(ascending=False)\n\n#here we will make our above list a column within our dataframe: \ndf['actor_counts_per_movie'] = df['movie_or_TV_name'].map(actor_counts_per_movie)\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nactor_counts_per_movie\n\n\n\n\n0\nAnya Taylor-Joy\nThe Gorge\n1\n\n\n1\nSuzie Toase\nEmma.\n49\n\n\n2\nSuzie Toase\nNational Theatre Live: Present Laughter\n1\n\n\n3\nSuzie Toase\nNational Theatre Live: One Man, Two Guvnors\n2\n\n\n4\nSuzie Toase\nHarry Potter and the Deathly Hallows: Part 2\n1\n\n\n...\n...\n...\n...\n\n\n923\nHannah Stokely\nThe Falling\n1\n\n\n924\nHannah Stokely\nSkyfall\n1\n\n\n925\nHannah Stokely\nVexed\n1\n\n\n926\nHannah Stokely\nThe Duchess\n1\n\n\n927\nHannah Stokely\nChromophobia\n1\n\n\n\n\n928 rows × 3 columns\n\n\n\nNow we can better use our data to make recommendations. After creating our second column, let’s test if everything worked. After doing some exploring the Movie Database, I noticed that three members of the “Emma.” cast also had roles in the hit show “Peaky Blinders”. However, our database has thousands of rows of movies. We will use the following cell of code to ensure that our database was created correctly.\n\n#we will pick a random movie that we know multiple actors have starred in \n#for this demo, we will use \"Peaky Blinders\"\nspecific_movie = \"Peaky Blinders\"\n#we will run the line to determine the number of actors using a specific movie title\nactor_count = sorted_actor_counts.get(specific_movie)\n\n#we will now use loops to determine the number of actors who are in \"Emma.\" and \"Peaky Blinders\"\nif actor_count is not None:\n    print(f\"The number of actors in '{specific_movie}' is {actor_count}.\")\nelse:\n    print(\"Error!\")\n\nThe number of actors in 'Peaky Blinders' is 3.\n\n\nOur test worked! Therefore, we can confirm that we have successfully created a dataframe that contains actor names, movies and shows for each actor, and the number of actors from our list that are in each project.\nWe need to make a few more adjustments to our dataset before we can create a readable graph. The first step is to our sort our dataframe. In order to make my dataset and my graph easier to read, I am going to put the rows in descending order by starting with the movies that share the most actors and ending with the ones that share the least.\n\n#creating our sorted dataframe:\nsorted_df = df.sort_values(by='actor_counts_per_movie', ascending=False)\n\n#as every actor in this film is in \"Emma.\", this movie should be in the first few rows\n#display our updated dataframe:\nsorted_df\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nactor_counts_per_movie\n\n\n\n\n771\nJohnny Flynn\nEmma.\n49\n\n\n249\nEdmund George Taylor\nEmma.\n49\n\n\n261\nAlastair Postlethwaite\nEmma.\n49\n\n\n228\nAngus Imrie\nEmma.\n49\n\n\n401\nAmber Anderson\nEmma.\n49\n\n\n...\n...\n...\n...\n\n\n386\nTanya Reynolds\nBreeders\n1\n\n\n388\nTanya Reynolds\nThe Mallorca Files\n1\n\n\n389\nTanya Reynolds\nLily Meets Charlie\n1\n\n\n390\nTanya Reynolds\nFor Love or Money\n1\n\n\n927\nHannah Stokely\nChromophobia\n1\n\n\n\n\n928 rows × 3 columns\n\n\n\nAs we are assuming that the best recommendation has the greatest number of actors from “Emma.”, the last couple rows (the movies with only one actor from our movie) are not great recommendations for our users. Thus, we can make our plot simpler and easier to read by only plotting the the best recommendations. In the next cell, I will create a new dataframe that only includes the top 150 rows of sorted data frame.\n\n#selecting our top choices with the most shared actors\ntop_movies = sorted_df.head(150)\ntop_movies\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nactor_counts_per_movie\n\n\n\n\n771\nJohnny Flynn\nEmma.\n49\n\n\n249\nEdmund George Taylor\nEmma.\n49\n\n\n261\nAlastair Postlethwaite\nEmma.\n49\n\n\n228\nAngus Imrie\nEmma.\n49\n\n\n401\nAmber Anderson\nEmma.\n49\n\n\n...\n...\n...\n...\n\n\n289\nOliver Chris\nBeauty and the Beast: A Comic Relief Pantomime...\n2\n\n\n285\nChloe Pirrie\nThe Victim\n2\n\n\n739\nBill Nighy\nHarry Potter and the Deathly Hallows: Part 1\n2\n\n\n204\nMyra McFadyen\nJonathan Creek\n2\n\n\n281\nChloe Pirrie\nThe Queen's Gambit\n2\n\n\n\n\n150 rows × 3 columns\n\n\n\nFinally, we can create our visualization and determine the best recommendations. The best way to model the answer to our question is through creating a bar graph. We will plot the movie names and the number of actors from “Emma.” in each film. The tall the bar, the better fit the film will be for fans of “Emma.”. As every actor discussed has played a role in “Emma.”, we will expect this film to have the tallest bar that accounts for the entire cast of 49 people. Use Plotly in the cell below to create a graph.\n\n#creating our bar graph with the given conditions\nfig = px.bar(top_movies, \n             x='movie_or_TV_name', \n             y='actor_counts_per_movie', \n             title='Movies with the \"Emma\" Actors')\n\n#make customizations to make our plot more readable\nfig.update_xaxes(title='Movie/TV Show')\nfig.update_yaxes(title='Number of Shared Actors')\nfig.update_layout(yaxis=dict(range=[0, 50]))\nfig.update_layout(title = 'Movie Recommendations for \"Emma.\" Fans')\n\n#output our figure\nfig.show()\n\n\n\n\nNow that we have created out graph, we can make educated recommendations. People who enjoy “Emma.” and its cast will most likely enjoy the shows “The Crown” and “Doctor Who”.\nAs we can see, we can use webscraping to answer a variety of questions and study a variety of topics! I hope that this post helps you use webscraping, data manipulation, and plotting in your assignments. Thanks for reading!"
  },
  {
    "objectID": "posts/HW0-Data Visualization/index.html",
    "href": "posts/HW0-Data Visualization/index.html",
    "title": "Data Visualization",
    "section": "",
    "text": "There are many different ways to model and display a set of data. Using the Plotly library, we have the ability to make histographes, boxplots, and more. Plotly makes graphing and modelling data sets very simple and straighforward. In general, you start with calling the type of figure you would like to create and then manually selecting which customization you need for the specific plot. These plots can be used to model a plethora of different things but for our purposes, we will focus on modelling data gathered by researchers.\n\n\n\nFor simplicity sake, today we will start with developing scatterplots. Scatterplots plot each individual data point onto a two dimensional axis. By hovering over the point, we will be able to see what the point means in relation to our data. Before we start making our graph, we must download and organize our data. The easiest way to complete this step is by using panda operations as seen below. Today, we will be using the Palmer Penguin data set that analyzes the differences between three different species of penguins. In order to use this data sheet, we must ensure that we have the file downloaded to the same folder as our Jupyter Notebook. Please run the code cell below to upload the needed data.\n\n#importing packages and our data set \n\nimport pandas as pd\nfilename = \"palmer_penguins.csv\"\npenguins = pd.read_csv(filename)\npenguins = penguins.dropna(subset = [\"Body Mass (g)\", \"Sex\"])\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)\npenguins = penguins[penguins[\"Sex\"] != \".\"]\n\ncols = [\"Species\", \"Island\", \"Sex\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]\npenguins = penguins[cols]\n\nAfter we have downloaded our data set, we must import Plotly in order to make our visualizations. Plotly is a useful tool that can be used to create different types of graphs. Unless we import this package, our keywords to create plots will not be recognized.\n\nimport plotly\n\nNext, we will create a visualization labeled “fig” and use our Plotly commands to organize our data. For this plot, we will see how the length and depth of the culmen vary for different species of penguins. The culmen describes the upper ridge of a penguin’s bill. Researches describe the culmen using depth and length.\nWe use the second and third lines of code to make the final plot visible to a blog user. If you are just planning on creating figures in your notebook, please only use the first line to import the necessary tools to create and customize the plot. We call our scatterplot in the fifth line of code. In the event that you are making a different type of plot, you would set “fig” equal to a different keyword. Within our “()” we will label our x and y axis, change dot color based on the species of penguin, and designate the size of the graph.\nIn the second to last line of code, we add extra customizations to the layout of the plot itself. By using these commands, we can decrease the amount of whitespace of our graph. Finally, we can see our final scatterplot using the last line of code.\n\n#importing our packages to print and create our plots \n#our first plot will be a scatterplot\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default='iframe'\n\nfig = px.scatter(data_frame = penguins,\n                 x = \"Culmen Length (mm)\",\n                 y = \"Culmen Depth (mm)\",\n                 color = \"Species\",\n                 width = 600,\n                 height = 400\n                )\n\n#add a title\nfig.update_layout(title_text = \"Culmen Length vs. Culmen Depth in Different Penguin Species\")\n\n#output our figure\nfig.show()\n \n\n\n\n\nIn all, scatterplots through Plotly are extremely customizable and only require basic calls. Plotly can be used for many different data sets and model many different ideas in a variety of forms.\n\n\n\nPlotly also has the ability to create more detailed scatterplots. For example, we can create facets within our scatterplots. Facets are smaller scatterplots that can add additional details to our visualizations. Similar to our above scatterplot, we will be comparing culmen measurements amoungest different species of penguins. However, we will further our understanding by creating facets that show the recorded culmen data in specific plots for female and male penguins.\nWe set up our plot in a relatively similar way to the demonstration above. The extra customizations will allow our graph to appear in two smaller sets.\n\n#for our second plot, we will make a scatterplot with facets\n\nfig = px.scatter(data_frame = penguins,\n                 x = \"Culmen Length (mm)\",\n                 y = \"Culmen Depth (mm)\",\n                 color = \"Species\",\n                 hover_name = \"Species\",\n                 hover_data = [\"Island\", \"Sex\"],\n                 size = \"Body Mass (g)\",\n                 size_max = 8,\n                 width = 850,\n                 height = 400,\n                opacity = 0.5,\n                facet_col= \"Sex\",\n                title = \"Culmen Length vs. Depth for Different Species in Both Male and Female Penguins\")\n\n#output figure\nfig.show()\n\n\n\n\n\n\n\nWe have now explored two different ways to work with scatterplots. However, these two plots are both in 2D and only compare two pieces of recorded data in their visualizations. Through Plotly, we can explore plots that compare three different types of measurements. This means we are making 3D scatterplot graphes! For our example, we will keep analyzing both culmen depth and culmen length but now will also incorporate body mass measurements. Luckily for us, the format of our customizations is very similar. Instead, we use a slightly different call that designates that this is a 3-dimensional plot. Run the code block below to see the 3D scatterplot.\n\n#finally, we will make our most advanced plot: a 3D scatter plot\n\nfig = px.scatter_3d(penguins,\n                    x = \"Body Mass (g)\",\n                    y = \"Culmen Length (mm)\",\n                    z = \"Culmen Depth (mm)\",\n                    color = \"Species\",\n                    opacity = 0.5)\n\n#add a title\nfig.update_layout(title = \"Culmen Length vs. Culmen Depth vs. Body Mass for Different Species of Penguins\")\n\n#output figure\nfig.show()\n\n\n\n\nIn addition to scatterplots, Plotly also has the capabilties to make box plots, heatmaps, and more! As we can see, Plotly is a fantastic tool that can be used in a variety of ways. Thank you for reading! Good luck making your visualizations with Plotly!"
  },
  {
    "objectID": "posts/HW0-Data Visualization/index.html#using-the-palmer-penguins-data-set",
    "href": "posts/HW0-Data Visualization/index.html#using-the-palmer-penguins-data-set",
    "title": "Data Visualization",
    "section": "",
    "text": "There are many different ways to model and display a set of data. Using the Plotly library, we have the ability to make histographes, boxplots, and more. Plotly makes graphing and modelling data sets very simple and straighforward. In general, you start with calling the type of figure you would like to create and then manually selecting which customization you need for the specific plot. These plots can be used to model a plethora of different things but for our purposes, we will focus on modelling data gathered by researchers."
  },
  {
    "objectID": "posts/HW0-Data Visualization/index.html#scatterplots",
    "href": "posts/HW0-Data Visualization/index.html#scatterplots",
    "title": "Data Visualization",
    "section": "",
    "text": "For simplicity sake, today we will start with developing scatterplots. Scatterplots plot each individual data point onto a two dimensional axis. By hovering over the point, we will be able to see what the point means in relation to our data. Before we start making our graph, we must download and organize our data. The easiest way to complete this step is by using panda operations as seen below. Today, we will be using the Palmer Penguin data set that analyzes the differences between three different species of penguins. In order to use this data sheet, we must ensure that we have the file downloaded to the same folder as our Jupyter Notebook. Please run the code cell below to upload the needed data.\n\n#importing packages and our data set \n\nimport pandas as pd\nfilename = \"palmer_penguins.csv\"\npenguins = pd.read_csv(filename)\npenguins = penguins.dropna(subset = [\"Body Mass (g)\", \"Sex\"])\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)\npenguins = penguins[penguins[\"Sex\"] != \".\"]\n\ncols = [\"Species\", \"Island\", \"Sex\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]\npenguins = penguins[cols]\n\nAfter we have downloaded our data set, we must import Plotly in order to make our visualizations. Plotly is a useful tool that can be used to create different types of graphs. Unless we import this package, our keywords to create plots will not be recognized.\n\nimport plotly\n\nNext, we will create a visualization labeled “fig” and use our Plotly commands to organize our data. For this plot, we will see how the length and depth of the culmen vary for different species of penguins. The culmen describes the upper ridge of a penguin’s bill. Researches describe the culmen using depth and length.\nWe use the second and third lines of code to make the final plot visible to a blog user. If you are just planning on creating figures in your notebook, please only use the first line to import the necessary tools to create and customize the plot. We call our scatterplot in the fifth line of code. In the event that you are making a different type of plot, you would set “fig” equal to a different keyword. Within our “()” we will label our x and y axis, change dot color based on the species of penguin, and designate the size of the graph.\nIn the second to last line of code, we add extra customizations to the layout of the plot itself. By using these commands, we can decrease the amount of whitespace of our graph. Finally, we can see our final scatterplot using the last line of code.\n\n#importing our packages to print and create our plots \n#our first plot will be a scatterplot\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default='iframe'\n\nfig = px.scatter(data_frame = penguins,\n                 x = \"Culmen Length (mm)\",\n                 y = \"Culmen Depth (mm)\",\n                 color = \"Species\",\n                 width = 600,\n                 height = 400\n                )\n\n#add a title\nfig.update_layout(title_text = \"Culmen Length vs. Culmen Depth in Different Penguin Species\")\n\n#output our figure\nfig.show()\n \n\n\n\n\nIn all, scatterplots through Plotly are extremely customizable and only require basic calls. Plotly can be used for many different data sets and model many different ideas in a variety of forms."
  },
  {
    "objectID": "posts/HW0-Data Visualization/index.html#scatterplots-with-facets",
    "href": "posts/HW0-Data Visualization/index.html#scatterplots-with-facets",
    "title": "Data Visualization",
    "section": "",
    "text": "Plotly also has the ability to create more detailed scatterplots. For example, we can create facets within our scatterplots. Facets are smaller scatterplots that can add additional details to our visualizations. Similar to our above scatterplot, we will be comparing culmen measurements amoungest different species of penguins. However, we will further our understanding by creating facets that show the recorded culmen data in specific plots for female and male penguins.\nWe set up our plot in a relatively similar way to the demonstration above. The extra customizations will allow our graph to appear in two smaller sets.\n\n#for our second plot, we will make a scatterplot with facets\n\nfig = px.scatter(data_frame = penguins,\n                 x = \"Culmen Length (mm)\",\n                 y = \"Culmen Depth (mm)\",\n                 color = \"Species\",\n                 hover_name = \"Species\",\n                 hover_data = [\"Island\", \"Sex\"],\n                 size = \"Body Mass (g)\",\n                 size_max = 8,\n                 width = 850,\n                 height = 400,\n                opacity = 0.5,\n                facet_col= \"Sex\",\n                title = \"Culmen Length vs. Depth for Different Species in Both Male and Female Penguins\")\n\n#output figure\nfig.show()"
  },
  {
    "objectID": "posts/HW0-Data Visualization/index.html#d-scatterplots",
    "href": "posts/HW0-Data Visualization/index.html#d-scatterplots",
    "title": "Data Visualization",
    "section": "",
    "text": "We have now explored two different ways to work with scatterplots. However, these two plots are both in 2D and only compare two pieces of recorded data in their visualizations. Through Plotly, we can explore plots that compare three different types of measurements. This means we are making 3D scatterplot graphes! For our example, we will keep analyzing both culmen depth and culmen length but now will also incorporate body mass measurements. Luckily for us, the format of our customizations is very similar. Instead, we use a slightly different call that designates that this is a 3-dimensional plot. Run the code block below to see the 3D scatterplot.\n\n#finally, we will make our most advanced plot: a 3D scatter plot\n\nfig = px.scatter_3d(penguins,\n                    x = \"Body Mass (g)\",\n                    y = \"Culmen Length (mm)\",\n                    z = \"Culmen Depth (mm)\",\n                    color = \"Species\",\n                    opacity = 0.5)\n\n#add a title\nfig.update_layout(title = \"Culmen Length vs. Culmen Depth vs. Body Mass for Different Species of Penguins\")\n\n#output figure\nfig.show()\n\n\n\n\nIn addition to scatterplots, Plotly also has the capabilties to make box plots, heatmaps, and more! As we can see, Plotly is a fantastic tool that can be used in a variety of ways. Thank you for reading! Good luck making your visualizations with Plotly!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Web Scraping\n\n\n\n\n\n\nHomework\n\n\ncode\n\n\nWeek 5\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nIsabella Woulfe\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping\n\n\n\n\n\n\nHomework\n\n\ncode\n\n\nWeek 5\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nIsabella Woulfe\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling and Visualization\n\n\n\n\n\n\nHomework\n\n\ncode\n\n\nWeek 1\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nIsabella Woulfe\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\n\n\n\n\nHomework\n\n\ncode\n\n\nWeek 0\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nIsabella Woulfe\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nIsabella Woulfe\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nIsabella Woulfe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Isabella Woulfe",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/HW 1/index.html",
    "href": "posts/HW 1/index.html",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "Today, we will be reviewing how to create interesting and interative data sets with the NOAA climate data. The data set that we will be using has a list of countries, different station names, recorded temperatures for every month of the year and more. By analyzing and organzing our data sets efficiently, we will be able to create interesting visualizations and use them to answer a variety of questions.\nBefore we start, we must download the necessary packages to create our database. Please run the code cell bellow to import Pandas, Numpy and SQL.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nNow that we have our packages, we will start importing three different datasets. We must save these csv files in the same folder as our Jupyter Notebook in order to use them here. In the following code cells, we will use the call stations.head() to show the first five rows of our data in a table. This step makes it easy for us to ensure that there was no problems when uploading our datasets. Please run the following code cells and then we can get started building our database.\n\nfilename = \"station-metadata.csv\"\nstations = pd.read_csv(filename)\nstations.head()\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\n\n\n\n\n\n\n\n\nfilename = \"country-codes.csv\"\ncountries = pd.read_csv(filename)\ncountries.head()\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n\n\n\n\n\n\nfilename = \"temps.csv\"\ntemperatures = pd.read_csv(filename)\ntemperatures.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\n\n\n\nNow that we have uploaded the three necessary datasets, we will create a new data frame that can be used to organize the specific data we want to use. We will use SQL to pull from our three data sets to craft out database.\n\nconn = sqlite3.connect(\"temps.db\")\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\ndf = df_iter.__next__()\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\n\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\nfor i, df in enumerate(df_iter):\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\n\nurl = \"station-metadata.csv\"\nstations = pd.read_csv(url)\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\n27585\n\n\n\nurl = \"country-codes.csv\"\ncountries = pd.read_csv(url)\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('country',), ('stations',), ('countries',)]\n\n\nWe now have downloaded all of our different data sets into our folder. We will now use the following commands to create tables. We should see three seperate table headings in our output.\n\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"country\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\nWe will fill our database using a function in a seperate Python file. From this file, we will import our function and as a result our database. This function requires us to use SQL keywords (written in all capital letters) in order to easily combine different sorts of data into one whole set.\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    conn = sqlite3.connect(db_file)\n    query = f'''\n        SELECT s.NAME, s.LATITUDE, s.LONGITUDE, c.NAME AS Country, t.Year, t.Month, t.Temp\n        FROM temperatures t\n        LEFT JOIN stations s ON t.ID = s.id\n        LEFT JOIN countries c ON t.ID LIKE c.\"FIPS 10-4\" || '%'\n        WHERE c.Name = '{country}' AND t.Month = {month} AND t.Year BETWEEN {year_begin} AND {year_end}\n\n'''\n    result_df = pd.read_sql_query(query, conn)\n    conn.close()\n    return result_df\n\n\n\nIt is very important that we test our code as we go so that we can be certain we have not encountered any errors. The code cell below will create a database for temperatures in India during January between the years of 1980 and 2020. Please run the cell below to show that we correctly implemented our database.\n\n#test case for our query function \nquery_climate_database(db_file = \"temps.db\", \n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020, \n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns\n\n\n\nAs our database for India matches the sample, we have correctly created our database!\n\n\n\nNow that we have all our data from our three sets organized, we can use them to answer interesting questions. Visualizations are a great way to display data as they are super versatile and easy to read. We will use Plotly to create our data sets. Please import the Plotly package below.\n\nfrom plotly import express as px\n\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nLet’s get started with our first question.\nHow does the average yearly change in temperature vary within a given country?\nThe best way to tackle this question is through creating a geographic scatter function. This figure will appear as a map but will mark specific stations in a given country and share their temperature measurements. We will start by creating a function called tenperature_coefficient_plot(). Please follow the code written below to create our first visualization. We must be sure to include the necessary parameters and be as detailed as possible with our plots.\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    df = df[df.groupby('NAME')['Year'].transform('count') &gt;= min_obs]\n\n    def temperature_average(data):\n        X = data['Year']\n        y = data['Temp']\n        \n        average = np.polyfit(X, y, deg=1)[0]\n        \n        return average\n\n    coefs = df.groupby('NAME').apply(temperature_average).reset_index()\n    coefs.columns = ['NAME', 'YearlyChange']\n\n    df = pd.merge(df, coefs, on='NAME')\n\n    fig = px.scatter_mapbox(df, \n                            lat='LATITUDE', \n                            lon='LONGITUDE',\n                            color='YearlyChange',\n                            hover_data = {'NAME' : True, 'YearlyChange': ':.3f'},\n                            labels = {'YearlyChange': 'Estimated Yearly Increase (Celsius)'}, \n                            title = f\"Estimates of Yearly Increase in Temperature in {pd.to_datetime(month, format='%m').month_name()} for stations in {country}, {year_begin}-{year_end}\",\n                            **kwargs)\n    fig.update_layout(mapbox_style=\"open-street-map\")\n    return fig\n\nWe will now use the following test case of India in January during 1980-2020 to test our function. As you can see, we will create a figure that appears as an interactive map that easily displays the answer to our posed question\n\n#test case \ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"temps.db\", \"India\", 1980, 2020, 1, \n                                   min_obs=10,\n                                   zoom=2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nNow, we can answer our question! For our chosen country of India, we can see that India has not experienced huge increases in temperature during the month of Januray over the period of forty years. Nevertheless, there are some stations along the coasts and borders that have seen significant increases in temperature.\n\n\n\nWhile we are programming, we want to ask questions that we can answer using our models. By asking these questions, we can connect our data sets to real world scenarios.\nFor our next question, let’s ask which regions of the world have the highest concentration of stations recording temperature data?\nThe best way to approach this question is to create a heatmap using our knowledge of Plotly. We can start by defining the following function. We can create the figure using specific columns from our database.\n\ndef temperature_coefficient_heatmap(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    df = df[df.groupby('NAME')['Year'].transform('count') &gt;= min_obs]\n\n    def temperature_average(data):\n        X = data['Year']\n        y = data['Temp']\n        \n        average = np.polyfit(X, y, deg=1)[0]\n        \n        return average\n\n    coefs = df.groupby('NAME').apply(temperature_average).reset_index()\n    coefs.columns = ['NAME', 'YearlyChange']\n\n    df = pd.merge(df, coefs, on='NAME')\n\n   \n#outputs our figure\nfig = px.density_mapbox(stations,\n                        lat = \"LATITUDE\",\n                        lon = \"LONGITUDE\",\n                        hover_data = {'NAME' : True},\n                        radius = 1,\n                        zoom = 0,\n                        height = 300)\n\nfig.update_layout(mapbox_style=\"carto-positron\")\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()\n\n\n\n\nNow that we have our plot, it is important that we understand what it is telling us. The lighter the color gets (or yellower) means that there is a higher concentration of stations recording these temperatures in our data set. In regions with only a few stations, we only note a handful of dots spread around the area.\nIn response to our question, it seems that the United States and parts of Europe have the highest concentration of stations.\nFinally, let’s create one more visualization and learn more about temperature using our data sets. During a twenty-year period, which part of Italy experienced higher average temperatures during the month of August: North Italy, Central Italy, or South Italy?\nAs someone who has lived in Italy, I am very aware of the different lifestyles of the people in each of these three regions. Moreover, I would like to inquire whether climate has played a role in these cultural differences.\nOnce again, I have decided to use a scatterplot to answer my question. However, this scatterplot will differ from the one above as it analyzes the average temperature of each station rather than the change in temperature. Please see the code below.\n\ndef regional_difference_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    df = df[df.groupby('NAME')['Temp'].transform('count') &gt;= min_obs]\n\n    def temperature_average(df):\n        average = np.mean(df['Temp'])\n        \n        return average\n\n    coefs = df.groupby('NAME').apply(temperature_average).reset_index()\n    coefs.columns = ['NAME', 'Average Temperature']\n\n    df = pd.merge(df, coefs, on='NAME')\n\n    fig = px.scatter_mapbox(df, \n                            lat='LATITUDE', \n                            lon='LONGITUDE',\n                            color='Average Temperature',\n                            hover_data = {'NAME' : True, 'Average Temperature': ':.3f'},\n                            labels = {'Average Temperature': 'Average Temperature (Celsius)'}, \n                            title = f\"Temperatures in {pd.to_datetime(month, format='%m').month_name()} for stations in {country}, {year_begin}-{year_end}\",\n                            **kwargs)\n    fig.update_layout(mapbox_style=\"open-street-map\")\n    return fig\n\nAlthough we can apply this function to any country, month, or year, we will look at Italy in August over a twenty year period to develop our conclusion.\n\n#test case \ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = regional_difference_plot(\"temps.db\", \"Italy\", 1980, 2000, 8, \n                                   min_obs=10,\n                                   zoom=2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nIn our plot above, we can see that South Italian stations have the points with the deepest red. This means that the South of Italy experienced the highest average temperatures compared to both the North and Central regions. I believe that this is very fitting considering the lifestyle and daily practices of people who reside in Southern Italy.\nThus, we have thoroughly reviewed how to create databases and make visualizations using our temperature data sets. Thank you for reading!"
  },
  {
    "objectID": "posts/HW 1/index.html#introduction",
    "href": "posts/HW 1/index.html#introduction",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "Today, we will be reviewing how to create interesting and interative data sets with the NOAA climate data. The data set that we will be using has a list of countries, different station names, recorded temperatures for every month of the year and more. By analyzing and organzing our data sets efficiently, we will be able to create interesting visualizations and use them to answer a variety of questions.\nBefore we start, we must download the necessary packages to create our database. Please run the code cell bellow to import Pandas, Numpy and SQL.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nNow that we have our packages, we will start importing three different datasets. We must save these csv files in the same folder as our Jupyter Notebook in order to use them here. In the following code cells, we will use the call stations.head() to show the first five rows of our data in a table. This step makes it easy for us to ensure that there was no problems when uploading our datasets. Please run the following code cells and then we can get started building our database.\n\nfilename = \"station-metadata.csv\"\nstations = pd.read_csv(filename)\nstations.head()\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\n\n\n\n\n\n\n\n\nfilename = \"country-codes.csv\"\ncountries = pd.read_csv(filename)\ncountries.head()\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n\n\n\n\n\n\nfilename = \"temps.csv\"\ntemperatures = pd.read_csv(filename)\ntemperatures.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0"
  },
  {
    "objectID": "posts/HW 1/index.html#creating-and-organizing-our-database",
    "href": "posts/HW 1/index.html#creating-and-organizing-our-database",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "Now that we have uploaded the three necessary datasets, we will create a new data frame that can be used to organize the specific data we want to use. We will use SQL to pull from our three data sets to craft out database.\n\nconn = sqlite3.connect(\"temps.db\")\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\ndf = df_iter.__next__()\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\n\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\nfor i, df in enumerate(df_iter):\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\n\nurl = \"station-metadata.csv\"\nstations = pd.read_csv(url)\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\n27585\n\n\n\nurl = \"country-codes.csv\"\ncountries = pd.read_csv(url)\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('country',), ('stations',), ('countries',)]\n\n\nWe now have downloaded all of our different data sets into our folder. We will now use the following commands to create tables. We should see three seperate table headings in our output.\n\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"country\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\nWe will fill our database using a function in a seperate Python file. From this file, we will import our function and as a result our database. This function requires us to use SQL keywords (written in all capital letters) in order to easily combine different sorts of data into one whole set.\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    conn = sqlite3.connect(db_file)\n    query = f'''\n        SELECT s.NAME, s.LATITUDE, s.LONGITUDE, c.NAME AS Country, t.Year, t.Month, t.Temp\n        FROM temperatures t\n        LEFT JOIN stations s ON t.ID = s.id\n        LEFT JOIN countries c ON t.ID LIKE c.\"FIPS 10-4\" || '%'\n        WHERE c.Name = '{country}' AND t.Month = {month} AND t.Year BETWEEN {year_begin} AND {year_end}\n\n'''\n    result_df = pd.read_sql_query(query, conn)\n    conn.close()\n    return result_df\n\n\n\nIt is very important that we test our code as we go so that we can be certain we have not encountered any errors. The code cell below will create a database for temperatures in India during January between the years of 1980 and 2020. Please run the cell below to show that we correctly implemented our database.\n\n#test case for our query function \nquery_climate_database(db_file = \"temps.db\", \n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020, \n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns\n\n\n\nAs our database for India matches the sample, we have correctly created our database!"
  },
  {
    "objectID": "posts/HW 1/index.html#developing-visualizations-from-our-database",
    "href": "posts/HW 1/index.html#developing-visualizations-from-our-database",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "Now that we have all our data from our three sets organized, we can use them to answer interesting questions. Visualizations are a great way to display data as they are super versatile and easy to read. We will use Plotly to create our data sets. Please import the Plotly package below.\n\nfrom plotly import express as px\n\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nLet’s get started with our first question.\nHow does the average yearly change in temperature vary within a given country?\nThe best way to tackle this question is through creating a geographic scatter function. This figure will appear as a map but will mark specific stations in a given country and share their temperature measurements. We will start by creating a function called tenperature_coefficient_plot(). Please follow the code written below to create our first visualization. We must be sure to include the necessary parameters and be as detailed as possible with our plots.\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    df = df[df.groupby('NAME')['Year'].transform('count') &gt;= min_obs]\n\n    def temperature_average(data):\n        X = data['Year']\n        y = data['Temp']\n        \n        average = np.polyfit(X, y, deg=1)[0]\n        \n        return average\n\n    coefs = df.groupby('NAME').apply(temperature_average).reset_index()\n    coefs.columns = ['NAME', 'YearlyChange']\n\n    df = pd.merge(df, coefs, on='NAME')\n\n    fig = px.scatter_mapbox(df, \n                            lat='LATITUDE', \n                            lon='LONGITUDE',\n                            color='YearlyChange',\n                            hover_data = {'NAME' : True, 'YearlyChange': ':.3f'},\n                            labels = {'YearlyChange': 'Estimated Yearly Increase (Celsius)'}, \n                            title = f\"Estimates of Yearly Increase in Temperature in {pd.to_datetime(month, format='%m').month_name()} for stations in {country}, {year_begin}-{year_end}\",\n                            **kwargs)\n    fig.update_layout(mapbox_style=\"open-street-map\")\n    return fig\n\nWe will now use the following test case of India in January during 1980-2020 to test our function. As you can see, we will create a figure that appears as an interactive map that easily displays the answer to our posed question\n\n#test case \ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"temps.db\", \"India\", 1980, 2020, 1, \n                                   min_obs=10,\n                                   zoom=2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nNow, we can answer our question! For our chosen country of India, we can see that India has not experienced huge increases in temperature during the month of Januray over the period of forty years. Nevertheless, there are some stations along the coasts and borders that have seen significant increases in temperature."
  },
  {
    "objectID": "posts/HW 1/index.html#other-forms-of-visualizations-heatmaps-more-scatter-plots",
    "href": "posts/HW 1/index.html#other-forms-of-visualizations-heatmaps-more-scatter-plots",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "While we are programming, we want to ask questions that we can answer using our models. By asking these questions, we can connect our data sets to real world scenarios.\nFor our next question, let’s ask which regions of the world have the highest concentration of stations recording temperature data?\nThe best way to approach this question is to create a heatmap using our knowledge of Plotly. We can start by defining the following function. We can create the figure using specific columns from our database.\n\ndef temperature_coefficient_heatmap(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    df = df[df.groupby('NAME')['Year'].transform('count') &gt;= min_obs]\n\n    def temperature_average(data):\n        X = data['Year']\n        y = data['Temp']\n        \n        average = np.polyfit(X, y, deg=1)[0]\n        \n        return average\n\n    coefs = df.groupby('NAME').apply(temperature_average).reset_index()\n    coefs.columns = ['NAME', 'YearlyChange']\n\n    df = pd.merge(df, coefs, on='NAME')\n\n   \n#outputs our figure\nfig = px.density_mapbox(stations,\n                        lat = \"LATITUDE\",\n                        lon = \"LONGITUDE\",\n                        hover_data = {'NAME' : True},\n                        radius = 1,\n                        zoom = 0,\n                        height = 300)\n\nfig.update_layout(mapbox_style=\"carto-positron\")\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()\n\n\n\n\nNow that we have our plot, it is important that we understand what it is telling us. The lighter the color gets (or yellower) means that there is a higher concentration of stations recording these temperatures in our data set. In regions with only a few stations, we only note a handful of dots spread around the area.\nIn response to our question, it seems that the United States and parts of Europe have the highest concentration of stations.\nFinally, let’s create one more visualization and learn more about temperature using our data sets. During a twenty-year period, which part of Italy experienced higher average temperatures during the month of August: North Italy, Central Italy, or South Italy?\nAs someone who has lived in Italy, I am very aware of the different lifestyles of the people in each of these three regions. Moreover, I would like to inquire whether climate has played a role in these cultural differences.\nOnce again, I have decided to use a scatterplot to answer my question. However, this scatterplot will differ from the one above as it analyzes the average temperature of each station rather than the change in temperature. Please see the code below.\n\ndef regional_difference_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    df = df[df.groupby('NAME')['Temp'].transform('count') &gt;= min_obs]\n\n    def temperature_average(df):\n        average = np.mean(df['Temp'])\n        \n        return average\n\n    coefs = df.groupby('NAME').apply(temperature_average).reset_index()\n    coefs.columns = ['NAME', 'Average Temperature']\n\n    df = pd.merge(df, coefs, on='NAME')\n\n    fig = px.scatter_mapbox(df, \n                            lat='LATITUDE', \n                            lon='LONGITUDE',\n                            color='Average Temperature',\n                            hover_data = {'NAME' : True, 'Average Temperature': ':.3f'},\n                            labels = {'Average Temperature': 'Average Temperature (Celsius)'}, \n                            title = f\"Temperatures in {pd.to_datetime(month, format='%m').month_name()} for stations in {country}, {year_begin}-{year_end}\",\n                            **kwargs)\n    fig.update_layout(mapbox_style=\"open-street-map\")\n    return fig\n\nAlthough we can apply this function to any country, month, or year, we will look at Italy in August over a twenty year period to develop our conclusion.\n\n#test case \ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = regional_difference_plot(\"temps.db\", \"Italy\", 1980, 2000, 8, \n                                   min_obs=10,\n                                   zoom=2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nIn our plot above, we can see that South Italian stations have the points with the deepest red. This means that the South of Italy experienced the highest average temperatures compared to both the North and Central regions. I believe that this is very fitting considering the lifestyle and daily practices of people who reside in Southern Italy.\nThus, we have thoroughly reviewed how to create databases and make visualizations using our temperature data sets. Thank you for reading!"
  },
  {
    "objectID": "posts/HW2/index.html",
    "href": "posts/HW2/index.html",
    "title": "Web Scraping",
    "section": "",
    "text": "Let’s start of today’s blog post by asking the following question: what movie or TV shows share actors with your favorite movie or show?\nHave you ever finished a great movie just to immediately look up every behind the scenes detail on the internet? I know that I definitely have. Often times, I find that if I enjoy one movie I am more likely to enjoy a second movie created by the same director or starring the same actor. Rather than going down a tedious “Google rabbit hole”, we can create a program that takes a movie and shows us which projects we can see the same actors in.\nWe will complete this project using the Python package called scrapy. This package allows us to extract data from a specific website and use it in our own way. We must start our project by importing all the necessary Python packages.\n\nimport scrapy\nimport pandas as pd\nimport numpy as py\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nScrapy allows us to webscrape and gather data from an online database. Pandas and Numpy enables us to create out own data frame with the information we extracted through webscraping and manipulate them. With Plotly, we can create visualizations using the elements of our dataframe. For our demo, we will use the 2020 movie “Emma.” starring Anya Joy Taylor. This personal favorite of mine stars very notable actors and actresses and is widely known as it is based off a book by Jane Austen. Many of these actors have overlapping projects with one another. Moreover, we can assume that someone who enjoys the movie “Emma.” will enjoy movies with a similar cast. This leads us to our question! The best way to answer our question and make some excellent movie recommendations will be to create a data frame and a visualization.\n\n\n\nWe created our scraper using two seperate Python files and the Scrapy package. The ‘settings.py’ file was autofilled with all of the necessary lines of code to enable our scraper. In this file, it is very important to change the user agent in order to avoid encountering errors while interacting with different websites.\nIn the ‘tmdb_scraper.py’ file we start by importing scrapy and defining our class. Within our class, we will have three functions. These functions will all work together to extract specific information from our online database.\nThe first function ‘def init’ is set. The only thing that changes in this function is the exact url for our specific movie.\nOur next function is ‘def parse’. In this function, we attach more elements to our original url so that it now links to the cast page from the movie overview page. We pass the result of this function to teh next function so that we may start extracting cast information from our website. Our first function is listed in the following code cell.\n\ndef parse(self, response):\n        cast_page_url = response.url + '/cast'\n        yield scrapy.Request(cast_page_url, callback=self.parse_full_credits)\n\nNext, we will define our ‘parse_full_credits’ function. We will use the call “response.css” along with specific source code from the website itself to identify each member of our movie’s cast. We will then use the url for each actor to pull information from their personal page and determine which movies each actor has worked in. The code for this function is written in the cell below.\n\ndef parse_full_credits(self, response):\n        actors = response.css(\"ol.people.credits\")[0].css(\"a::attr(href)\").extract() \n        for actor_url in actors:\n            yield scrapy.Request(response.urljoin(actor_url), callback=self.parse_actor_page)\n\nFinally, we will create one more function called ‘def parse_actor_page’. This function will actually pull the movie and TV show names from the list of credits for each actor. It is important that we select the correct part of the website to use in the ‘response.css’. After implementing the code from the cell below, we will be able to create our CSV file.\n\ndef parse_actor_page(self, response):\n        actor_name = response.css('a::text').extract()[35]\n        movie_or_TV_names = response.css('div.credits_list').css('bdi::text').extract()\n        for movie_or_TV_name in movie_or_TV_names:\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name}\n\nNow that we have created all of our functions, we will now be able to create a CSV file that contains both actor names and movie/show names. Run the following line in the terminal: #### scrapy crawl tmdb_spider -o results3.csv -a subdir=556678-emma We will now have a CSV file in our ‘TMDB_scraper’ file. After completing all of the above steps, we can now use our extracted data to create a dataframe and a visualization.\n\n\n\nNow that we have gathered our data using our webscraper, we will now organzie it into a database. We will start by simporting our csv file that we filled using our scraper. Please ensure that this CSV file is saved in the same location as your Jupyter notebook.\n\n#creating our initial database using out CSV file\nfilename = \"results.csv\"\ndf = pd.read_csv(filename)\ndf\n\nFileNotFoundError: [Errno 2] No such file or directory: 'results.csv'\n\n\nAfter printing our dataframe, we can see that it looks extremely similar to the table in our csv file. This is great! However, in order to find movies that share the same actors, we must know which of the films in our “movie_or_TV_name” column have multiple actors from “Emma.”. We can do this by creating a sorted list and adding a new column to our dataframe. We can implement this using the code below.\n\n#identifies the movies that have more that stars more than one of our 50 actors in \"Emma.\":\nactor_counts_per_movie = df.groupby('movie_or_TV_name')['actor'].nunique()\nsorted_actor_counts = actor_counts_per_movie.sort_values(ascending=False)\n\n#here we will make our above list a column within our dataframe: \ndf['actor_counts_per_movie'] = df['movie_or_TV_name'].map(actor_counts_per_movie)\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nactor_counts_per_movie\n\n\n\n\n0\nAnya Taylor-Joy\nSplit\n1\n\n\n1\nAnya Taylor-Joy\nThe Witch\n1\n\n\n2\nAnya Taylor-Joy\nThe Menu\n1\n\n\n3\nAnya Taylor-Joy\nThe Queen's Gambit\n2\n\n\n4\nAnya Taylor-Joy\nThe Super Mario Bros. Movie\n1\n\n\n...\n...\n...\n...\n\n\n1146\nMia Goth\nNymphomaniac: Vol. II\n1\n\n\n1147\nMia Goth\nMaXXXine\n1\n\n\n1148\nMia Goth\nPearl\n1\n\n\n1149\nMia Goth\nDisappear Into the Blue\n1\n\n\n1150\nMia Goth\nPearl\n1\n\n\n\n\n1151 rows × 3 columns\n\n\n\nNow we can better use our data to make recommendations. After creating our second column, let’s test if everything worked. After doing some exploring the Movie Database, I noticed that three members of the “Emma.” cast also had roles in the hit show “Peaky Blinders”. However, our database has thousands of rows of movies. We will use the following cell of code to ensure that our database was created correctly.\n\n#we will pick a random movie that we know multiple actors have starred in \n#for this demo, we will use \"Peaky Blinders\"\nspecific_movie = \"Peaky Blinders\"\n#we will run the line to determine the number of actors using a specific movie title\nactor_count = sorted_actor_counts.get(specific_movie)\n\n#we will now use loops to determine the number of actors who are in \"Emma.\" and \"Peaky Blinders\"\nif actor_count is not None:\n    print(f\"The number of actors in '{specific_movie}' is {actor_count}.\")\nelse:\n    print(\"Error!\")\n\nThe number of actors in 'Peaky Blinders' is 3.\n\n\nOur test worked! Therefore, we can confirm that we have successfully created a dataframe that contains actor names, movies and shows for each actor, and the number of actors from our list that are in each project.\nWe need to make a few more adjustments to our dataset before we can create a readable graph. The first step is to our sort our dataframe. In order to make my dataset and my graph easier to read, I am going to put the rows in descending order by starting with the movies that share the most actors and ending with the ones that share the least.\n\n#creating our sorted dataframe:\nsorted_df = df.sort_values(by='actor_counts_per_movie', ascending=False)\n\n#as every actor in this film is in \"Emma.\", this movie should be in the first few rows\n#display our updated dataframe:\nsorted_df\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nactor_counts_per_movie\n\n\n\n\n552\nEdward Davis\nEmma.\n50\n\n\n148\nJohnny Flynn\nEmma.\n50\n\n\n316\nAnna Francolini\nEmma.\n50\n\n\n377\nLeigh Daniels\nEmma.\n50\n\n\n58\nCallum Turner\nEmma.\n50\n\n\n...\n...\n...\n...\n\n\n461\nNicholas Burns\nGhost Stories\n1\n\n\n462\nNicholas Burns\nCensor\n1\n\n\n465\nNicholas Burns\nBenidorm\n1\n\n\n467\nNicholas Burns\nThe Lady in the Van\n1\n\n\n1150\nMia Goth\nPearl\n1\n\n\n\n\n1151 rows × 3 columns\n\n\n\nAs we are assuming that the best recommendation has the greatest number of actors from “Emma.”, the last couple rows (the movies with only one actor from our movie) are not great recommendations for our users. Thus, we can make our plot simpler and easier to read by only plotting the the best recommendations. In the next cell, I will create a new dataframe that only includes the top 300 rows of sorted data frame.\n\n#selecting our top choices with the most shared actors\ntop_movies = sorted_df.head(150)\ntop_movies\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nactor_counts_per_movie\n\n\n\n\n552\nEdward Davis\nEmma.\n50\n\n\n148\nJohnny Flynn\nEmma.\n50\n\n\n316\nAnna Francolini\nEmma.\n50\n\n\n377\nLeigh Daniels\nEmma.\n50\n\n\n58\nCallum Turner\nEmma.\n50\n\n\n...\n...\n...\n...\n\n\n884\nBill Nighy\nSalting the Battlefield\n2\n\n\n262\nLucy Briers\nUnnatural Causes\n2\n\n\n258\nLucy Briers\nBeast\n2\n\n\n386\nRose Shalloo\nCall the Midwife\n2\n\n\n259\nLucy Briers\nMidsomer Murders\n2\n\n\n\n\n150 rows × 3 columns\n\n\n\nFinally, we can create our visualization and determine the best recommendations. The best way to model the answer to our question is through creating a bar graph. We will plot the movie names and the number of actors from “Emma.” in each film. The tall the bar, the better fit the film will be for fans of “Emma.”. As every actor discussed has played a role in “Emma.”, we will expect this film to have the tallest bar that accounts for the entire cast of 50 people. Use Plotly in the cell below to create a graph.\n\n#creating our bar graph with the given conditions\nfig = px.bar(top_movies, \n             x='movie_or_TV_name', \n             y='actor_counts_per_movie', \n             title='Movies with the \"Emma\" Actors')\n\n#make customizations to make our plot more readable\nfig.update_xaxes(title='Movie/TV Show')\nfig.update_yaxes(title='Number of Shared Actors')\nfig.update_layout(yaxis=dict(range=[0, 50]))\nfig.update_layout(title = 'Movie Recommendations for \"Emma.\" Fans')\n\n#output our figure\nfig.show()\n\n\n\n\nNow that we have created out graph, we can make educated recommendations. People who enjoy “Emma.” and its cast will most likely enjoy the shows “The Crown” and “Doctor Who”.\nAs we can see, we can use webscraping to answer a variety of questions and study a variety of topics! I hope that this post helps you use webscraping, data manipulation, and plotting in your assignments. Thanks for reading!"
  },
  {
    "objectID": "posts/HW2/index.html#using-scrapy-and-the-film-emma.-2020",
    "href": "posts/HW2/index.html#using-scrapy-and-the-film-emma.-2020",
    "title": "Web Scraping",
    "section": "",
    "text": "Let’s start of today’s blog post by asking the following question: what movie or TV shows share actors with your favorite movie or show?\nHave you ever finished a great movie just to immediately look up every behind the scenes detail on the internet? I know that I definitely have. Often times, I find that if I enjoy one movie I am more likely to enjoy a second movie created by the same director or starring the same actor. Rather than going down a tedious “Google rabbit hole”, we can create a program that takes a movie and shows us which projects we can see the same actors in.\nWe will complete this project using the Python package called scrapy. This package allows us to extract data from a specific website and use it in our own way. We must start our project by importing all the necessary Python packages.\n\nimport scrapy\nimport pandas as pd\nimport numpy as py\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nScrapy allows us to webscrape and gather data from an online database. Pandas and Numpy enables us to create out own data frame with the information we extracted through webscraping and manipulate them. With Plotly, we can create visualizations using the elements of our dataframe. For our demo, we will use the 2020 movie “Emma.” starring Anya Joy Taylor. This personal favorite of mine stars very notable actors and actresses and is widely known as it is based off a book by Jane Austen. Many of these actors have overlapping projects with one another. Moreover, we can assume that someone who enjoys the movie “Emma.” will enjoy movies with a similar cast. This leads us to our question! The best way to answer our question and make some excellent movie recommendations will be to create a data frame and a visualization."
  },
  {
    "objectID": "posts/HW2/index.html#creating-our-scraper",
    "href": "posts/HW2/index.html#creating-our-scraper",
    "title": "Web Scraping",
    "section": "",
    "text": "We created our scraper using two seperate Python files and the Scrapy package. The ‘settings.py’ file was autofilled with all of the necessary lines of code to enable our scraper. In this file, it is very important to change the user agent in order to avoid encountering errors while interacting with different websites.\nIn the ‘tmdb_scraper.py’ file we start by importing scrapy and defining our class. Within our class, we will have three functions. These functions will all work together to extract specific information from our online database.\nThe first function ‘def init’ is set. The only thing that changes in this function is the exact url for our specific movie.\nOur next function is ‘def parse’. In this function, we attach more elements to our original url so that it now links to the cast page from the movie overview page. We pass the result of this function to teh next function so that we may start extracting cast information from our website. Our first function is listed in the following code cell.\n\ndef parse(self, response):\n        cast_page_url = response.url + '/cast'\n        yield scrapy.Request(cast_page_url, callback=self.parse_full_credits)\n\nNext, we will define our ‘parse_full_credits’ function. We will use the call “response.css” along with specific source code from the website itself to identify each member of our movie’s cast. We will then use the url for each actor to pull information from their personal page and determine which movies each actor has worked in. The code for this function is written in the cell below.\n\ndef parse_full_credits(self, response):\n        actors = response.css(\"ol.people.credits\")[0].css(\"a::attr(href)\").extract() \n        for actor_url in actors:\n            yield scrapy.Request(response.urljoin(actor_url), callback=self.parse_actor_page)\n\nFinally, we will create one more function called ‘def parse_actor_page’. This function will actually pull the movie and TV show names from the list of credits for each actor. It is important that we select the correct part of the website to use in the ‘response.css’. After implementing the code from the cell below, we will be able to create our CSV file.\n\ndef parse_actor_page(self, response):\n        actor_name = response.css('a::text').extract()[35]\n        movie_or_TV_names = response.css('div.credits_list').css('bdi::text').extract()\n        for movie_or_TV_name in movie_or_TV_names:\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name}\n\nNow that we have created all of our functions, we will now be able to create a CSV file that contains both actor names and movie/show names. Run the following line in the terminal: #### scrapy crawl tmdb_spider -o results3.csv -a subdir=556678-emma We will now have a CSV file in our ‘TMDB_scraper’ file. After completing all of the above steps, we can now use our extracted data to create a dataframe and a visualization."
  },
  {
    "objectID": "posts/HW2/index.html#creating-our-database",
    "href": "posts/HW2/index.html#creating-our-database",
    "title": "Web Scraping",
    "section": "",
    "text": "Now that we have gathered our data using our webscraper, we will now organzie it into a database. We will start by simporting our csv file that we filled using our scraper. Please ensure that this CSV file is saved in the same location as your Jupyter notebook.\n\n#creating our initial database using out CSV file\nfilename = \"results.csv\"\ndf = pd.read_csv(filename)\ndf\n\nFileNotFoundError: [Errno 2] No such file or directory: 'results.csv'\n\n\nAfter printing our dataframe, we can see that it looks extremely similar to the table in our csv file. This is great! However, in order to find movies that share the same actors, we must know which of the films in our “movie_or_TV_name” column have multiple actors from “Emma.”. We can do this by creating a sorted list and adding a new column to our dataframe. We can implement this using the code below.\n\n#identifies the movies that have more that stars more than one of our 50 actors in \"Emma.\":\nactor_counts_per_movie = df.groupby('movie_or_TV_name')['actor'].nunique()\nsorted_actor_counts = actor_counts_per_movie.sort_values(ascending=False)\n\n#here we will make our above list a column within our dataframe: \ndf['actor_counts_per_movie'] = df['movie_or_TV_name'].map(actor_counts_per_movie)\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nactor_counts_per_movie\n\n\n\n\n0\nAnya Taylor-Joy\nSplit\n1\n\n\n1\nAnya Taylor-Joy\nThe Witch\n1\n\n\n2\nAnya Taylor-Joy\nThe Menu\n1\n\n\n3\nAnya Taylor-Joy\nThe Queen's Gambit\n2\n\n\n4\nAnya Taylor-Joy\nThe Super Mario Bros. Movie\n1\n\n\n...\n...\n...\n...\n\n\n1146\nMia Goth\nNymphomaniac: Vol. II\n1\n\n\n1147\nMia Goth\nMaXXXine\n1\n\n\n1148\nMia Goth\nPearl\n1\n\n\n1149\nMia Goth\nDisappear Into the Blue\n1\n\n\n1150\nMia Goth\nPearl\n1\n\n\n\n\n1151 rows × 3 columns\n\n\n\nNow we can better use our data to make recommendations. After creating our second column, let’s test if everything worked. After doing some exploring the Movie Database, I noticed that three members of the “Emma.” cast also had roles in the hit show “Peaky Blinders”. However, our database has thousands of rows of movies. We will use the following cell of code to ensure that our database was created correctly.\n\n#we will pick a random movie that we know multiple actors have starred in \n#for this demo, we will use \"Peaky Blinders\"\nspecific_movie = \"Peaky Blinders\"\n#we will run the line to determine the number of actors using a specific movie title\nactor_count = sorted_actor_counts.get(specific_movie)\n\n#we will now use loops to determine the number of actors who are in \"Emma.\" and \"Peaky Blinders\"\nif actor_count is not None:\n    print(f\"The number of actors in '{specific_movie}' is {actor_count}.\")\nelse:\n    print(\"Error!\")\n\nThe number of actors in 'Peaky Blinders' is 3.\n\n\nOur test worked! Therefore, we can confirm that we have successfully created a dataframe that contains actor names, movies and shows for each actor, and the number of actors from our list that are in each project.\nWe need to make a few more adjustments to our dataset before we can create a readable graph. The first step is to our sort our dataframe. In order to make my dataset and my graph easier to read, I am going to put the rows in descending order by starting with the movies that share the most actors and ending with the ones that share the least.\n\n#creating our sorted dataframe:\nsorted_df = df.sort_values(by='actor_counts_per_movie', ascending=False)\n\n#as every actor in this film is in \"Emma.\", this movie should be in the first few rows\n#display our updated dataframe:\nsorted_df\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nactor_counts_per_movie\n\n\n\n\n552\nEdward Davis\nEmma.\n50\n\n\n148\nJohnny Flynn\nEmma.\n50\n\n\n316\nAnna Francolini\nEmma.\n50\n\n\n377\nLeigh Daniels\nEmma.\n50\n\n\n58\nCallum Turner\nEmma.\n50\n\n\n...\n...\n...\n...\n\n\n461\nNicholas Burns\nGhost Stories\n1\n\n\n462\nNicholas Burns\nCensor\n1\n\n\n465\nNicholas Burns\nBenidorm\n1\n\n\n467\nNicholas Burns\nThe Lady in the Van\n1\n\n\n1150\nMia Goth\nPearl\n1\n\n\n\n\n1151 rows × 3 columns\n\n\n\nAs we are assuming that the best recommendation has the greatest number of actors from “Emma.”, the last couple rows (the movies with only one actor from our movie) are not great recommendations for our users. Thus, we can make our plot simpler and easier to read by only plotting the the best recommendations. In the next cell, I will create a new dataframe that only includes the top 300 rows of sorted data frame.\n\n#selecting our top choices with the most shared actors\ntop_movies = sorted_df.head(150)\ntop_movies\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nactor_counts_per_movie\n\n\n\n\n552\nEdward Davis\nEmma.\n50\n\n\n148\nJohnny Flynn\nEmma.\n50\n\n\n316\nAnna Francolini\nEmma.\n50\n\n\n377\nLeigh Daniels\nEmma.\n50\n\n\n58\nCallum Turner\nEmma.\n50\n\n\n...\n...\n...\n...\n\n\n884\nBill Nighy\nSalting the Battlefield\n2\n\n\n262\nLucy Briers\nUnnatural Causes\n2\n\n\n258\nLucy Briers\nBeast\n2\n\n\n386\nRose Shalloo\nCall the Midwife\n2\n\n\n259\nLucy Briers\nMidsomer Murders\n2\n\n\n\n\n150 rows × 3 columns\n\n\n\nFinally, we can create our visualization and determine the best recommendations. The best way to model the answer to our question is through creating a bar graph. We will plot the movie names and the number of actors from “Emma.” in each film. The tall the bar, the better fit the film will be for fans of “Emma.”. As every actor discussed has played a role in “Emma.”, we will expect this film to have the tallest bar that accounts for the entire cast of 50 people. Use Plotly in the cell below to create a graph.\n\n#creating our bar graph with the given conditions\nfig = px.bar(top_movies, \n             x='movie_or_TV_name', \n             y='actor_counts_per_movie', \n             title='Movies with the \"Emma\" Actors')\n\n#make customizations to make our plot more readable\nfig.update_xaxes(title='Movie/TV Show')\nfig.update_yaxes(title='Number of Shared Actors')\nfig.update_layout(yaxis=dict(range=[0, 50]))\nfig.update_layout(title = 'Movie Recommendations for \"Emma.\" Fans')\n\n#output our figure\nfig.show()\n\n\n\n\nNow that we have created out graph, we can make educated recommendations. People who enjoy “Emma.” and its cast will most likely enjoy the shows “The Crown” and “Doctor Who”.\nAs we can see, we can use webscraping to answer a variety of questions and study a variety of topics! I hope that this post helps you use webscraping, data manipulation, and plotting in your assignments. Thanks for reading!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]