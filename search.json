[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome! Here I will be documenting my journey through PIC 16B.\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/HW6/index.html",
    "href": "posts/HW6/index.html",
    "title": "Fact or Fiction? Fake News Classification Using Keras",
    "section": "",
    "text": "The spread of false information, also reffered to as “fake news”, is a contant plague on modern life. Fake news can be extremely harmful to our society as a whole. But with today’s technology, we have tools that could combat the force that is fake news. In today’s blog post, we will use Keras and Machine Learning to classify articles as being truthful or not.\nAs always, we must start by importing the necassary packages and upgrading our Keras. Please run the following two code cells before we start uploading our dataset and defining our model.\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\nCollecting keras\n  Downloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 6.9 MB/s eta 0:00:00\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nCollecting namex (from keras)\n  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\nInstalling collected packages: namex, keras\n  Attempting uninstall: keras\n    Found existing installation: keras 2.15.0\n    Uninstalling keras-2.15.0:\n      Successfully uninstalled keras-2.15.0\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.15.0 requires keras&lt;2.16,&gt;=2.15.0, but you have keras 3.0.5 which is incompatible.\nSuccessfully installed keras-3.0.5 namex-0.0.7\n\n\n\n!pip install nltk\n\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\nRequirement already satisfied: regex&gt;=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n\n\n\n#import all of the necessary packages\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import stopwords\nimport keras\nimport re\nimport string\nfrom keras import layers, losses\nfrom keras.layers import TextVectorization\nfrom keras import utils\n\nNow that we have imported all of our packages, we can start by acquiring our training data. Our data is in the form of different articles that we accessed from Kaggle. By running the code cell below, we can upload the link to the data and organize it into a dataframe using pandas. Please run the code cell below and see the resulting table for our\n\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\n\n#csv file into dataframe\ndf = pd.read_csv(train_url)\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nWe now have successfully accessed our data that we will use to train our model! Before we continue, we must organize this data into a dataset. Using the following function, we will change all text to lowercase letters, remove stopwords, and create a dataset with inputs title and text and outputs a fake column. It is very important to have all of the necessary packages in order for us to filter out stop words and make a dataframe.\nPlease run the code cell below to define the make_dataset function.\n\ndef make_dataset(df):\n    \"\"\"\n    Function will create a dataset of our data and organize it with the given conditions\n\n    Arguments:\n      df : pandas DataFrame containing columns: title, text, and fake\n\n    Returns:\n      dataset : the dataset we created with the specified conditions (TensorFlow)\n    \"\"\"\n\n    #lowercase text\n    df['text'] = df['text'].apply(lambda x: x.lower())\n\n    #remove stopwords\n    nltk.download('stopwords')\n    stop = stopwords.words('english')\n    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n\n\n    title_tensor = tf.constant(df['title'].values, dtype=tf.string)\n    text_tensor = tf.constant(df['text'].values, dtype=tf.string)\n    fake_tensor = tf.constant(df['fake'].values, dtype=tf.int32)\n    dataset = tf.data.Dataset.from_tensor_slices(({\"title\": title_tensor, \"text\": text_tensor}, fake_tensor))\n\n    dataset = dataset.batch(100)\n\n    #outputs our dataset\n    return dataset\n\nNow that our function is defined, we can use our training dataframe to output a dataset. Please run the code cell below to create a dataset from our training dataframe.\n\n#create dataset using training data (see above)\nds = make_dataset(df)\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\n\nFor our next step, we will work with the validation set. This will involve taking 20% of our training data set to use for the validation.\n\n#create validation set\nds = ds.shuffle(buffer_size = len(ds), reshuffle_each_iteration=False)\nval_size = int(0.2 * len(ds))\n\nval = ds.take(val_size)\ntrain = ds.skip(val_size)\n\n\nlen(train), len(val)\n\n(180, 45)\n\n\nThe base rate is used to determine the accuracy of the model. It allows us to find a value to compare the results of our model to. Please run the code cell below to determine the base rate for our dataset.\n\n#determine the base rate for our training dataset\nfake_count = 0\ntotal_count = 0\n\nfor _, labels in train:\n    fake_count += tf.reduce_sum(labels).numpy()\n    total_count += len(labels)\n\nbase_rate = fake_count / total_count\nprint(\"Base rate:\", base_rate)\n\nBase rate: 0.5237222222222222\n\n\nAfter printing the base rate, ppreapre the text vectorization for the model. The following code cell was given in the Homework 6 blog post. Please run the code cell below before proceeding with the models.\n\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\ntitle_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\n\nThe following cell will also add an embedding layer that will be used in our models. This layer will access the word embeddings that we will use to determine whether or not our articles can be classified as fake news. Please run the code cell below.\n\nshared_embedding_layer = layers.Embedding(size_vocabulary, 3, name=\"embedding\")\n\nWe can now start on defining our three models that will be used to determine the validity of different news sources.\n\n\n\nIn our first model, our only input will be the article title. This means that the article title will be used by our model to determine how we classify the given article. We will define the following layers below to create our model. Please run the code cell below to create the first model.\n\n#defining the model when the input is our article title\ntitle_input = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"title\")\n\ntitle_features = title_vectorize_layer(title_input)\ntitle_features = shared_embedding_layer(title_features)\ntitle_features = layers.Dropout(0.5)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dense(64, activation='relu')(title_features)\ntitle_features = layers.Dropout(0.3)(title_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\n\ntitle_output = layers.Dense(1, activation='sigmoid', name=\"title_output\")(title_features)\n\n#create model 1\nmodel_1 = tf.keras.Model(inputs=title_input, outputs=title_output)\nmodel_1.summary()\n\nModel: \"functional_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)                   │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (Dropout)                    │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d             │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (Dense)                        │ (None, 64)                  │             256 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (Dropout)                  │ (None, 64)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (Dense)                      │ (None, 32)                  │           2,080 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ title_output (Dense)                 │ (None, 1)                   │              33 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 8,369 (32.69 KB)\n\n\n\n Trainable params: 8,369 (32.69 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNow that we have created Model 1, we can visualize and further understand the structure of the model by creating a flowchart. The following code was provided in the blog post instructions to complete this step. Please run the code below.\n\n#model visualization (flowchart)\nutils.plot_model(model_1, \"model_1.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nPlease run the code cell below to compile our data.\n\n#compile with training and valiation data\nmodel_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhistory_1 = model_1.fit(train, epochs=20, validation_data=val)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 8ms/step - accuracy: 0.5165 - loss: 0.6926 - val_accuracy: 0.5199 - val_loss: 0.6919\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.5219 - loss: 0.6917 - val_accuracy: 0.5923 - val_loss: 0.6859\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 6ms/step - accuracy: 0.6044 - loss: 0.6717 - val_accuracy: 0.7824 - val_loss: 0.5390\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 7ms/step - accuracy: 0.7453 - loss: 0.5289 - val_accuracy: 0.8505 - val_loss: 0.3977\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.8039 - loss: 0.4297 - val_accuracy: 0.8863 - val_loss: 0.3155\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.8196 - loss: 0.4045 - val_accuracy: 0.9018 - val_loss: 0.2730\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8544 - loss: 0.3353 - val_accuracy: 0.9189 - val_loss: 0.2363\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8662 - loss: 0.3100 - val_accuracy: 0.8824 - val_loss: 0.2688\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8515 - loss: 0.3422 - val_accuracy: 0.9310 - val_loss: 0.2065\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8834 - loss: 0.2849 - val_accuracy: 0.9249 - val_loss: 0.1991\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.8906 - loss: 0.2740 - val_accuracy: 0.8490 - val_loss: 0.3144\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8930 - loss: 0.2611 - val_accuracy: 0.9193 - val_loss: 0.2177\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.8955 - loss: 0.2612 - val_accuracy: 0.9281 - val_loss: 0.1861\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.8975 - loss: 0.2596 - val_accuracy: 0.8926 - val_loss: 0.2520\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8861 - loss: 0.2713 - val_accuracy: 0.9398 - val_loss: 0.1664\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9121 - loss: 0.2240 - val_accuracy: 0.8528 - val_loss: 0.3171\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.9013 - loss: 0.2499 - val_accuracy: 0.8696 - val_loss: 0.3139\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.9031 - loss: 0.2414 - val_accuracy: 0.8150 - val_loss: 0.4198\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.9078 - loss: 0.2330 - val_accuracy: 0.9103 - val_loss: 0.2111\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 8ms/step - accuracy: 0.8908 - loss: 0.2645 - val_accuracy: 0.9319 - val_loss: 0.1729\n\n\nNow that we have completed all of work to run and populate our model with our training data, we can create a visualization to the accuracy of our model. Similar to last blog post, we will use the package matplotlib to create line graphs that compare the validation accuracy with the model accuracy. Please run the code cell below to create our plot for Model 1.\n\n#plot accuracy of training data vs. validation data\nplt.plot(history_1.history['accuracy'], label='Training Accuracy')\nplt.plot(history_1.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 1: Article Titles')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe validation accuracy is consistantly higher than the training accuracy. The validation accuract flucutates for periods but never experiences dramatic drops. In all, the accuracy never seems to exceed a rate of 95%\n\n\n\nIn our second model, our only input will be the article text. This means that the text of each article will be used by our model to determine how we classify the given article. We will define the following layers below to create our model. Please run the code cell below to create the second model.\n\n#defining our model when our input is the text of the article\ntext_input = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"text\")\n\ntext_features = title_vectorize_layer(text_input)\ntext_features = shared_embedding_layer(text_features)\ntext_features = layers.Dropout(0.5)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dense(64, activation='relu')(text_features)\ntext_features = layers.Dropout(0.3)(text_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\ntext_output = layers.Dense(1, activation='sigmoid', name=\"text_output\")(text_features)\n\n#create nodel 2\nmodel_2 = tf.keras.Model(inputs=text_input, outputs=text_output)\nmodel_2.summary()\n\nModel: \"functional_3\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ text (InputLayer)                    │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (Dropout)                  │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_1           │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (Dense)                      │ (None, 64)                  │             256 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (Dropout)                  │ (None, 64)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (Dense)                      │ (None, 32)                  │           2,080 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_output (Dense)                  │ (None, 1)                   │              33 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 8,369 (32.69 KB)\n\n\n\n Trainable params: 8,369 (32.69 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNow that we have created Model 2, we can visualize and further understand the structure of the model by creating a flowchart. The following code was provided in the blog post instructions to complete this step. Please run the code below.\n\n#visualize model 2 (flowchart)\nutils.plot_model(model_2, \"model_2.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nPlease run the code cell below to compile our data.\n\n#compile with the training and validation data\nmodel_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhistory_2 = model_2.fit(train, epochs=20, validation_data=val)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 13ms/step - accuracy: 0.6636 - loss: 0.6200 - val_accuracy: 0.9126 - val_loss: 0.2883\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.8958 - loss: 0.2797 - val_accuracy: 0.9350 - val_loss: 0.2134\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9223 - loss: 0.2190 - val_accuracy: 0.9425 - val_loss: 0.1837\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9345 - loss: 0.1913 - val_accuracy: 0.9476 - val_loss: 0.1657\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9405 - loss: 0.1748 - val_accuracy: 0.9517 - val_loss: 0.1534\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9445 - loss: 0.1633 - val_accuracy: 0.9541 - val_loss: 0.1445\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9478 - loss: 0.1502 - val_accuracy: 0.9595 - val_loss: 0.1355\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 19ms/step - accuracy: 0.9473 - loss: 0.1415 - val_accuracy: 0.9602 - val_loss: 0.1297\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9516 - loss: 0.1382 - val_accuracy: 0.9631 - val_loss: 0.1233\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9521 - loss: 0.1304 - val_accuracy: 0.9573 - val_loss: 0.1260\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9552 - loss: 0.1270 - val_accuracy: 0.9573 - val_loss: 0.1238\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 17ms/step - accuracy: 0.9547 - loss: 0.1260 - val_accuracy: 0.9661 - val_loss: 0.1118\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9601 - loss: 0.1112 - val_accuracy: 0.9586 - val_loss: 0.1211\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9575 - loss: 0.1181 - val_accuracy: 0.9679 - val_loss: 0.1063\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9607 - loss: 0.1107 - val_accuracy: 0.9683 - val_loss: 0.1035\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9592 - loss: 0.1133 - val_accuracy: 0.9629 - val_loss: 0.1120\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9631 - loss: 0.1033 - val_accuracy: 0.9670 - val_loss: 0.1071\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9590 - loss: 0.1092 - val_accuracy: 0.9697 - val_loss: 0.0982\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9636 - loss: 0.1029 - val_accuracy: 0.9721 - val_loss: 0.0952\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9657 - loss: 0.0982 - val_accuracy: 0.9703 - val_loss: 0.0977\n\n\nNow that we have completed all of work to run and populate our model with our training data, we can create a visualization to the accuracy of our model. Similar to last blog post, we will use the package matplotlib to create line graphs that compare the validation accuracy with the model accuracy. Please run the code cell below to create our plot for Model 2.\n\n#plot validation accuract vs. training accuracy of modele 2\nplt.plot(history_2.history['accuracy'], label='Training Accuracy')\nplt.plot(history_2.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 2: Article Texts')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe validation accuracy for model 2 was consistently higher than the training accuracy. In general, we can see rates under about 96%.\n\n\n\nIn our third model, our inputs will be both the article text and the article title. This will work as a sort of combination of the first and second model. In theory, we should expect this model to be the most accurate as it takes the most information as inputs and allows the model to make the best decision. Please run the code cell below to create the third model.\n\nmain = layers.concatenate([title_features, text_features], axis = 1)\n\n\nmain = layers.Dense(32, activation='relu')(main)\nmain = layers.Dense(32, activation='relu')(main)\nmain = layers.Dropout(0.3)(main)\noutput = layers.Dense(1, name = \"genre\")(main)\n\n\n#define our model so that our inputs are both article title and article text\nmodel_3 = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = output\n)\n\nmodel_3.summary()\n\nModel: \"functional_5\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)              ┃ Output Shape           ┃        Param # ┃ Connected to           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)        │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text (InputLayer)         │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization        │ (None, 500)            │              0 │ title[0][0],           │\n│ (TextVectorization)       │                        │                │ text[0][0]             │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding (Embedding)     │ (None, 500, 3)         │          6,000 │ text_vectorization[0]… │\n│                           │                        │                │ text_vectorization[1]… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (Dropout)         │ (None, 500, 3)         │              0 │ embedding[0][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_2 (Dropout)       │ (None, 500, 3)         │              0 │ embedding[1][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d  │ (None, 3)              │              0 │ dropout[0][0]          │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (None, 3)              │              0 │ dropout_2[0][0]        │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (Dense)             │ (None, 64)             │            256 │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (Dense)           │ (None, 64)             │            256 │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_1 (Dropout)       │ (None, 64)             │              0 │ dense[0][0]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_3 (Dropout)       │ (None, 64)             │              0 │ dense_2[0][0]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (Dense)           │ (None, 32)             │          2,080 │ dropout_1[0][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (Dense)           │ (None, 32)             │          2,080 │ dropout_3[0][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate (Concatenate) │ (None, 64)             │              0 │ dense_1[0][0],         │\n│                           │                        │                │ dense_3[0][0]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_4 (Dense)           │ (None, 32)             │          2,080 │ concatenate[0][0]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_5 (Dense)           │ (None, 32)             │          1,056 │ dense_4[0][0]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_4 (Dropout)       │ (None, 32)             │              0 │ dense_5[0][0]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ genre (Dense)             │ (None, 1)              │             33 │ dropout_4[0][0]        │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n\n\n\n Total params: 13,841 (54.07 KB)\n\n\n\n Trainable params: 13,841 (54.07 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNow that we have created Model 3, we can visualize and further understand the structure of the model by creating a flowchart. The following code was provided in the blog post instructions to complete this step. Please run the code below.\n\n#create visualization of model 3 (flowchart)\nutils.plot_model(model_3, \"model_3.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nPlease run the code cell below to compile our data.\n\n#compile with the training and validation data\nmodel_3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhistory_3 = model_3.fit(train, epochs=20, validation_data=val)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 19ms/step - accuracy: 0.8611 - loss: 0.7254 - val_accuracy: 0.9728 - val_loss: 0.1327\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9535 - loss: 0.1890 - val_accuracy: 0.9706 - val_loss: 0.1583\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9657 - loss: 0.1542 - val_accuracy: 0.9519 - val_loss: 0.1626\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.8256 - loss: 1.4467 - val_accuracy: 0.9721 - val_loss: 0.1703\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 16ms/step - accuracy: 0.9587 - loss: 0.1797 - val_accuracy: 0.9708 - val_loss: 0.1766\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9604 - loss: 0.1686 - val_accuracy: 0.9719 - val_loss: 0.1447\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9672 - loss: 0.1466 - val_accuracy: 0.9690 - val_loss: 0.1568\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 16ms/step - accuracy: 0.9552 - loss: 0.2289 - val_accuracy: 0.9742 - val_loss: 0.1517\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 16ms/step - accuracy: 0.8895 - loss: 0.6017 - val_accuracy: 0.9742 - val_loss: 0.1415\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9558 - loss: 0.1524 - val_accuracy: 0.9164 - val_loss: 0.2347\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9606 - loss: 0.1697 - val_accuracy: 0.9780 - val_loss: 0.1261\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9693 - loss: 0.1424 - val_accuracy: 0.5158 - val_loss: 7.6405\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.7216 - loss: 3.3430 - val_accuracy: 0.9697 - val_loss: 0.1836\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9695 - loss: 0.1480 - val_accuracy: 0.9757 - val_loss: 0.1399\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 22ms/step - accuracy: 0.9597 - loss: 0.2133 - val_accuracy: 0.8656 - val_loss: 0.4425\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 15ms/step - accuracy: 0.9394 - loss: 0.3012 - val_accuracy: 0.7791 - val_loss: 1.2603\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 16ms/step - accuracy: 0.6832 - loss: 1.3117 - val_accuracy: 0.8984 - val_loss: 0.3200\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 16ms/step - accuracy: 0.8509 - loss: 0.4206 - val_accuracy: 0.9672 - val_loss: 0.1770\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9211 - loss: 0.2654 - val_accuracy: 0.9708 - val_loss: 0.1934\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 19ms/step - accuracy: 0.9428 - loss: 0.2238 - val_accuracy: 0.9609 - val_loss: 0.2176\n\n\nNow that we have completed all of work to run and populate our model with our training data, we can create a visualization to the accuracy of our model. Similar to last blog post, we will use the package matplotlib to create line graphs that compare the validation accuracy with the model accuracy. Please run the code cell below to create our plot for Model 3.\n\n#plot validation accuracy vs. training accuracy for model 3\nplt.plot(history_3.history['accuracy'], label='Training Accuracy')\nplt.plot(history_3.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 3: Titles and Text')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThis model experienced some overfitting and lots of flucuations. However, it did see a very high accuracy and regularly reached a rate that exceeded 97%.\n\n\n\nWe have successfully created three different models! Using the model that performed the best, model 3, we will use a new dataset and classify these articles as fake news or not. This dataset will be different than the one we used to train our three models.\nTo start off, please run the code cell below to import our new dataset.\n\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\n\ndf_test = pd.read_csv(test_url)\ndf_test.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n420\nCNN And MSNBC Destroy Trump, Black Out His Fa...\nDonald Trump practically does something to cri...\n1\n\n\n1\n14902\nExclusive: Kremlin tells companies to deliver ...\nThe Kremlin wants good news. The Russian lead...\n0\n\n\n2\n322\nGolden State Warriors Coach Just WRECKED Trum...\nOn Saturday, the man we re forced to call Pre...\n1\n\n\n3\n16108\nPutin opens monument to Stalin's victims, diss...\nPresident Vladimir Putin inaugurated a monumen...\n0\n\n\n4\n10304\nBREAKING: DNC HACKER FIRED For Bank Fraud…Blam...\nApparently breaking the law and scamming the g...\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nNow that we have uploaded our new data, we will organize it into our dataset by using the same function we defined for our training dataset.\n\ntest_ds = make_dataset(df_test)\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nFinally, we will test the accuracy of model 3 using this data.\n\ntest_loss, test_accuracy = model_3.evaluate(test_ds)\nprint(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.9595 - loss: 0.2043\nTest Accuracy: 95.96%\n\n\nThis model had an accuracy of about 97% for this new set of data.\n\n\n\nIn the last section of today’s blog post, we will visualize the embedding of our third model. This requires us to use PCA from sklearn. Please run the code cell below.\n\nweights = model_3.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer\nvocab = title_vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nweights = pca.fit_transform(weights)\n\nembedding_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\n\nFinally, import our plotly package and make a visualization in the form of the scatterplot\n\n#necessary packages\nimport plotly.express as px\nimport numpy as np\n\n#create visualization\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 5,\n                 hover_name = \"word\",\n                 title='Word Embeddings Visualization')\n\nfig.update_layout(\n    xaxis=dict(title='Principal Component 1'),\n    yaxis=dict(title='Principal Component 2'),\n)\n\nfig.show()\n\n\n\n\ntext\nWords in our articles are closer to each other in this visualization if they are thought to have similar meetings. This scatterplot helps further our understanding of how our model determines what is clasified as fake news. Logically, I believe our plot makes sense as words that I would anticipate to be related are plotted next to each other such as “rule” and “court”. Other words such as “blacklivesmatter”, “cnn”, and “boycotts” were also plotted near one another. In my opinion, these words are correlated as they are all associated with the democratic party. In all, this allowed me to ensure my plot was accurate.\nThrough this post, I hope you have a better understanding of how to use machine learning in the form of language processors and text classification. As fake news is a growing phenomena in the US, it is very important that we as a country find ways to combat the rampant spread of misinformation. Thank you for reading!"
  },
  {
    "objectID": "posts/HW6/index.html#fake-news-classification-using-keras",
    "href": "posts/HW6/index.html#fake-news-classification-using-keras",
    "title": "Fact or Fiction? Fake News Classification Using Keras",
    "section": "",
    "text": "The spread of false information, also reffered to as “fake news”, is a contant plague on modern life. Fake news can be extremely harmful to our society as a whole. But with today’s technology, we have tools that could combat the force that is fake news. In today’s blog post, we will use Keras and Machine Learning to classify articles as being truthful or not.\nAs always, we must start by importing the necassary packages and upgrading our Keras. Please run the following two code cells before we start uploading our dataset and defining our model.\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\nCollecting keras\n  Downloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 6.9 MB/s eta 0:00:00\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nCollecting namex (from keras)\n  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\nInstalling collected packages: namex, keras\n  Attempting uninstall: keras\n    Found existing installation: keras 2.15.0\n    Uninstalling keras-2.15.0:\n      Successfully uninstalled keras-2.15.0\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.15.0 requires keras&lt;2.16,&gt;=2.15.0, but you have keras 3.0.5 which is incompatible.\nSuccessfully installed keras-3.0.5 namex-0.0.7\n\n\n\n!pip install nltk\n\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\nRequirement already satisfied: regex&gt;=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n\n\n\n#import all of the necessary packages\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import stopwords\nimport keras\nimport re\nimport string\nfrom keras import layers, losses\nfrom keras.layers import TextVectorization\nfrom keras import utils\n\nNow that we have imported all of our packages, we can start by acquiring our training data. Our data is in the form of different articles that we accessed from Kaggle. By running the code cell below, we can upload the link to the data and organize it into a dataframe using pandas. Please run the code cell below and see the resulting table for our\n\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\n\n#csv file into dataframe\ndf = pd.read_csv(train_url)\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nWe now have successfully accessed our data that we will use to train our model! Before we continue, we must organize this data into a dataset. Using the following function, we will change all text to lowercase letters, remove stopwords, and create a dataset with inputs title and text and outputs a fake column. It is very important to have all of the necessary packages in order for us to filter out stop words and make a dataframe.\nPlease run the code cell below to define the make_dataset function.\n\ndef make_dataset(df):\n    \"\"\"\n    Function will create a dataset of our data and organize it with the given conditions\n\n    Arguments:\n      df : pandas DataFrame containing columns: title, text, and fake\n\n    Returns:\n      dataset : the dataset we created with the specified conditions (TensorFlow)\n    \"\"\"\n\n    #lowercase text\n    df['text'] = df['text'].apply(lambda x: x.lower())\n\n    #remove stopwords\n    nltk.download('stopwords')\n    stop = stopwords.words('english')\n    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n\n\n    title_tensor = tf.constant(df['title'].values, dtype=tf.string)\n    text_tensor = tf.constant(df['text'].values, dtype=tf.string)\n    fake_tensor = tf.constant(df['fake'].values, dtype=tf.int32)\n    dataset = tf.data.Dataset.from_tensor_slices(({\"title\": title_tensor, \"text\": text_tensor}, fake_tensor))\n\n    dataset = dataset.batch(100)\n\n    #outputs our dataset\n    return dataset\n\nNow that our function is defined, we can use our training dataframe to output a dataset. Please run the code cell below to create a dataset from our training dataframe.\n\n#create dataset using training data (see above)\nds = make_dataset(df)\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\n\nFor our next step, we will work with the validation set. This will involve taking 20% of our training data set to use for the validation.\n\n#create validation set\nds = ds.shuffle(buffer_size = len(ds), reshuffle_each_iteration=False)\nval_size = int(0.2 * len(ds))\n\nval = ds.take(val_size)\ntrain = ds.skip(val_size)\n\n\nlen(train), len(val)\n\n(180, 45)\n\n\nThe base rate is used to determine the accuracy of the model. It allows us to find a value to compare the results of our model to. Please run the code cell below to determine the base rate for our dataset.\n\n#determine the base rate for our training dataset\nfake_count = 0\ntotal_count = 0\n\nfor _, labels in train:\n    fake_count += tf.reduce_sum(labels).numpy()\n    total_count += len(labels)\n\nbase_rate = fake_count / total_count\nprint(\"Base rate:\", base_rate)\n\nBase rate: 0.5237222222222222\n\n\nAfter printing the base rate, ppreapre the text vectorization for the model. The following code cell was given in the Homework 6 blog post. Please run the code cell below before proceeding with the models.\n\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\ntitle_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\n\nThe following cell will also add an embedding layer that will be used in our models. This layer will access the word embeddings that we will use to determine whether or not our articles can be classified as fake news. Please run the code cell below.\n\nshared_embedding_layer = layers.Embedding(size_vocabulary, 3, name=\"embedding\")\n\nWe can now start on defining our three models that will be used to determine the validity of different news sources."
  },
  {
    "objectID": "posts/HW6/index.html#model-1",
    "href": "posts/HW6/index.html#model-1",
    "title": "Fact or Fiction? Fake News Classification Using Keras",
    "section": "",
    "text": "In our first model, our only input will be the article title. This means that the article title will be used by our model to determine how we classify the given article. We will define the following layers below to create our model. Please run the code cell below to create the first model.\n\n#defining the model when the input is our article title\ntitle_input = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"title\")\n\ntitle_features = title_vectorize_layer(title_input)\ntitle_features = shared_embedding_layer(title_features)\ntitle_features = layers.Dropout(0.5)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dense(64, activation='relu')(title_features)\ntitle_features = layers.Dropout(0.3)(title_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\n\ntitle_output = layers.Dense(1, activation='sigmoid', name=\"title_output\")(title_features)\n\n#create model 1\nmodel_1 = tf.keras.Model(inputs=title_input, outputs=title_output)\nmodel_1.summary()\n\nModel: \"functional_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)                   │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (Dropout)                    │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d             │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (Dense)                        │ (None, 64)                  │             256 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (Dropout)                  │ (None, 64)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (Dense)                      │ (None, 32)                  │           2,080 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ title_output (Dense)                 │ (None, 1)                   │              33 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 8,369 (32.69 KB)\n\n\n\n Trainable params: 8,369 (32.69 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNow that we have created Model 1, we can visualize and further understand the structure of the model by creating a flowchart. The following code was provided in the blog post instructions to complete this step. Please run the code below.\n\n#model visualization (flowchart)\nutils.plot_model(model_1, \"model_1.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nPlease run the code cell below to compile our data.\n\n#compile with training and valiation data\nmodel_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhistory_1 = model_1.fit(train, epochs=20, validation_data=val)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 8ms/step - accuracy: 0.5165 - loss: 0.6926 - val_accuracy: 0.5199 - val_loss: 0.6919\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.5219 - loss: 0.6917 - val_accuracy: 0.5923 - val_loss: 0.6859\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 6ms/step - accuracy: 0.6044 - loss: 0.6717 - val_accuracy: 0.7824 - val_loss: 0.5390\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 7ms/step - accuracy: 0.7453 - loss: 0.5289 - val_accuracy: 0.8505 - val_loss: 0.3977\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.8039 - loss: 0.4297 - val_accuracy: 0.8863 - val_loss: 0.3155\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.8196 - loss: 0.4045 - val_accuracy: 0.9018 - val_loss: 0.2730\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8544 - loss: 0.3353 - val_accuracy: 0.9189 - val_loss: 0.2363\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8662 - loss: 0.3100 - val_accuracy: 0.8824 - val_loss: 0.2688\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8515 - loss: 0.3422 - val_accuracy: 0.9310 - val_loss: 0.2065\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8834 - loss: 0.2849 - val_accuracy: 0.9249 - val_loss: 0.1991\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.8906 - loss: 0.2740 - val_accuracy: 0.8490 - val_loss: 0.3144\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8930 - loss: 0.2611 - val_accuracy: 0.9193 - val_loss: 0.2177\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.8955 - loss: 0.2612 - val_accuracy: 0.9281 - val_loss: 0.1861\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.8975 - loss: 0.2596 - val_accuracy: 0.8926 - val_loss: 0.2520\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8861 - loss: 0.2713 - val_accuracy: 0.9398 - val_loss: 0.1664\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9121 - loss: 0.2240 - val_accuracy: 0.8528 - val_loss: 0.3171\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.9013 - loss: 0.2499 - val_accuracy: 0.8696 - val_loss: 0.3139\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.9031 - loss: 0.2414 - val_accuracy: 0.8150 - val_loss: 0.4198\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.9078 - loss: 0.2330 - val_accuracy: 0.9103 - val_loss: 0.2111\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 8ms/step - accuracy: 0.8908 - loss: 0.2645 - val_accuracy: 0.9319 - val_loss: 0.1729\n\n\nNow that we have completed all of work to run and populate our model with our training data, we can create a visualization to the accuracy of our model. Similar to last blog post, we will use the package matplotlib to create line graphs that compare the validation accuracy with the model accuracy. Please run the code cell below to create our plot for Model 1.\n\n#plot accuracy of training data vs. validation data\nplt.plot(history_1.history['accuracy'], label='Training Accuracy')\nplt.plot(history_1.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 1: Article Titles')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe validation accuracy is consistantly higher than the training accuracy. The validation accuract flucutates for periods but never experiences dramatic drops. In all, the accuracy never seems to exceed a rate of 95%"
  },
  {
    "objectID": "posts/HW6/index.html#model-2",
    "href": "posts/HW6/index.html#model-2",
    "title": "Fact or Fiction? Fake News Classification Using Keras",
    "section": "",
    "text": "In our second model, our only input will be the article text. This means that the text of each article will be used by our model to determine how we classify the given article. We will define the following layers below to create our model. Please run the code cell below to create the second model.\n\n#defining our model when our input is the text of the article\ntext_input = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"text\")\n\ntext_features = title_vectorize_layer(text_input)\ntext_features = shared_embedding_layer(text_features)\ntext_features = layers.Dropout(0.5)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dense(64, activation='relu')(text_features)\ntext_features = layers.Dropout(0.3)(text_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\ntext_output = layers.Dense(1, activation='sigmoid', name=\"text_output\")(text_features)\n\n#create nodel 2\nmodel_2 = tf.keras.Model(inputs=text_input, outputs=text_output)\nmodel_2.summary()\n\nModel: \"functional_3\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ text (InputLayer)                    │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (Dropout)                  │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_1           │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (Dense)                      │ (None, 64)                  │             256 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (Dropout)                  │ (None, 64)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (Dense)                      │ (None, 32)                  │           2,080 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_output (Dense)                  │ (None, 1)                   │              33 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 8,369 (32.69 KB)\n\n\n\n Trainable params: 8,369 (32.69 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNow that we have created Model 2, we can visualize and further understand the structure of the model by creating a flowchart. The following code was provided in the blog post instructions to complete this step. Please run the code below.\n\n#visualize model 2 (flowchart)\nutils.plot_model(model_2, \"model_2.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nPlease run the code cell below to compile our data.\n\n#compile with the training and validation data\nmodel_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhistory_2 = model_2.fit(train, epochs=20, validation_data=val)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 13ms/step - accuracy: 0.6636 - loss: 0.6200 - val_accuracy: 0.9126 - val_loss: 0.2883\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.8958 - loss: 0.2797 - val_accuracy: 0.9350 - val_loss: 0.2134\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9223 - loss: 0.2190 - val_accuracy: 0.9425 - val_loss: 0.1837\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9345 - loss: 0.1913 - val_accuracy: 0.9476 - val_loss: 0.1657\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9405 - loss: 0.1748 - val_accuracy: 0.9517 - val_loss: 0.1534\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9445 - loss: 0.1633 - val_accuracy: 0.9541 - val_loss: 0.1445\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9478 - loss: 0.1502 - val_accuracy: 0.9595 - val_loss: 0.1355\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 19ms/step - accuracy: 0.9473 - loss: 0.1415 - val_accuracy: 0.9602 - val_loss: 0.1297\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9516 - loss: 0.1382 - val_accuracy: 0.9631 - val_loss: 0.1233\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9521 - loss: 0.1304 - val_accuracy: 0.9573 - val_loss: 0.1260\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9552 - loss: 0.1270 - val_accuracy: 0.9573 - val_loss: 0.1238\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 17ms/step - accuracy: 0.9547 - loss: 0.1260 - val_accuracy: 0.9661 - val_loss: 0.1118\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9601 - loss: 0.1112 - val_accuracy: 0.9586 - val_loss: 0.1211\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9575 - loss: 0.1181 - val_accuracy: 0.9679 - val_loss: 0.1063\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9607 - loss: 0.1107 - val_accuracy: 0.9683 - val_loss: 0.1035\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9592 - loss: 0.1133 - val_accuracy: 0.9629 - val_loss: 0.1120\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9631 - loss: 0.1033 - val_accuracy: 0.9670 - val_loss: 0.1071\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9590 - loss: 0.1092 - val_accuracy: 0.9697 - val_loss: 0.0982\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9636 - loss: 0.1029 - val_accuracy: 0.9721 - val_loss: 0.0952\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9657 - loss: 0.0982 - val_accuracy: 0.9703 - val_loss: 0.0977\n\n\nNow that we have completed all of work to run and populate our model with our training data, we can create a visualization to the accuracy of our model. Similar to last blog post, we will use the package matplotlib to create line graphs that compare the validation accuracy with the model accuracy. Please run the code cell below to create our plot for Model 2.\n\n#plot validation accuract vs. training accuracy of modele 2\nplt.plot(history_2.history['accuracy'], label='Training Accuracy')\nplt.plot(history_2.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 2: Article Texts')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe validation accuracy for model 2 was consistently higher than the training accuracy. In general, we can see rates under about 96%."
  },
  {
    "objectID": "posts/HW6/index.html#model-3",
    "href": "posts/HW6/index.html#model-3",
    "title": "Fact or Fiction? Fake News Classification Using Keras",
    "section": "",
    "text": "In our third model, our inputs will be both the article text and the article title. This will work as a sort of combination of the first and second model. In theory, we should expect this model to be the most accurate as it takes the most information as inputs and allows the model to make the best decision. Please run the code cell below to create the third model.\n\nmain = layers.concatenate([title_features, text_features], axis = 1)\n\n\nmain = layers.Dense(32, activation='relu')(main)\nmain = layers.Dense(32, activation='relu')(main)\nmain = layers.Dropout(0.3)(main)\noutput = layers.Dense(1, name = \"genre\")(main)\n\n\n#define our model so that our inputs are both article title and article text\nmodel_3 = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = output\n)\n\nmodel_3.summary()\n\nModel: \"functional_5\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)              ┃ Output Shape           ┃        Param # ┃ Connected to           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)        │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text (InputLayer)         │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization        │ (None, 500)            │              0 │ title[0][0],           │\n│ (TextVectorization)       │                        │                │ text[0][0]             │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding (Embedding)     │ (None, 500, 3)         │          6,000 │ text_vectorization[0]… │\n│                           │                        │                │ text_vectorization[1]… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (Dropout)         │ (None, 500, 3)         │              0 │ embedding[0][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_2 (Dropout)       │ (None, 500, 3)         │              0 │ embedding[1][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d  │ (None, 3)              │              0 │ dropout[0][0]          │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (None, 3)              │              0 │ dropout_2[0][0]        │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (Dense)             │ (None, 64)             │            256 │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (Dense)           │ (None, 64)             │            256 │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_1 (Dropout)       │ (None, 64)             │              0 │ dense[0][0]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_3 (Dropout)       │ (None, 64)             │              0 │ dense_2[0][0]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (Dense)           │ (None, 32)             │          2,080 │ dropout_1[0][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (Dense)           │ (None, 32)             │          2,080 │ dropout_3[0][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate (Concatenate) │ (None, 64)             │              0 │ dense_1[0][0],         │\n│                           │                        │                │ dense_3[0][0]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_4 (Dense)           │ (None, 32)             │          2,080 │ concatenate[0][0]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_5 (Dense)           │ (None, 32)             │          1,056 │ dense_4[0][0]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_4 (Dropout)       │ (None, 32)             │              0 │ dense_5[0][0]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ genre (Dense)             │ (None, 1)              │             33 │ dropout_4[0][0]        │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n\n\n\n Total params: 13,841 (54.07 KB)\n\n\n\n Trainable params: 13,841 (54.07 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNow that we have created Model 3, we can visualize and further understand the structure of the model by creating a flowchart. The following code was provided in the blog post instructions to complete this step. Please run the code below.\n\n#create visualization of model 3 (flowchart)\nutils.plot_model(model_3, \"model_3.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nPlease run the code cell below to compile our data.\n\n#compile with the training and validation data\nmodel_3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhistory_3 = model_3.fit(train, epochs=20, validation_data=val)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 19ms/step - accuracy: 0.8611 - loss: 0.7254 - val_accuracy: 0.9728 - val_loss: 0.1327\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9535 - loss: 0.1890 - val_accuracy: 0.9706 - val_loss: 0.1583\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9657 - loss: 0.1542 - val_accuracy: 0.9519 - val_loss: 0.1626\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.8256 - loss: 1.4467 - val_accuracy: 0.9721 - val_loss: 0.1703\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 16ms/step - accuracy: 0.9587 - loss: 0.1797 - val_accuracy: 0.9708 - val_loss: 0.1766\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9604 - loss: 0.1686 - val_accuracy: 0.9719 - val_loss: 0.1447\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9672 - loss: 0.1466 - val_accuracy: 0.9690 - val_loss: 0.1568\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 16ms/step - accuracy: 0.9552 - loss: 0.2289 - val_accuracy: 0.9742 - val_loss: 0.1517\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 16ms/step - accuracy: 0.8895 - loss: 0.6017 - val_accuracy: 0.9742 - val_loss: 0.1415\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9558 - loss: 0.1524 - val_accuracy: 0.9164 - val_loss: 0.2347\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9606 - loss: 0.1697 - val_accuracy: 0.9780 - val_loss: 0.1261\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9693 - loss: 0.1424 - val_accuracy: 0.5158 - val_loss: 7.6405\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.7216 - loss: 3.3430 - val_accuracy: 0.9697 - val_loss: 0.1836\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9695 - loss: 0.1480 - val_accuracy: 0.9757 - val_loss: 0.1399\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 22ms/step - accuracy: 0.9597 - loss: 0.2133 - val_accuracy: 0.8656 - val_loss: 0.4425\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 15ms/step - accuracy: 0.9394 - loss: 0.3012 - val_accuracy: 0.7791 - val_loss: 1.2603\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 16ms/step - accuracy: 0.6832 - loss: 1.3117 - val_accuracy: 0.8984 - val_loss: 0.3200\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 16ms/step - accuracy: 0.8509 - loss: 0.4206 - val_accuracy: 0.9672 - val_loss: 0.1770\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9211 - loss: 0.2654 - val_accuracy: 0.9708 - val_loss: 0.1934\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 19ms/step - accuracy: 0.9428 - loss: 0.2238 - val_accuracy: 0.9609 - val_loss: 0.2176\n\n\nNow that we have completed all of work to run and populate our model with our training data, we can create a visualization to the accuracy of our model. Similar to last blog post, we will use the package matplotlib to create line graphs that compare the validation accuracy with the model accuracy. Please run the code cell below to create our plot for Model 3.\n\n#plot validation accuracy vs. training accuracy for model 3\nplt.plot(history_3.history['accuracy'], label='Training Accuracy')\nplt.plot(history_3.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model 3: Titles and Text')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThis model experienced some overfitting and lots of flucuations. However, it did see a very high accuracy and regularly reached a rate that exceeded 97%."
  },
  {
    "objectID": "posts/HW6/index.html#testing-with-new-data",
    "href": "posts/HW6/index.html#testing-with-new-data",
    "title": "Fact or Fiction? Fake News Classification Using Keras",
    "section": "",
    "text": "We have successfully created three different models! Using the model that performed the best, model 3, we will use a new dataset and classify these articles as fake news or not. This dataset will be different than the one we used to train our three models.\nTo start off, please run the code cell below to import our new dataset.\n\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\n\ndf_test = pd.read_csv(test_url)\ndf_test.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n420\nCNN And MSNBC Destroy Trump, Black Out His Fa...\nDonald Trump practically does something to cri...\n1\n\n\n1\n14902\nExclusive: Kremlin tells companies to deliver ...\nThe Kremlin wants good news. The Russian lead...\n0\n\n\n2\n322\nGolden State Warriors Coach Just WRECKED Trum...\nOn Saturday, the man we re forced to call Pre...\n1\n\n\n3\n16108\nPutin opens monument to Stalin's victims, diss...\nPresident Vladimir Putin inaugurated a monumen...\n0\n\n\n4\n10304\nBREAKING: DNC HACKER FIRED For Bank Fraud…Blam...\nApparently breaking the law and scamming the g...\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nNow that we have uploaded our new data, we will organize it into our dataset by using the same function we defined for our training dataset.\n\ntest_ds = make_dataset(df_test)\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nFinally, we will test the accuracy of model 3 using this data.\n\ntest_loss, test_accuracy = model_3.evaluate(test_ds)\nprint(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.9595 - loss: 0.2043\nTest Accuracy: 95.96%\n\n\nThis model had an accuracy of about 97% for this new set of data."
  },
  {
    "objectID": "posts/HW6/index.html#visual-embedding",
    "href": "posts/HW6/index.html#visual-embedding",
    "title": "Fact or Fiction? Fake News Classification Using Keras",
    "section": "",
    "text": "In the last section of today’s blog post, we will visualize the embedding of our third model. This requires us to use PCA from sklearn. Please run the code cell below.\n\nweights = model_3.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer\nvocab = title_vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nweights = pca.fit_transform(weights)\n\nembedding_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\n\nFinally, import our plotly package and make a visualization in the form of the scatterplot\n\n#necessary packages\nimport plotly.express as px\nimport numpy as np\n\n#create visualization\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 5,\n                 hover_name = \"word\",\n                 title='Word Embeddings Visualization')\n\nfig.update_layout(\n    xaxis=dict(title='Principal Component 1'),\n    yaxis=dict(title='Principal Component 2'),\n)\n\nfig.show()\n\n\n\n\ntext\nWords in our articles are closer to each other in this visualization if they are thought to have similar meetings. This scatterplot helps further our understanding of how our model determines what is clasified as fake news. Logically, I believe our plot makes sense as words that I would anticipate to be related are plotted next to each other such as “rule” and “court”. Other words such as “blacklivesmatter”, “cnn”, and “boycotts” were also plotted near one another. In my opinion, these words are correlated as they are all associated with the democratic party. In all, this allowed me to ensure my plot was accurate.\nThrough this post, I hope you have a better understanding of how to use machine learning in the form of language processors and text classification. As fake news is a growing phenomena in the US, it is very important that we as a country find ways to combat the rampant spread of misinformation. Thank you for reading!"
  },
  {
    "objectID": "posts/HW3/index.html",
    "href": "posts/HW3/index.html",
    "title": "Website Development - Message Submission Website",
    "section": "",
    "text": "Welcome! Today we will explore website development using Flask. Flask is a great tool that we can use to create original websites with interactive features.\nFor this specific website, we will create a page that allows users to submit a message. After submitting, they will be able to view other messages and see a little note.\n\n\nAs always, we first must import the necessary Flask packages into our Python file. Please see the code cell below for the specific imports. Next, we will create a file that allows a user to input messages. The message will then be put into our database. This code can also create a database if one does not already exist. We do this by using SQL commands. We will be able to access these messages in our next steps. Please look at the code cell below for our full function definition.\n\nfrom flask import Flask\nimport sqlite3\nfrom flask import render_template, request\nimport random\nfrom flask import g\n\napp = Flask(__name__)\n\n\n#get message database\ndef get_message_db():\n    if 'message_db' not in g:\n        g.message_db = sqlite3.connect('messages_db.sqlite')\n    cursor = g.message_db.cursor()\n  \n    cursor.execute('CREATE TABLE IF NOT EXISTS messages (id INTEGER PRIMARY KEY, handle TEXT, message TEXT)')\n    return g.message_db\n\n\n\n\nWe will use the specific function to take in the message the user writes. The function connects to database by calling the function we defined above.\nThis function will then store both the inserted message and the user’s ID or name into the table we created above. Please follow the code below to create the function.\n\n#create insert function\ndef insert_message(request): \n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute('INSERT INTO messages (handle, message) VALUES (?, ?)', (request.form['user'], request.form['message']))\n    db.commit() \n    db.close()\n\n\n\n\nThis function will be used to create random messages that we can contain in our database. This is a necessary step as we are not actually having users submit to our site at this point. We will start by connecting to our database that we have worked with in the first two functions. Nextm we will use SQL to select the messages and the ID’s of the users. We will then use the random library to create a random number of messages. For this demo, we will let the number of random messages equal 5. Please follow the code below to correctly execute the function\n\n#random message function \ndef random_messages(n):\n    #add message to the database\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute('SELECT * FROM messages')\n    messages = cursor.fetchall()\n  \n    db.close()\n\n    #return our result\n    return random.sample(messages, min(n, len(messages)))\n\n\n\n\nThe route functions allow us to directly connect to Flask. The routes are important as they allow the functions to correctly run in our URL. We will implement all of these functions in the same Python file as the ones listed above. First, we will add a line to create our web server.\nOur first route function will bring the user to the homepage. Please follow the code cell below to route to our homepage.\n\n#route from function to homepage    \n@app.route('/', methods=['GET'])\ndef home():\n    return render_template('base.html')\n\nThis is the starting point for all of the other HTML functions that we will create. We must have the following information contained in our base.html.\nNext, we will create the submit.html page using another route function. Please follow the cell below to create our route function.\n\n#route to submit page\n@app.route('/submit', methods=['GET', 'POST'])\ndef submit():\n    message = None\n    user = None\n    if request.method == 'POST':\n        user = request.form['user']\n        message = request.form['message']\n        insert_message(request)\n    return render_template('submit.html', user=user, message=message)\n\nIf our submit.html page and our route function has been set up correctly, we should see the following page.\nSimilarly, we must also create a page that allows us to view other submitted messages. This next route function will render the view.html page that will show our five random entries (created in Step 3). The function will resemble our other route functions and use the command ‘GET’ as well. Please run the code cell below to implement this function.\n\n#route to view message page \n@app.route('/messages', methods=['GET'])\ndef messages(): \n    #generate random messages\n    messages = random_messages(5)\n    return render_template('view.html', messages=messages)\n\nAlso, please ensure that the view.html function is created with the following details.\nWe will now be able to see messages submitted by other users (or our random generator).\nFinally, we will customize our website by creating style.css. Here, I changed the color to purples. I listed all of the code I used to create my html files below.\n\nwith open('base.html', 'w') as f:\n    html_content = \"\"\"\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;\n        &lt;title&gt;{% block title %}Message Bank{% endblock %}&lt;/title&gt;\n        &lt;link rel = \"stylesheet\" type = \"text/css\" href=\"{{ url_for('statis', filename='style.css') }}\"&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;div class=\"center\"&gt;\n        {% block header %}\n        &lt;h1&gt;Welcome to the message bank!&lt;/h1&gt;\n        &lt;nav&gt;\n            &lt;a href=\"/submit\"&gt;Submit Message&lt;/a&gt; |\n            &lt;a href=\"/messages\"&gt;View Messages&lt;/a&gt;\n        &lt;/nav&gt;\n        {% endblock %}\n        {% block content %}\n        {% endblock %}\n        &lt;/div&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \"\"\"\n    f.write(html_content)\n\nwith open('submit.html', 'w') as f:\n    html_content = \"\"\"\n    {% extends \"base.html\" %}\n    {% block content %}\n    &lt;h2&gt;Submit your message below:&lt;/h2&gt;\n    &lt;form method=\"POST\"&gt; \n        &lt;label for=\"message\"&gt;Your message:&lt;/label&gt;&lt;br&gt;\n        &lt;input type=\"text\" name=\"user_message\"&gt;Your name or handle:&lt;/label&gt;&lt;br&gt;\n        &lt;input type=\"submit\" value=\"Submit\"&gt;\n    &lt;/form&gt;\n    {% if message %}\n    &lt;p&gt;Thank you for your submission, {{ user }}!&lt;/p&gt;\n    {% endif %}\n    {% endblock %}\n    \"\"\"\n    f.write(html_content)\n\nwith open('view.html', 'w') as f:\n    html_content = \"\"\"\n    {% extends \"base.html\" %}\n    {% block content %}\n    &lt;h1&gt;Messages&lt;/hi&gt;\n    &lt;u1&gt;\n    {% for message in messages %}\n        &lt;li&gt;{{ message[1] }}: {{ message[2] }}&lt;/li&gt;\n    {% endfor %}\n    &lt;/ul&gt;\n    {% endblock %}\n    \"\"\"\n    f.write(html_content)\n\nwith open('style.css', 'w') as f:\n    html_content = \"\"\"\n    html {\n        font-family: cursive; \n        background-color: rgb(250, 209, 228); \n        padding: 1rem; \n    }\n\n    body {\n        background-color: purple; \n        font-family: san-serif; \n        color: darkpurple\n    }\n\n    .center {\n        display: flex; \n        flex-direction: column; \n        align-items: center; \n        justify-content: center; \n        height: 100vh; \n        text-align: center; \n    }\n    \"\"\"\n    f.write(html_content)\n\n\n\n\nNow that we have created all of the necessary code for our website, let’s go to our page and use it. We first must run:\nconda activate PIC16B-24W; set FLASK_ENV=development; flask run\nin our terminal. Them, we can follow along and see each page of our website. Lastly, please go to the following link to find code for our website: https://github.com/woulfeig/PIC16B_REPOSITORY/blob/main/myblog/posts/app.py\nThank you for reading! Good luck creating your own unique and dynamic website."
  },
  {
    "objectID": "posts/HW3/index.html#step-1-creating-our-function-to-take-in-messages",
    "href": "posts/HW3/index.html#step-1-creating-our-function-to-take-in-messages",
    "title": "Website Development - Message Submission Website",
    "section": "",
    "text": "As always, we first must import the necessary Flask packages into our Python file. Please see the code cell below for the specific imports. Next, we will create a file that allows a user to input messages. The message will then be put into our database. This code can also create a database if one does not already exist. We do this by using SQL commands. We will be able to access these messages in our next steps. Please look at the code cell below for our full function definition.\n\nfrom flask import Flask\nimport sqlite3\nfrom flask import render_template, request\nimport random\nfrom flask import g\n\napp = Flask(__name__)\n\n\n#get message database\ndef get_message_db():\n    if 'message_db' not in g:\n        g.message_db = sqlite3.connect('messages_db.sqlite')\n    cursor = g.message_db.cursor()\n  \n    cursor.execute('CREATE TABLE IF NOT EXISTS messages (id INTEGER PRIMARY KEY, handle TEXT, message TEXT)')\n    return g.message_db"
  },
  {
    "objectID": "posts/HW3/index.html#step-2-create-a-function-that-allows-users-to-insert-messages",
    "href": "posts/HW3/index.html#step-2-create-a-function-that-allows-users-to-insert-messages",
    "title": "Website Development - Message Submission Website",
    "section": "",
    "text": "We will use the specific function to take in the message the user writes. The function connects to database by calling the function we defined above.\nThis function will then store both the inserted message and the user’s ID or name into the table we created above. Please follow the code below to create the function.\n\n#create insert function\ndef insert_message(request): \n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute('INSERT INTO messages (handle, message) VALUES (?, ?)', (request.form['user'], request.form['message']))\n    db.commit() \n    db.close()"
  },
  {
    "objectID": "posts/HW3/index.html#step-3-create-a-function-that-will-create-random-messages",
    "href": "posts/HW3/index.html#step-3-create-a-function-that-will-create-random-messages",
    "title": "Website Development - Message Submission Website",
    "section": "",
    "text": "This function will be used to create random messages that we can contain in our database. This is a necessary step as we are not actually having users submit to our site at this point. We will start by connecting to our database that we have worked with in the first two functions. Nextm we will use SQL to select the messages and the ID’s of the users. We will then use the random library to create a random number of messages. For this demo, we will let the number of random messages equal 5. Please follow the code below to correctly execute the function\n\n#random message function \ndef random_messages(n):\n    #add message to the database\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute('SELECT * FROM messages')\n    messages = cursor.fetchall()\n  \n    db.close()\n\n    #return our result\n    return random.sample(messages, min(n, len(messages)))"
  },
  {
    "objectID": "posts/HW3/index.html#step-4-create-the-route-functions",
    "href": "posts/HW3/index.html#step-4-create-the-route-functions",
    "title": "Website Development - Message Submission Website",
    "section": "",
    "text": "The route functions allow us to directly connect to Flask. The routes are important as they allow the functions to correctly run in our URL. We will implement all of these functions in the same Python file as the ones listed above. First, we will add a line to create our web server.\nOur first route function will bring the user to the homepage. Please follow the code cell below to route to our homepage.\n\n#route from function to homepage    \n@app.route('/', methods=['GET'])\ndef home():\n    return render_template('base.html')\n\nThis is the starting point for all of the other HTML functions that we will create. We must have the following information contained in our base.html.\nNext, we will create the submit.html page using another route function. Please follow the cell below to create our route function.\n\n#route to submit page\n@app.route('/submit', methods=['GET', 'POST'])\ndef submit():\n    message = None\n    user = None\n    if request.method == 'POST':\n        user = request.form['user']\n        message = request.form['message']\n        insert_message(request)\n    return render_template('submit.html', user=user, message=message)\n\nIf our submit.html page and our route function has been set up correctly, we should see the following page.\nSimilarly, we must also create a page that allows us to view other submitted messages. This next route function will render the view.html page that will show our five random entries (created in Step 3). The function will resemble our other route functions and use the command ‘GET’ as well. Please run the code cell below to implement this function.\n\n#route to view message page \n@app.route('/messages', methods=['GET'])\ndef messages(): \n    #generate random messages\n    messages = random_messages(5)\n    return render_template('view.html', messages=messages)\n\nAlso, please ensure that the view.html function is created with the following details.\nWe will now be able to see messages submitted by other users (or our random generator).\nFinally, we will customize our website by creating style.css. Here, I changed the color to purples. I listed all of the code I used to create my html files below.\n\nwith open('base.html', 'w') as f:\n    html_content = \"\"\"\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;\n        &lt;title&gt;{% block title %}Message Bank{% endblock %}&lt;/title&gt;\n        &lt;link rel = \"stylesheet\" type = \"text/css\" href=\"{{ url_for('statis', filename='style.css') }}\"&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;div class=\"center\"&gt;\n        {% block header %}\n        &lt;h1&gt;Welcome to the message bank!&lt;/h1&gt;\n        &lt;nav&gt;\n            &lt;a href=\"/submit\"&gt;Submit Message&lt;/a&gt; |\n            &lt;a href=\"/messages\"&gt;View Messages&lt;/a&gt;\n        &lt;/nav&gt;\n        {% endblock %}\n        {% block content %}\n        {% endblock %}\n        &lt;/div&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \"\"\"\n    f.write(html_content)\n\nwith open('submit.html', 'w') as f:\n    html_content = \"\"\"\n    {% extends \"base.html\" %}\n    {% block content %}\n    &lt;h2&gt;Submit your message below:&lt;/h2&gt;\n    &lt;form method=\"POST\"&gt; \n        &lt;label for=\"message\"&gt;Your message:&lt;/label&gt;&lt;br&gt;\n        &lt;input type=\"text\" name=\"user_message\"&gt;Your name or handle:&lt;/label&gt;&lt;br&gt;\n        &lt;input type=\"submit\" value=\"Submit\"&gt;\n    &lt;/form&gt;\n    {% if message %}\n    &lt;p&gt;Thank you for your submission, {{ user }}!&lt;/p&gt;\n    {% endif %}\n    {% endblock %}\n    \"\"\"\n    f.write(html_content)\n\nwith open('view.html', 'w') as f:\n    html_content = \"\"\"\n    {% extends \"base.html\" %}\n    {% block content %}\n    &lt;h1&gt;Messages&lt;/hi&gt;\n    &lt;u1&gt;\n    {% for message in messages %}\n        &lt;li&gt;{{ message[1] }}: {{ message[2] }}&lt;/li&gt;\n    {% endfor %}\n    &lt;/ul&gt;\n    {% endblock %}\n    \"\"\"\n    f.write(html_content)\n\nwith open('style.css', 'w') as f:\n    html_content = \"\"\"\n    html {\n        font-family: cursive; \n        background-color: rgb(250, 209, 228); \n        padding: 1rem; \n    }\n\n    body {\n        background-color: purple; \n        font-family: san-serif; \n        color: darkpurple\n    }\n\n    .center {\n        display: flex; \n        flex-direction: column; \n        align-items: center; \n        justify-content: center; \n        height: 100vh; \n        text-align: center; \n    }\n    \"\"\"\n    f.write(html_content)"
  },
  {
    "objectID": "posts/HW3/index.html#step-5-lets-try-it-out",
    "href": "posts/HW3/index.html#step-5-lets-try-it-out",
    "title": "Website Development - Message Submission Website",
    "section": "",
    "text": "Now that we have created all of the necessary code for our website, let’s go to our page and use it. We first must run:\nconda activate PIC16B-24W; set FLASK_ENV=development; flask run\nin our terminal. Them, we can follow along and see each page of our website. Lastly, please go to the following link to find code for our website: https://github.com/woulfeig/PIC16B_REPOSITORY/blob/main/myblog/posts/app.py\nThank you for reading! Good luck creating your own unique and dynamic website."
  },
  {
    "objectID": "posts/HW0-Data Visualization/index.html",
    "href": "posts/HW0-Data Visualization/index.html",
    "title": "Data Visualization",
    "section": "",
    "text": "There are many different ways to model and display a set of data. Using the Plotly library, we have the ability to make histographes, boxplots, and more. Plotly makes graphing and modelling data sets very simple and straighforward. In general, you start with calling the type of figure you would like to create and then manually selecting which customization you need for the specific plot. These plots can be used to model a plethora of different things but for our purposes, we will focus on modelling data gathered by researchers.\n\n\n\nFor simplicity sake, today we will start with developing scatterplots. Scatterplots plot each individual data point onto a two dimensional axis. By hovering over the point, we will be able to see what the point means in relation to our data. Before we start making our graph, we must download and organize our data. The easiest way to complete this step is by using panda operations as seen below. Today, we will be using the Palmer Penguin data set that analyzes the differences between three different species of penguins. In order to use this data sheet, we must ensure that we have the file downloaded to the same folder as our Jupyter Notebook. Please run the code cell below to upload the needed data.\n\n#importing packages and our data set \n\nimport pandas as pd\nfilename = \"palmer_penguins.csv\"\npenguins = pd.read_csv(filename)\npenguins = penguins.dropna(subset = [\"Body Mass (g)\", \"Sex\"])\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)\npenguins = penguins[penguins[\"Sex\"] != \".\"]\n\ncols = [\"Species\", \"Island\", \"Sex\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]\npenguins = penguins[cols]\n\nAfter we have downloaded our data set, we must import Plotly in order to make our visualizations. Plotly is a useful tool that can be used to create different types of graphs. Unless we import this package, our keywords to create plots will not be recognized.\n\nimport plotly\n\nNext, we will create a visualization labeled “fig” and use our Plotly commands to organize our data. For this plot, we will see how the length and depth of the culmen vary for different species of penguins. The culmen describes the upper ridge of a penguin’s bill. Researches describe the culmen using depth and length.\nWe use the second and third lines of code to make the final plot visible to a blog user. If you are just planning on creating figures in your notebook, please only use the first line to import the necessary tools to create and customize the plot. We call our scatterplot in the fifth line of code. In the event that you are making a different type of plot, you would set “fig” equal to a different keyword. Within our “()” we will label our x and y axis, change dot color based on the species of penguin, and designate the size of the graph.\nIn the second to last line of code, we add extra customizations to the layout of the plot itself. By using these commands, we can decrease the amount of whitespace of our graph. Finally, we can see our final scatterplot using the last line of code.\n\n#importing our packages to print and create our plots \n#our first plot will be a scatterplot\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default='iframe'\n\nfig = px.scatter(data_frame = penguins,\n                 x = \"Culmen Length (mm)\",\n                 y = \"Culmen Depth (mm)\",\n                 color = \"Species\",\n                 width = 600,\n                 height = 400\n                )\n\n#add a title\nfig.update_layout(title_text = \"Culmen Length vs. Culmen Depth in Different Penguin Species\")\n\n#output our figure\nfig.show()\n \n\n\n\n\nIn all, scatterplots through Plotly are extremely customizable and only require basic calls. Plotly can be used for many different data sets and model many different ideas in a variety of forms.\n\n\n\nPlotly also has the ability to create more detailed scatterplots. For example, we can create facets within our scatterplots. Facets are smaller scatterplots that can add additional details to our visualizations. Similar to our above scatterplot, we will be comparing culmen measurements amoungest different species of penguins. However, we will further our understanding by creating facets that show the recorded culmen data in specific plots for female and male penguins.\nWe set up our plot in a relatively similar way to the demonstration above. The extra customizations will allow our graph to appear in two smaller sets.\n\n#for our second plot, we will make a scatterplot with facets\n\nfig = px.scatter(data_frame = penguins,\n                 x = \"Culmen Length (mm)\",\n                 y = \"Culmen Depth (mm)\",\n                 color = \"Species\",\n                 hover_name = \"Species\",\n                 hover_data = [\"Island\", \"Sex\"],\n                 size = \"Body Mass (g)\",\n                 size_max = 8,\n                 width = 850,\n                 height = 400,\n                opacity = 0.5,\n                facet_col= \"Sex\",\n                title = \"Culmen Length vs. Depth for Different Species in Both Male and Female Penguins\")\n\n#output figure\nfig.show()\n\n\n\n\n\n\n\nWe have now explored two different ways to work with scatterplots. However, these two plots are both in 2D and only compare two pieces of recorded data in their visualizations. Through Plotly, we can explore plots that compare three different types of measurements. This means we are making 3D scatterplot graphes! For our example, we will keep analyzing both culmen depth and culmen length but now will also incorporate body mass measurements. Luckily for us, the format of our customizations is very similar. Instead, we use a slightly different call that designates that this is a 3-dimensional plot. Run the code block below to see the 3D scatterplot.\n\n#finally, we will make our most advanced plot: a 3D scatter plot\n\nfig = px.scatter_3d(penguins,\n                    x = \"Body Mass (g)\",\n                    y = \"Culmen Length (mm)\",\n                    z = \"Culmen Depth (mm)\",\n                    color = \"Species\",\n                    opacity = 0.5)\n\n#add a title\nfig.update_layout(title = \"Culmen Length vs. Culmen Depth vs. Body Mass for Different Species of Penguins\")\n\n#output figure\nfig.show()\n\n\n\n\nIn addition to scatterplots, Plotly also has the capabilties to make box plots, heatmaps, and more! As we can see, Plotly is a fantastic tool that can be used in a variety of ways. Thank you for reading! Good luck making your visualizations with Plotly!"
  },
  {
    "objectID": "posts/HW0-Data Visualization/index.html#using-the-palmer-penguins-data-set",
    "href": "posts/HW0-Data Visualization/index.html#using-the-palmer-penguins-data-set",
    "title": "Data Visualization",
    "section": "",
    "text": "There are many different ways to model and display a set of data. Using the Plotly library, we have the ability to make histographes, boxplots, and more. Plotly makes graphing and modelling data sets very simple and straighforward. In general, you start with calling the type of figure you would like to create and then manually selecting which customization you need for the specific plot. These plots can be used to model a plethora of different things but for our purposes, we will focus on modelling data gathered by researchers."
  },
  {
    "objectID": "posts/HW0-Data Visualization/index.html#scatterplots",
    "href": "posts/HW0-Data Visualization/index.html#scatterplots",
    "title": "Data Visualization",
    "section": "",
    "text": "For simplicity sake, today we will start with developing scatterplots. Scatterplots plot each individual data point onto a two dimensional axis. By hovering over the point, we will be able to see what the point means in relation to our data. Before we start making our graph, we must download and organize our data. The easiest way to complete this step is by using panda operations as seen below. Today, we will be using the Palmer Penguin data set that analyzes the differences between three different species of penguins. In order to use this data sheet, we must ensure that we have the file downloaded to the same folder as our Jupyter Notebook. Please run the code cell below to upload the needed data.\n\n#importing packages and our data set \n\nimport pandas as pd\nfilename = \"palmer_penguins.csv\"\npenguins = pd.read_csv(filename)\npenguins = penguins.dropna(subset = [\"Body Mass (g)\", \"Sex\"])\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)\npenguins = penguins[penguins[\"Sex\"] != \".\"]\n\ncols = [\"Species\", \"Island\", \"Sex\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]\npenguins = penguins[cols]\n\nAfter we have downloaded our data set, we must import Plotly in order to make our visualizations. Plotly is a useful tool that can be used to create different types of graphs. Unless we import this package, our keywords to create plots will not be recognized.\n\nimport plotly\n\nNext, we will create a visualization labeled “fig” and use our Plotly commands to organize our data. For this plot, we will see how the length and depth of the culmen vary for different species of penguins. The culmen describes the upper ridge of a penguin’s bill. Researches describe the culmen using depth and length.\nWe use the second and third lines of code to make the final plot visible to a blog user. If you are just planning on creating figures in your notebook, please only use the first line to import the necessary tools to create and customize the plot. We call our scatterplot in the fifth line of code. In the event that you are making a different type of plot, you would set “fig” equal to a different keyword. Within our “()” we will label our x and y axis, change dot color based on the species of penguin, and designate the size of the graph.\nIn the second to last line of code, we add extra customizations to the layout of the plot itself. By using these commands, we can decrease the amount of whitespace of our graph. Finally, we can see our final scatterplot using the last line of code.\n\n#importing our packages to print and create our plots \n#our first plot will be a scatterplot\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default='iframe'\n\nfig = px.scatter(data_frame = penguins,\n                 x = \"Culmen Length (mm)\",\n                 y = \"Culmen Depth (mm)\",\n                 color = \"Species\",\n                 width = 600,\n                 height = 400\n                )\n\n#add a title\nfig.update_layout(title_text = \"Culmen Length vs. Culmen Depth in Different Penguin Species\")\n\n#output our figure\nfig.show()\n \n\n\n\n\nIn all, scatterplots through Plotly are extremely customizable and only require basic calls. Plotly can be used for many different data sets and model many different ideas in a variety of forms."
  },
  {
    "objectID": "posts/HW0-Data Visualization/index.html#scatterplots-with-facets",
    "href": "posts/HW0-Data Visualization/index.html#scatterplots-with-facets",
    "title": "Data Visualization",
    "section": "",
    "text": "Plotly also has the ability to create more detailed scatterplots. For example, we can create facets within our scatterplots. Facets are smaller scatterplots that can add additional details to our visualizations. Similar to our above scatterplot, we will be comparing culmen measurements amoungest different species of penguins. However, we will further our understanding by creating facets that show the recorded culmen data in specific plots for female and male penguins.\nWe set up our plot in a relatively similar way to the demonstration above. The extra customizations will allow our graph to appear in two smaller sets.\n\n#for our second plot, we will make a scatterplot with facets\n\nfig = px.scatter(data_frame = penguins,\n                 x = \"Culmen Length (mm)\",\n                 y = \"Culmen Depth (mm)\",\n                 color = \"Species\",\n                 hover_name = \"Species\",\n                 hover_data = [\"Island\", \"Sex\"],\n                 size = \"Body Mass (g)\",\n                 size_max = 8,\n                 width = 850,\n                 height = 400,\n                opacity = 0.5,\n                facet_col= \"Sex\",\n                title = \"Culmen Length vs. Depth for Different Species in Both Male and Female Penguins\")\n\n#output figure\nfig.show()"
  },
  {
    "objectID": "posts/HW0-Data Visualization/index.html#d-scatterplots",
    "href": "posts/HW0-Data Visualization/index.html#d-scatterplots",
    "title": "Data Visualization",
    "section": "",
    "text": "We have now explored two different ways to work with scatterplots. However, these two plots are both in 2D and only compare two pieces of recorded data in their visualizations. Through Plotly, we can explore plots that compare three different types of measurements. This means we are making 3D scatterplot graphes! For our example, we will keep analyzing both culmen depth and culmen length but now will also incorporate body mass measurements. Luckily for us, the format of our customizations is very similar. Instead, we use a slightly different call that designates that this is a 3-dimensional plot. Run the code block below to see the 3D scatterplot.\n\n#finally, we will make our most advanced plot: a 3D scatter plot\n\nfig = px.scatter_3d(penguins,\n                    x = \"Body Mass (g)\",\n                    y = \"Culmen Length (mm)\",\n                    z = \"Culmen Depth (mm)\",\n                    color = \"Species\",\n                    opacity = 0.5)\n\n#add a title\nfig.update_layout(title = \"Culmen Length vs. Culmen Depth vs. Body Mass for Different Species of Penguins\")\n\n#output figure\nfig.show()\n\n\n\n\nIn addition to scatterplots, Plotly also has the capabilties to make box plots, heatmaps, and more! As we can see, Plotly is a fantastic tool that can be used in a variety of ways. Thank you for reading! Good luck making your visualizations with Plotly!"
  },
  {
    "objectID": "posts/HW 4/Another_copy_of_hw4_index.html",
    "href": "posts/HW 4/Another_copy_of_hw4_index.html",
    "title": "Heat Diffusion in Two Dimensions: Comparing Simulation Methods",
    "section": "",
    "text": "Although we have taken the time to explore heat diffusion in one dimension, heat diffusion in two dimensions uses an updated equation and different steps.\nFor our demonstrations today, we will set N = 101 and episilon = 0.2.\nAdditionally, we will use the following intitial condition (same as the lecture for one dimensional heat diffusion) to explore today’s question.\nFirst we must download the necessary packages. Numpy will allow us to work with arrays. Matplotlob will allow us to make visualizations. Jax allows us to complete complex numerical computing projects.\n\n#given in blog post directions\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport jax\nimport jax.numpy as jnp\nimport time\nfrom jax.experimental import sparse\nimport inspect\n\nNow that we have introduced today’s topics and imported the necessary Python packages, let’s explore the different methods of stimulating two dimensional heat diffusion.\n\n\n\nFirst we must include the following function which was given to us in the assignment directions. This function allows us to work with the different iterations of the function and compare our results.\n\n@jax.jit # decorator that tells the function to use XLA for computing\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"\n    Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2.\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    # u is NxN, so this is how we determine the size of u\n    N = u.shape[0]\n    # perform matrix vector multiplication with A and u, then reshape to proper size\n    # multiply by chosen epsilon as a stability constant to control timstep size\n    # add this product to u in order to update it\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n\n    # return updated u\n    return u\n\n\nN = 101\nepsilon = 0.2\n\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n\n\n\n\n\n\n\nNext, we will import the get_A(N) function from our seperate python file (heat_equation.py). This file must be saved in the same folder on your device as this Jupyter notebook file. Our first matrix is similar to the one dimensional matrix that we worked with in lecture.\n\ndef get_A(N):\n    \"\"\"\n    Creates a (N^2xN^2) finite difference matrix A to be used in heat diffusion, without all 0 rows or columns.\n    Args:\n        N: the square root of the row and column sizes of A\n    Returns:\n        A: The 2d finite difference matrix with dimensions (N^2xN^2).\n    \"\"\"\n    n = N * N\n    # create proper entries for 2d heat diffusion\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n\n    # rest of the entries are 0s\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n\n    # create A\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\nGiven that we implement the conditions given to us in the problem, we will receive the resulting array.\n\nsolutions = dict()\nstart_time = time.time()\n# obtain finite difference matrix\nA = get_A(N)\n\nfig, axs = plt.subplots(3,3)\nfor i in range(2700):\n    # run the iteration\n    u0 = advance_time_matvecmul(A, u0, epsilon)\n    # store immediate solution\n    solutions[i] = u0\n\nend_time = time.time()\ntotal_time = end_time - start_time\n\n# we don't want to waste time visualizing, so now we will utilize the solutions object to plot\nfor i in range(2700):\n    if (i + 1) % 300 == 0:\n        ax = axs[i // 300 // 3, i // 300 % 3]\n        ax.imshow(solutions[i])\n        ax.set_title(f'Iteration {i+1}')\n\n# see how long it took to run the simulation (NOT including plotting time)\nprint(f\"time it took: {total_time:0.2f} sec\")\n\ntime it took: 828.93 sec\n\n\n\n\n\n\n\n\n\nNow that we have successfully created our array, we must create a heatmap visulaization. Please follow the steps below to see our resulting heat maps that show our results every 300 iterations for 2700 total iterations. Additionally, we will also implement features of the time package that allows us to identify the time we start our iterations and the time 2700 iterations finishes. We will output this information with our diffusion plots so that we can compare the elapsed time with the other methods.\n\n\n\nIn this secind method, the matrix A will be returned in sparse format. Moreoverm we will be utilizing the batched coordinate format (BCOO) and take in a slightly different value than the matrix we created above. Please follow the steps below to create our resulting array.\n\ndef get_sparse_A(N):\n    \"\"\"\n    Creates finite difference matrix that is sparse.\n    Args:\n        N: the square root of the row and column sizes of A\n    Returns:\n        A: the N^2 x N^2 finite difference matrix A that is in sparse format\n    \"\"\"\n    n = N * N\n    # create proper entries for 2D heat diffusion in matrix-vector multiplication\n    # utilize jax.numpy in order to be compatible with jax's XLA compiler\n    diagonals = [-4 * jnp.ones(n), jnp.ones(n-1), jnp.ones(n-1), jnp.ones(n-N), jnp.ones(n-N)]\n\n    # make the other rows and columns 0\n    diagonals[1] = diagonals[1].at[(N-1)::N].set(0)\n    diagonals[2] = diagonals[2].at[(N-1)::N].set(0)\n\n    # create A\n    A = jnp.diag(diagonals[0]) + jnp.diag(diagonals[1], 1) + jnp.diag(diagonals[2], -1) + jnp.diag(diagonals[3], N) + jnp.diag(diagonals[4], -N)\n\n    # use batched coordinate format from Jax to use less space in the matrix\n    A_sp_matrix = sparse.BCOO.fromdense(A)\n    return A_sp_matrix\n\n\n# redefine u0 (since it was used in our last simulation)\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n# get sparse matrix format of A\nA_sparse = get_sparse_A(N)\n\n# run the simulation\nsolutions = dict()\nstart_time = time.time()\n\nfig, axs = plt.subplots(3,3)\nfor i in range(2700):\n    u0 = advance_time_matvecmul(A_sparse, u0, epsilon)\n    # store immediate solution\n    solutions[i] = u0\nend_time = time.time()\n# calculate total time for simulation, not including visualization\ntotal_time = end_time - start_time\n\n# visualize\nfor i in range(2700):\n    if (i + 1) % 300 == 0:\n        ax = axs[i // 300 // 3, i // 300 % 3]\n        ax.imshow(solutions[i])\n        ax.set_title(f'Iteration {i+1}')\n\nprint(f\"time it took: {total_time:0.2f} sec\")\n\ntime it took: 1.27 sec\n\n\n\n\n\n\n\n\n\nWe can confirm that we have correctly implemented our sparse function because we successfully created the same array as method 1, using a completely different method.\nThis array resembles the array we created with the function get_A(N). This is a good indication that we successfully implemented our get_A_sparse(N) function. Now, please follow the next function to create our visulization. In this next code cell, please note that we also found a method of determining the total elapsed time as we also did in method 1.\n\n\n\nFor our third method of creating our array and visualization, we will be able to achieve our result through a different sort of computation. This method works best with Poisson equations and may not be the most efficient implementation of heat diffusion. This function involves lots of Numpy principles which is the Python package that allows us to work with arrays. Please follow the code cell below to see how I created this function.\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"Advances the simulation by one timestep, utilizing vectorized numpy operations\n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    # get N\n    N = u.shape[0]\n    # pad edges with 0s to handle boundary finite differences\n    u_pad = np.pad(u, pad_width=1, mode='constant', constant_values=0)\n\n    # calculate difference btwn each element of u and its 4 neighboring terms\n     # -1 is subtracted 4 times in this equation,\n    # multiply by epsilon\n    u_new = u + epsilon * (-4*u_pad[1:-1, 1:-1] + u_pad[:-2, 1:-1] + u_pad[2:, 1:-1] + u_pad[1:-1, :-2] + u_pad[1:-1, 2:])\n    return u_new\n\nAfter importing the function, we will now create our visualization and determined our elapsed time. As we defined this function in a slightly different way than the functions of the first two methods, the lines that we use to create our visualization will also be different. However, we will still be using the time function to determine the length of time from when our iterations start to when we reach iteration 2700. Please follow the code below.\n\n# redefine u0 (since it was used in our last simulation)\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n# run the simulation\nsolutions = dict()\nstart_time = time.time()\n\nfig, axs = plt.subplots(3,3)\nfor i in range(2700):\n    u0 = advance_time_numpy(u0, epsilon)\n    # store immediate solution\n    solutions[i] = u0\nend_time = time.time()\n# calculate total time for simulation, not including visualization\ntotal_time = end_time - start_time\n\n# visualize\nfor i in range(2700):\n    if (i + 1) % 300 == 0:\n        ax = axs[i // 300 // 3, i // 300 % 3]\n        ax.imshow(solutions[i])\n        ax.set_title(f'Iteration {i+1}')\n\nprint(f\"time it took: {total_time:0.2f} sec\")\n\ntime it took: 0.62 sec\n\n\n\n\n\n\n\n\n\nAs seen in our images above, we have 9 iterations that occur every 300 iterations out of 2700. These iterations are labeled so that a viewer can see and easily understand the two dimensional heat diffusion that occurs. Our elapsed time is 1.4257 seconds for all 2700 iterations to run. We will keep this value in mind so that we may compare it in our conclusion (section 5).\n\n\n\nThe final method we look at will create our array using JAX operations. The Jax package in python allows us to complete complex numerical computations in a more efficient and clear way. In this method, we do not implement the sparse matrix multiplication processes as we have above. Please follow the code cell below and implement the function for our final method.\n\n@jax.jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via jax operations\n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    # get N\n    N = u.shape[0]\n\n    # pad edges with 0s to handle boundary finite differences\n    u_pad = jnp.pad(u, pad_width=1, mode='constant', constant_values=0)\n    # update u for 2d heat diffusion accordingly\n    u_new = u + epsilon * (-4*u_pad[1:-1, 1:-1] + u_pad[:-2, 1:-1] + u_pad[2:, 1:-1] + u_pad[1:-1, :-2] + u_pad[1:-1, 2:])\n    return u_new\n\nNow that we have successfully imported our function, we can create our final two-dimensional heat diffusion visualization. This will be created using a function that is very similar to the one we created for method 3. Additionally, we will also be recording the elapsed time for our iterations to occur.\n\n# redefine u0 (since it was used in our last simulation)\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n# run the simulation\nsolutions = dict()\nstart_time = time.time()\n\nfig, axs = plt.subplots(3,3)\nfor i in range(2700):\n    u0 = advance_time_jax(u0, epsilon)\n    # store immediate solution\n    solutions[i] = u0\nend_time = time.time()\n# calculate total time for simulation, not including visualization\ntotal_time = end_time - start_time\n\n# visualize\nfor i in range(2700):\n    if (i + 1) % 300 == 0:\n        ax = axs[i // 300 // 3, i // 300 % 3]\n        ax.imshow(solutions[i])\n        ax.set_title(f'Iteration {i+1}')\n\nprint(f\"time it took: {total_time:0.2f} sec\")\n\ntime it took: 0.32 sec\n\n\n\n\n\n\n\n\n\nWe have successfully created our final set of visualizations!! The 2700 iterations took about 3.8895 seconds to complete. Now, we can finally look back on our different methods and compare.\n\n\n\nAlthough some methods may be more efficient than others, they are all equally effective at modelling two dimensional heat diffusion. Method 1 took ___ for all iterations to occur. Method 2 took ___ for all iterations to occur. Method 3 took ____ for all iterations to occur. Method 4 took ___ for all methods to occur. Therefore, ____ is the fast function we implemented. In my opinion, ___ was the easiest to write ____. Regardless of which was the easiest or fastest method for this specific situation, it is important to learn and understand all four methods because different methods will be useful for different situations we will encounter in this class.\nThank you for reading!"
  },
  {
    "objectID": "posts/HW 4/Another_copy_of_hw4_index.html#introduction-heat-diffusion-explained",
    "href": "posts/HW 4/Another_copy_of_hw4_index.html#introduction-heat-diffusion-explained",
    "title": "Heat Diffusion in Two Dimensions: Comparing Simulation Methods",
    "section": "",
    "text": "Although we have taken the time to explore heat diffusion in one dimension, heat diffusion in two dimensions uses an updated equation and different steps.\nFor our demonstrations today, we will set N = 101 and episilon = 0.2.\nAdditionally, we will use the following intitial condition (same as the lecture for one dimensional heat diffusion) to explore today’s question.\nFirst we must download the necessary packages. Numpy will allow us to work with arrays. Matplotlob will allow us to make visualizations. Jax allows us to complete complex numerical computing projects.\n\n#given in blog post directions\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport jax\nimport jax.numpy as jnp\nimport time\nfrom jax.experimental import sparse\nimport inspect\n\nNow that we have introduced today’s topics and imported the necessary Python packages, let’s explore the different methods of stimulating two dimensional heat diffusion."
  },
  {
    "objectID": "posts/HW 4/Another_copy_of_hw4_index.html#method-1-matrix-multiplication",
    "href": "posts/HW 4/Another_copy_of_hw4_index.html#method-1-matrix-multiplication",
    "title": "Heat Diffusion in Two Dimensions: Comparing Simulation Methods",
    "section": "",
    "text": "First we must include the following function which was given to us in the assignment directions. This function allows us to work with the different iterations of the function and compare our results.\n\n@jax.jit # decorator that tells the function to use XLA for computing\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"\n    Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2.\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    # u is NxN, so this is how we determine the size of u\n    N = u.shape[0]\n    # perform matrix vector multiplication with A and u, then reshape to proper size\n    # multiply by chosen epsilon as a stability constant to control timstep size\n    # add this product to u in order to update it\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n\n    # return updated u\n    return u\n\n\nN = 101\nepsilon = 0.2\n\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n\n\n\n\n\n\n\nNext, we will import the get_A(N) function from our seperate python file (heat_equation.py). This file must be saved in the same folder on your device as this Jupyter notebook file. Our first matrix is similar to the one dimensional matrix that we worked with in lecture.\n\ndef get_A(N):\n    \"\"\"\n    Creates a (N^2xN^2) finite difference matrix A to be used in heat diffusion, without all 0 rows or columns.\n    Args:\n        N: the square root of the row and column sizes of A\n    Returns:\n        A: The 2d finite difference matrix with dimensions (N^2xN^2).\n    \"\"\"\n    n = N * N\n    # create proper entries for 2d heat diffusion\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n\n    # rest of the entries are 0s\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n\n    # create A\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\nGiven that we implement the conditions given to us in the problem, we will receive the resulting array.\n\nsolutions = dict()\nstart_time = time.time()\n# obtain finite difference matrix\nA = get_A(N)\n\nfig, axs = plt.subplots(3,3)\nfor i in range(2700):\n    # run the iteration\n    u0 = advance_time_matvecmul(A, u0, epsilon)\n    # store immediate solution\n    solutions[i] = u0\n\nend_time = time.time()\ntotal_time = end_time - start_time\n\n# we don't want to waste time visualizing, so now we will utilize the solutions object to plot\nfor i in range(2700):\n    if (i + 1) % 300 == 0:\n        ax = axs[i // 300 // 3, i // 300 % 3]\n        ax.imshow(solutions[i])\n        ax.set_title(f'Iteration {i+1}')\n\n# see how long it took to run the simulation (NOT including plotting time)\nprint(f\"time it took: {total_time:0.2f} sec\")\n\ntime it took: 828.93 sec\n\n\n\n\n\n\n\n\n\nNow that we have successfully created our array, we must create a heatmap visulaization. Please follow the steps below to see our resulting heat maps that show our results every 300 iterations for 2700 total iterations. Additionally, we will also implement features of the time package that allows us to identify the time we start our iterations and the time 2700 iterations finishes. We will output this information with our diffusion plots so that we can compare the elapsed time with the other methods."
  },
  {
    "objectID": "posts/HW 4/Another_copy_of_hw4_index.html#method-2-sparse-matrix-jax",
    "href": "posts/HW 4/Another_copy_of_hw4_index.html#method-2-sparse-matrix-jax",
    "title": "Heat Diffusion in Two Dimensions: Comparing Simulation Methods",
    "section": "",
    "text": "In this secind method, the matrix A will be returned in sparse format. Moreoverm we will be utilizing the batched coordinate format (BCOO) and take in a slightly different value than the matrix we created above. Please follow the steps below to create our resulting array.\n\ndef get_sparse_A(N):\n    \"\"\"\n    Creates finite difference matrix that is sparse.\n    Args:\n        N: the square root of the row and column sizes of A\n    Returns:\n        A: the N^2 x N^2 finite difference matrix A that is in sparse format\n    \"\"\"\n    n = N * N\n    # create proper entries for 2D heat diffusion in matrix-vector multiplication\n    # utilize jax.numpy in order to be compatible with jax's XLA compiler\n    diagonals = [-4 * jnp.ones(n), jnp.ones(n-1), jnp.ones(n-1), jnp.ones(n-N), jnp.ones(n-N)]\n\n    # make the other rows and columns 0\n    diagonals[1] = diagonals[1].at[(N-1)::N].set(0)\n    diagonals[2] = diagonals[2].at[(N-1)::N].set(0)\n\n    # create A\n    A = jnp.diag(diagonals[0]) + jnp.diag(diagonals[1], 1) + jnp.diag(diagonals[2], -1) + jnp.diag(diagonals[3], N) + jnp.diag(diagonals[4], -N)\n\n    # use batched coordinate format from Jax to use less space in the matrix\n    A_sp_matrix = sparse.BCOO.fromdense(A)\n    return A_sp_matrix\n\n\n# redefine u0 (since it was used in our last simulation)\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n# get sparse matrix format of A\nA_sparse = get_sparse_A(N)\n\n# run the simulation\nsolutions = dict()\nstart_time = time.time()\n\nfig, axs = plt.subplots(3,3)\nfor i in range(2700):\n    u0 = advance_time_matvecmul(A_sparse, u0, epsilon)\n    # store immediate solution\n    solutions[i] = u0\nend_time = time.time()\n# calculate total time for simulation, not including visualization\ntotal_time = end_time - start_time\n\n# visualize\nfor i in range(2700):\n    if (i + 1) % 300 == 0:\n        ax = axs[i // 300 // 3, i // 300 % 3]\n        ax.imshow(solutions[i])\n        ax.set_title(f'Iteration {i+1}')\n\nprint(f\"time it took: {total_time:0.2f} sec\")\n\ntime it took: 1.27 sec\n\n\n\n\n\n\n\n\n\nWe can confirm that we have correctly implemented our sparse function because we successfully created the same array as method 1, using a completely different method.\nThis array resembles the array we created with the function get_A(N). This is a good indication that we successfully implemented our get_A_sparse(N) function. Now, please follow the next function to create our visulization. In this next code cell, please note that we also found a method of determining the total elapsed time as we also did in method 1."
  },
  {
    "objectID": "posts/HW 4/Another_copy_of_hw4_index.html#method-3-direct-operation-numpy",
    "href": "posts/HW 4/Another_copy_of_hw4_index.html#method-3-direct-operation-numpy",
    "title": "Heat Diffusion in Two Dimensions: Comparing Simulation Methods",
    "section": "",
    "text": "For our third method of creating our array and visualization, we will be able to achieve our result through a different sort of computation. This method works best with Poisson equations and may not be the most efficient implementation of heat diffusion. This function involves lots of Numpy principles which is the Python package that allows us to work with arrays. Please follow the code cell below to see how I created this function.\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"Advances the simulation by one timestep, utilizing vectorized numpy operations\n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    # get N\n    N = u.shape[0]\n    # pad edges with 0s to handle boundary finite differences\n    u_pad = np.pad(u, pad_width=1, mode='constant', constant_values=0)\n\n    # calculate difference btwn each element of u and its 4 neighboring terms\n     # -1 is subtracted 4 times in this equation,\n    # multiply by epsilon\n    u_new = u + epsilon * (-4*u_pad[1:-1, 1:-1] + u_pad[:-2, 1:-1] + u_pad[2:, 1:-1] + u_pad[1:-1, :-2] + u_pad[1:-1, 2:])\n    return u_new\n\nAfter importing the function, we will now create our visualization and determined our elapsed time. As we defined this function in a slightly different way than the functions of the first two methods, the lines that we use to create our visualization will also be different. However, we will still be using the time function to determine the length of time from when our iterations start to when we reach iteration 2700. Please follow the code below.\n\n# redefine u0 (since it was used in our last simulation)\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n# run the simulation\nsolutions = dict()\nstart_time = time.time()\n\nfig, axs = plt.subplots(3,3)\nfor i in range(2700):\n    u0 = advance_time_numpy(u0, epsilon)\n    # store immediate solution\n    solutions[i] = u0\nend_time = time.time()\n# calculate total time for simulation, not including visualization\ntotal_time = end_time - start_time\n\n# visualize\nfor i in range(2700):\n    if (i + 1) % 300 == 0:\n        ax = axs[i // 300 // 3, i // 300 % 3]\n        ax.imshow(solutions[i])\n        ax.set_title(f'Iteration {i+1}')\n\nprint(f\"time it took: {total_time:0.2f} sec\")\n\ntime it took: 0.62 sec\n\n\n\n\n\n\n\n\n\nAs seen in our images above, we have 9 iterations that occur every 300 iterations out of 2700. These iterations are labeled so that a viewer can see and easily understand the two dimensional heat diffusion that occurs. Our elapsed time is 1.4257 seconds for all 2700 iterations to run. We will keep this value in mind so that we may compare it in our conclusion (section 5)."
  },
  {
    "objectID": "posts/HW 4/Another_copy_of_hw4_index.html#method-4-jax",
    "href": "posts/HW 4/Another_copy_of_hw4_index.html#method-4-jax",
    "title": "Heat Diffusion in Two Dimensions: Comparing Simulation Methods",
    "section": "",
    "text": "The final method we look at will create our array using JAX operations. The Jax package in python allows us to complete complex numerical computations in a more efficient and clear way. In this method, we do not implement the sparse matrix multiplication processes as we have above. Please follow the code cell below and implement the function for our final method.\n\n@jax.jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via jax operations\n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    # get N\n    N = u.shape[0]\n\n    # pad edges with 0s to handle boundary finite differences\n    u_pad = jnp.pad(u, pad_width=1, mode='constant', constant_values=0)\n    # update u for 2d heat diffusion accordingly\n    u_new = u + epsilon * (-4*u_pad[1:-1, 1:-1] + u_pad[:-2, 1:-1] + u_pad[2:, 1:-1] + u_pad[1:-1, :-2] + u_pad[1:-1, 2:])\n    return u_new\n\nNow that we have successfully imported our function, we can create our final two-dimensional heat diffusion visualization. This will be created using a function that is very similar to the one we created for method 3. Additionally, we will also be recording the elapsed time for our iterations to occur.\n\n# redefine u0 (since it was used in our last simulation)\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n# run the simulation\nsolutions = dict()\nstart_time = time.time()\n\nfig, axs = plt.subplots(3,3)\nfor i in range(2700):\n    u0 = advance_time_jax(u0, epsilon)\n    # store immediate solution\n    solutions[i] = u0\nend_time = time.time()\n# calculate total time for simulation, not including visualization\ntotal_time = end_time - start_time\n\n# visualize\nfor i in range(2700):\n    if (i + 1) % 300 == 0:\n        ax = axs[i // 300 // 3, i // 300 % 3]\n        ax.imshow(solutions[i])\n        ax.set_title(f'Iteration {i+1}')\n\nprint(f\"time it took: {total_time:0.2f} sec\")\n\ntime it took: 0.32 sec\n\n\n\n\n\n\n\n\n\nWe have successfully created our final set of visualizations!! The 2700 iterations took about 3.8895 seconds to complete. Now, we can finally look back on our different methods and compare."
  },
  {
    "objectID": "posts/HW 4/Another_copy_of_hw4_index.html#comparison-of-methods",
    "href": "posts/HW 4/Another_copy_of_hw4_index.html#comparison-of-methods",
    "title": "Heat Diffusion in Two Dimensions: Comparing Simulation Methods",
    "section": "",
    "text": "Although some methods may be more efficient than others, they are all equally effective at modelling two dimensional heat diffusion. Method 1 took ___ for all iterations to occur. Method 2 took ___ for all iterations to occur. Method 3 took ____ for all iterations to occur. Method 4 took ___ for all methods to occur. Therefore, ____ is the fast function we implemented. In my opinion, ___ was the easiest to write ____. Regardless of which was the easiest or fastest method for this specific situation, it is important to learn and understand all four methods because different methods will be useful for different situations we will encounter in this class.\nThank you for reading!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Heat Diffusion in Two Dimensions: Comparing Simulation Methods\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFact or Fiction? Fake News Classification Using Keras\n\n\n\n\n\n\nHomework\n\n\ncode\n\n\nWeek 10\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nIsabella Woulfe\n\n\n\n\n\n\n\n\n\n\n\n\nCats vs. Dogs: Image Classification\n\n\n\n\n\n\nHomework\n\n\ncode\n\n\nWeek 9\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nIsabella Woulfe\n\n\n\n\n\n\n\n\n\n\n\n\nHeat Diffusion in Two Dimensions - Comparing Simulation Methods\n\n\n\n\n\n\nHomework\n\n\ncode\n\n\nWeek 7\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nIsabella Woulfe\n\n\n\n\n\n\n\n\n\n\n\n\nWebsite Development - Message Submission Website\n\n\n\n\n\n\nHomework\n\n\ncode\n\n\nWeek 6\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nIsabella Woulfe\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping\n\n\n\n\n\n\nHomework\n\n\ncode\n\n\nWeek 5\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nIsabella Woulfe\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Temperatures and Climate of the World!\n\n\n\n\n\n\nHomework\n\n\ncode\n\n\nWeek 1\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nIsabella Woulfe\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\n\n\n\n\nHomework\n\n\ncode\n\n\nWeek 0\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nIsabella Woulfe\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nIsabella Woulfe\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nIsabella Woulfe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Isabella Woulfe",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/HW 1/index.html",
    "href": "posts/HW 1/index.html",
    "title": "Exploring Temperatures and Climate of the World!",
    "section": "",
    "text": "Today, we will be reviewing how to create interesting and interative data sets with the NOAA climate data. The data set that we will be using has a list of countries, different station names, recorded temperatures for every month of the year and more. By analyzing and organzing our data sets efficiently, we will be able to create interesting visualizations and use them to answer a variety of questions.\nBefore we start, we must download the necessary packages to create our database. Please run the code cell bellow to import Pandas, Numpy and SQL.\n\n#import all of our necessary packages\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nNow that we have our packages, we will start importing three different datasets. We must save these csv files in the same folder as our Jupyter Notebook in order to use them here. In the following code cells, we will use the call stations.head() to show the first five rows of our data in a table. This step makes it easy for us to ensure that there was no problems when uploading our datasets. Please run the following code cells and then we can get started building our database.\n\n#import the data that records the location of each station making measurements\nfilename = \"station-metadata.csv\"\nstations = pd.read_csv(filename)\nstations.head() #head will output the first five columns so that we can ensure our csv file was correctly imported\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\n\n\n\n\n\n\n\n\n#import the data set that organizes the names of our studied countries\nfilename = \"country-codes.csv\"\ncountries = pd.read_csv(filename)\ncountries.head()\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n\n\n\n\n\n\n#import the temperature data collected from our stations in the select countries\nfilename = \"temps.csv\"\ntemperatures = pd.read_csv(filename)\ntemperatures.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\n\n\n\nNow that we have uploaded the three necessary datasets, we will create a new data frame that can be used to organize the specific data we want to use. We will use SQL to pull from our three data sets to craft out database.\n\n#using SQL and pandas to combine our three data sets\nconn = sqlite3.connect(\"temps.db\")\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\ndf = df_iter.__next__()\ndef prepare_df(df):\n    \"\"\"\n    Function that will create our dataframe with the designated columns and information \n\n    Arguments: (df) - the function takes in our pandas dataframe and will organize all of the \n                    different pieces of data from our CSV files that we uploaded above\n\n    Returns: (df) - we will be left with our updated pandas data frame\n    \"\"\"\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\n\n#continuation of combining our data sets\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\nfor i, df in enumerate(df_iter):\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\n\n#test to ensure we are correctly applying everything\nurl = \"station-metadata.csv\"\nstations = pd.read_csv(url)\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\n27585\n\n\n\nurl = \"country-codes.csv\"\ncountries = pd.read_csv(url)\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\n\n#outputs our column names \ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\nWe now have downloaded all of our different data sets into our folder. We will now use the following commands to create tables. We should see three seperate table headings in our output.\n\n#creates our empty database that we will fill with our query function\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\n#as our output matches our expectation, we have completed this task properly!\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\nWe will fill our database using a function in a seperate Python file. From this file, we will import our function and as a result our database. This function requires us to use SQL keywords (written in all capital letters) in order to easily combine different sorts of data into one whole set.\n\n#importing our function from a seperate python file saved within our folder\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n\n    \"\"\"\n    Function that uses SQL commands to organize the data from our three CSV files into the created columns\n\n    Arguments:  db_file - the file for our database (in the form of a string)\n                country - string that describes the name of the country \n                year_begin - integer value that states the start year \n                year_end - integer value that states the end year \n                month - integer value that states the specific month of date we are working with \n    \n    Returns: df - the result is a pandas data frame that organizes the values from the arguments explained above\n    \"\"\"\n\n    conn = sqlite3.connect(db_file)\n    query = f'''\n        SELECT s.NAME, s.LATITUDE, s.LONGITUDE, c.NAME AS Country, t.Year, t.Month, t.Temp\n        FROM temperatures t\n        LEFT JOIN stations s ON t.ID = s.id\n        LEFT JOIN countries c ON t.ID LIKE c.\"FIPS 10-4\" || '%'\n        WHERE c.Name = '{country}' AND t.Month = {month} AND t.Year BETWEEN {year_begin} AND {year_end}\n\n'''\n    result_df = pd.read_sql_query(query, conn)\n    conn.close()\n    return result_df\n\n\n\nIt is very important that we test our code as we go so that we can be certain we have not encountered any errors. The code cell below will create a database for temperatures in India during January between the years of 1980 and 2020. Please run the cell below to show that we correctly implemented our database.\n\n#test case for our query function \nquery_climate_database(db_file = \"temps.db\", \n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020, \n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns\n\n\n\nAs our database for India matches the sample, we have correctly created our database!\n\n\n\nNow that we have all our data from our three sets organized, we can use them to answer interesting questions. Visualizations are a great way to display data as they are super versatile and easy to read. We will use Plotly to create our data sets. Please import the Plotly package below.\n\n#importing plotly so that we may create visualizations\nfrom plotly import express as px\n\n\n#creating an iframe folder so that our visualizations may be visible on our blog\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nLet’s get started with our first question.\nHow does the average yearly change in temperature vary within a given country?\nThe best way to tackle this question is through creating a geographic scatter function. This figure will appear as a map but will mark specific stations in a given country and share their temperature measurements. We will start by creating a function called tenperature_coefficient_plot(). Please follow the code written below to create our first visualization. We must be sure to include the necessary parameters and be as detailed as possible with our plots.\n\n#our function will allow us to create out plot\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    Function will create a scatter map box plot that explores the average yearly change in temperature \n\n    Arguments: db_file - the file for our database that we will use to make our plot (in the form of a string)\n                country - string that describes the name of the country that will appear on our plot\n                year_begin - integer value that states the start year for our plot \n                year_end - integer value that states the end year for our plot\n                month - integer value that states the specific month of date we plotting\n\n    Returns: fig - a scatterplot map box that shows the map of a country with the different \n                   measurements placed on the location where they were taken \n    \"\"\"\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    df = df[df.groupby('NAME')['Year'].transform('count') &gt;= min_obs]\n\n    #average tenperature will allow us to use one measurement per station \n    def temperature_average(data):\n        \"\"\"\n        Function will be used to determine the average temperature recorded in a station during a given year\n\n        Arguments: data['Year'] - the integer value from our dataframe that looks at a single year \n                                 in between our year_begin and our year_end\n                   data['Temp'] - the intger value that describes the temperature recorded in the \n                                  given year\n\n        Returns: average - an integer value that will be used to calculate the yearly change\n        \"\"\"\n\n        X = data['Year']\n        y = data['Temp']\n        \n        average = np.polyfit(X, y, deg=1)[0]\n        \n        return average\n\n    #creates columns for our visalization\n    coefs = df.groupby('NAME').apply(temperature_average).reset_index()\n    coefs.columns = ['NAME', 'YearlyChange']\n\n    df = pd.merge(df, coefs, on='NAME')\n\n    #this type of visualization will appear as an interactive map with data points\n    fig = px.scatter_mapbox(df, \n                            lat='LATITUDE', \n                            lon='LONGITUDE',\n                            color='YearlyChange',\n                            hover_data = {'NAME' : True, 'YearlyChange': ':.3f'},\n                            labels = {'YearlyChange': 'Estimated Yearly Increase (Celsius)'}, \n                            title = f\"Estimates of Yearly Increase in Temperature in {pd.to_datetime(month, format='%m').month_name()} for stations in {country}, {year_begin}-{year_end}\",\n                            **kwargs)\n    fig.update_layout(mapbox_style=\"open-street-map\")\n    return fig\n\nWe will now use the following test case of India in January during 1980-2020 to test our function. As you can see, we will create a figure that appears as an interactive map that easily displays the answer to our posed question\n\n#test case \ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"temps.db\", \"India\", 1980, 2020, 1, \n                                   min_obs=10,\n                                   zoom=2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nNow, we can answer our question! For our chosen country of India, we can see that India has not experienced huge increases in temperature during the month of Januray over the period of forty years. Nevertheless, there are some stations along the coasts and borders that have seen significant increases in temperature.\n\n\n\nWhile we are programming, we want to ask questions that we can answer using our models. By asking these questions, we can connect our data sets to real world scenarios.\nFor our next question, let’s ask which regions of the world have the highest concentration of stations recording temperature data?\nThe best way to approach this question is to create a heatmap using our knowledge of Plotly. We can start by defining the following function. We can create the figure using specific columns from our database.\n\n#heatmap/density plot will show concentrations using different colors and a key explaining each color's signficance\ndef temperature_coefficient_heatmap(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    Function will create a density/heatmap map box plot that explores the concentration of stations\n\n    Arguments: db_file - the file for our database that we will use to make our plot (in the form of a string)\n                country - string that describes the name of the country that will appear on our plot\n                year_begin - integer value that states the start year for our plot \n                year_end - integer value that states the end year for our plot\n                month - integer value that states the specific month of date we plotting\n\n    Returns: fig - a density map that resembles a map of the world and shows the concentration \n                   of stations throughout the world using colors and our key\n    \"\"\"\n    \n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    df = df[df.groupby('NAME')['Year'].transform('count') &gt;= min_obs]\n\n    def temperature_average(data):\n        \"\"\"\n        Function will be used to determine the average temperature recorded in a station during a given year\n\n        Arguments: data['Year'] - the integer value from our dataframe that looks at a single year \n                                 in between our year_begin and our year_end\n                   data['Temp'] - the intger value that describes the temperature recorded in the \n                                  given year\n\n        Returns: average - an integer value that will be used to calculate the yearly change\n        \"\"\"\n\n        X = data['Year']\n        y = data['Temp']\n        \n        average = np.polyfit(X, y, deg=1)[0]\n        \n        return average\n\n    coefs = df.groupby('NAME').apply(temperature_average).reset_index()\n    coefs.columns = ['NAME', 'YearlyChange']\n\n    df = pd.merge(df, coefs, on='NAME')\n\n   \n#outputs our figure\nfig = px.density_mapbox(stations,\n                        lat = \"LATITUDE\",\n                        lon = \"LONGITUDE\",\n                        hover_data = {'NAME' : True},\n                        radius = 1,\n                        zoom = 0,\n                        height = 300,\n                        title = \"Station Concentration Map\")\n\nfig.update_layout(title = \"Station Concentration Map\")\nfig.update_layout(mapbox_style=\"carto-positron\")\nfig.update_layout(margin={\"r\":20,\"t\":25,\"l\":20,\"b\":20})\nfig.show()\n\n\n\n\nNow that we have our plot, it is important that we understand what it is telling us. The lighter the color gets (or yellower) means that there is a higher concentration of stations recording these temperatures in our data set. In regions with only a few stations, we only note a handful of dots spread around the area.\nIn response to our question, it seems that the United States and parts of Europe have the highest concentration of stations.\nFinally, let’s create one more visualization and learn more about temperature using our data sets. During a twenty-year period, which part of Italy experienced higher average temperatures during the month of August: North Italy, Central Italy, or South Italy?\nAs someone who has lived in Italy, I am very aware of the different lifestyles of the people in each of these three regions. Moreover, I would like to inquire whether climate has played a role in these cultural differences.\nOnce again, I have decided to use a scatterplot to answer my question. However, this scatterplot will differ from the one above as it analyzes the average temperature of each station rather than the change in temperature. Please see the code below.\n\n#uses a scatterplot to explore the temperature changes within a given country\ndef regional_difference_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    Function will create a scatter map box plot that explores how temperatures differ in North \n    Italy compared to South Italy\n\n    Arguments: db_file - the file for our database that we will use to make our plot (in the form of a string)\n                country - string that describes the name of the country that will appear on our plot\n                year_begin - integer value that states the start year for our plot \n                year_end - integer value that states the end year for our plot\n                month - integer value that states the specific month of date we plotting\n\n    Returns: fig - a scatterplot map box that shows the map of a country with the different \n                   measurements placed on the location where they were taken that will make it very \n                   easy to compare the differences between regions of one country\n    \"\"\"\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    df = df[df.groupby('NAME')['Temp'].transform('count') &gt;= min_obs]\n\n    def temperature_average(df):\n        \"\"\"\n        Function will be used to determine the average temperature recorded in a station during a given year\n\n        Arguments: data['Year'] - the integer value from our dataframe that looks at a single year \n                                 in between our year_begin and our year_end\n                   data['Temp'] - the intger value that describes the temperature recorded in the \n                                  given year\n\n        Returns: average - an integer value that will be used to calculate the yearly change\n        \"\"\"\n\n        average = np.mean(df['Temp'])\n        \n        return average\n\n    coefs = df.groupby('NAME').apply(temperature_average).reset_index()\n    coefs.columns = ['NAME', 'Average Temperature']\n\n    df = pd.merge(df, coefs, on='NAME')\n\n    fig = px.scatter_mapbox(df, \n                            lat='LATITUDE', \n                            lon='LONGITUDE',\n                            color='Average Temperature',\n                            hover_data = {'NAME' : True, 'Average Temperature': ':.3f'},\n                            labels = {'Average Temperature': 'Average Temperature (Celsius)'}, \n                            title = f\"Temperatures in {pd.to_datetime(month, format='%m').month_name()} for stations in {country}, {year_begin}-{year_end}\",\n                            **kwargs)\n    fig.update_layout(mapbox_style=\"open-street-map\")\n    return fig\n\nAlthough we can apply this function to any country, month, or year, we will look at Italy in August over a twenty year period to develop our conclusion.\n\n#test case \ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = regional_difference_plot(\"temps.db\", \"Italy\", 1980, 2000, 8, \n                                   min_obs=10,\n                                   zoom=2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nIn our plot above, we can see that South Italian stations have the points with the deepest red. This means that the South of Italy experienced the highest average temperatures compared to both the North and Central regions. I believe that this is very fitting considering the lifestyle and daily practices of people who reside in Southern Italy.\nThus, we have thoroughly reviewed how to create databases and make visualizations using our temperature data sets.\n\n\n\nBelow, we will make a second query function that we will use to further explore the data within our dataframe. Follow the code below to see how we create this new database which replace the year begin and year end columns with a single year.\n\n#importing our new query function from the same file as before\nfrom climate_database import second_query_climate_database\nimport inspect\nprint(inspect.getsource(second_query_climate_database))\n\ndef second_query_climate_database(db_file, country, year, month):\n    \"\"\"\n    Function that uses SQL commands to organize the data from our three CSV files into the created columns\n\n    Arguments:  db_file - the file for our database (in the form of a string)\n                country - string that describes the name of the country \n                year - integer value that describes the year the data was collected \n                month - integer value that states the specific month of date we are working with \n    \n    Returns: df - the result is a pandas data frame that organizes the values from the arguments explained above\n    \"\"\"\n\n    conn = sqlite3.connect(db_file)\n    query = f'''\n        SELECT s.NAME, s.LATITUDE, s.LONGITUDE, c.NAME AS Country, t.Year, t.Month, t.Temp\n        FROM temperatures t\n        LEFT JOIN stations s ON t.ID = s.id\n        LEFT JOIN countries c ON t.ID LIKE c.\"FIPS 10-4\" || '%'\n        WHERE c.Name = '{country}' AND t.Year = '{year}' AND t.Month = '{month}'\n\n\n'''\n    result_df = pd.read_sql_query(query, conn)\n    conn.close()\n    return result_df\n\n\n\n\n#testing with Canada to see that our database is correctly created\nsecond_query_climate_database(db_file = \"temps.db\", \n                       country = \"Canada\", \n                       year = 2000,  \n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nCHEMAINUS\n48.9333\n-123.7500\nCanada\n2000\n1\n3.52\n\n\n1\nCOWICHAN_LAKE_FORESTRY\n48.8167\n-124.1333\nCanada\n2000\n1\n2.21\n\n\n2\nLAKE_COWICHAN\n48.8333\n-124.0500\nCanada\n2000\n1\n1.48\n\n\n3\nDUNCAN_KELVIN_CREEK\n48.7333\n-123.7333\nCanada\n2000\n1\n2.02\n\n\n4\nESQUIMALT_HARBOUR\n48.4333\n-123.4333\nCanada\n2000\n1\n4.58\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1531\nMAKKOVIK_A\n55.0833\n-59.1833\nCanada\n2000\n1\n-15.06\n\n\n1532\nWABUSH_LAKE_A\n52.9333\n-66.8667\nCanada\n2000\n1\n-22.00\n\n\n1533\nCHAPLEAU_ARPTSAWR\n47.6500\n-83.3500\nCanada\n2000\n1\n-15.30\n\n\n1534\nFROBISHER_BAY\n63.8000\n-68.6000\nCanada\n2000\n1\n-27.50\n\n\n1535\nKNOB_LAKE_QUE\n54.8000\n-66.7000\nCanada\n2000\n1\n-23.90\n\n\n\n\n1536 rows × 7 columns\n\n\n\nNow, we can use this new data set in interesting ways!\n\n\n\nMulti-faceted graphes are a great way for us to share more information through out graphes. Facets allow us to break up our graphs so that we have a figure that resembles two plots side by side. For our data set, graphs with facets are great for us to show examine the varying temperatures of two different countries. This leads us to our question, who experienced warmer temperatures in August of 2000: Brazil or Mexico?\nLet’s start by defininng our function! We will be using data from our second query function.\n\ndef month_vs_temp(db_file, country1, country2, year, month, **kwargs): \n    \"\"\"\n    Function will create a basic scatter plot that explores how temperatures differ in Brazil \n    and Mexico during the given month and year\n\n    Arguments: db_file - the file for our database that we will use to make our plot (in the form of a string)\n                country - string that describes the name of the country that will appear on our plot\n                year - integer value that describes the year the data was collected\n                month - integer value that states the specific month of date we plotting\n\n    Returns: fig - a scatterplot that will plot Month vs Temperature for two countries contained in \n                   seperate facets that will make it appear as two graphes plotted on one plane\n    \"\"\"\n\n      #using our second query function with four inputs\n    df1 = second_query_climate_database(db_file, country1, year, month)\n    df2 = second_query_climate_database(db_file, country2, year, month)\n\n      #defining an equation for our coefficients\n    def coef(group):\n          X = group['Year']\n          y = group['Temp']\n          coef = np.polyfit(X, y, deg=1)[0]\n          return coef\n\n      #using our coefficients to combine our two countries into one graph\n    coefs1 = df1.groupby('NAME').apply(coef).reset_index()\n    coefs2 = df2.groupby('NAME').apply(coef).reset_index()\n    coefs1.columns = ['NAME','YearlyChange']\n    coefs1['Country'] = country1\n    coefs2.columns = ['NAME', 'YearlyChange']\n    coefs2['Country'] = country2\n   \n    coefs_combined = pd.concat([coefs1, coefs2])\n    df1 = pd.merge(df1, coefs1, on='NAME') \n    df2 = pd.merge(df2, coefs2, on='NAME')\n    df_combined = pd.concat([df1, df2])\n    \n      #defining our scatterplot function\n    fig = px.scatter(df_combined, x='Month',\n                     y='Temp',\n                     hover_data={'NAME': True, 'Month': True, 'Temp' : True},\n                     labels={'Month': 'Month', 'Temp': 'Temperature (°C)'},\n                     title=f'Differences in Temperatures within a Year for {country1} and {country2} during August', \n                     facet_col='Country_x',\n                     **kwargs)\n    \n    fig.update_xaxes(range=[1, 12])\n    fig.update_yaxes(range=[-10, 40])\n\n    return fig \n\n\n#test case\nmonth_vs_temp('temps.db', \"Brazil\", \"Mexico\", 2000, 8)\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\n\n\n\n\n\nClearly, we can see that Mexico experienced higher temperatures!\nThank you for reading! Good luck with your next project!!"
  },
  {
    "objectID": "posts/HW 1/index.html#introduction",
    "href": "posts/HW 1/index.html#introduction",
    "title": "Exploring Temperatures and Climate of the World!",
    "section": "",
    "text": "Today, we will be reviewing how to create interesting and interative data sets with the NOAA climate data. The data set that we will be using has a list of countries, different station names, recorded temperatures for every month of the year and more. By analyzing and organzing our data sets efficiently, we will be able to create interesting visualizations and use them to answer a variety of questions.\nBefore we start, we must download the necessary packages to create our database. Please run the code cell bellow to import Pandas, Numpy and SQL.\n\n#import all of our necessary packages\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nNow that we have our packages, we will start importing three different datasets. We must save these csv files in the same folder as our Jupyter Notebook in order to use them here. In the following code cells, we will use the call stations.head() to show the first five rows of our data in a table. This step makes it easy for us to ensure that there was no problems when uploading our datasets. Please run the following code cells and then we can get started building our database.\n\n#import the data that records the location of each station making measurements\nfilename = \"station-metadata.csv\"\nstations = pd.read_csv(filename)\nstations.head() #head will output the first five columns so that we can ensure our csv file was correctly imported\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\n\n\n\n\n\n\n\n\n#import the data set that organizes the names of our studied countries\nfilename = \"country-codes.csv\"\ncountries = pd.read_csv(filename)\ncountries.head()\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n\n\n\n\n\n\n#import the temperature data collected from our stations in the select countries\nfilename = \"temps.csv\"\ntemperatures = pd.read_csv(filename)\ntemperatures.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0"
  },
  {
    "objectID": "posts/HW 1/index.html#creating-and-organizing-our-database",
    "href": "posts/HW 1/index.html#creating-and-organizing-our-database",
    "title": "Exploring Temperatures and Climate of the World!",
    "section": "",
    "text": "Now that we have uploaded the three necessary datasets, we will create a new data frame that can be used to organize the specific data we want to use. We will use SQL to pull from our three data sets to craft out database.\n\n#using SQL and pandas to combine our three data sets\nconn = sqlite3.connect(\"temps.db\")\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\ndf = df_iter.__next__()\ndef prepare_df(df):\n    \"\"\"\n    Function that will create our dataframe with the designated columns and information \n\n    Arguments: (df) - the function takes in our pandas dataframe and will organize all of the \n                    different pieces of data from our CSV files that we uploaded above\n\n    Returns: (df) - we will be left with our updated pandas data frame\n    \"\"\"\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\n\n#continuation of combining our data sets\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\nfor i, df in enumerate(df_iter):\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\n\n#test to ensure we are correctly applying everything\nurl = \"station-metadata.csv\"\nstations = pd.read_csv(url)\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\n27585\n\n\n\nurl = \"country-codes.csv\"\ncountries = pd.read_csv(url)\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\n\n#outputs our column names \ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\nWe now have downloaded all of our different data sets into our folder. We will now use the following commands to create tables. We should see three seperate table headings in our output.\n\n#creates our empty database that we will fill with our query function\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\n#as our output matches our expectation, we have completed this task properly!\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\nWe will fill our database using a function in a seperate Python file. From this file, we will import our function and as a result our database. This function requires us to use SQL keywords (written in all capital letters) in order to easily combine different sorts of data into one whole set.\n\n#importing our function from a seperate python file saved within our folder\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n\n    \"\"\"\n    Function that uses SQL commands to organize the data from our three CSV files into the created columns\n\n    Arguments:  db_file - the file for our database (in the form of a string)\n                country - string that describes the name of the country \n                year_begin - integer value that states the start year \n                year_end - integer value that states the end year \n                month - integer value that states the specific month of date we are working with \n    \n    Returns: df - the result is a pandas data frame that organizes the values from the arguments explained above\n    \"\"\"\n\n    conn = sqlite3.connect(db_file)\n    query = f'''\n        SELECT s.NAME, s.LATITUDE, s.LONGITUDE, c.NAME AS Country, t.Year, t.Month, t.Temp\n        FROM temperatures t\n        LEFT JOIN stations s ON t.ID = s.id\n        LEFT JOIN countries c ON t.ID LIKE c.\"FIPS 10-4\" || '%'\n        WHERE c.Name = '{country}' AND t.Month = {month} AND t.Year BETWEEN {year_begin} AND {year_end}\n\n'''\n    result_df = pd.read_sql_query(query, conn)\n    conn.close()\n    return result_df\n\n\n\nIt is very important that we test our code as we go so that we can be certain we have not encountered any errors. The code cell below will create a database for temperatures in India during January between the years of 1980 and 2020. Please run the cell below to show that we correctly implemented our database.\n\n#test case for our query function \nquery_climate_database(db_file = \"temps.db\", \n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020, \n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns\n\n\n\nAs our database for India matches the sample, we have correctly created our database!"
  },
  {
    "objectID": "posts/HW 1/index.html#developing-visualizations-from-our-database",
    "href": "posts/HW 1/index.html#developing-visualizations-from-our-database",
    "title": "Exploring Temperatures and Climate of the World!",
    "section": "",
    "text": "Now that we have all our data from our three sets organized, we can use them to answer interesting questions. Visualizations are a great way to display data as they are super versatile and easy to read. We will use Plotly to create our data sets. Please import the Plotly package below.\n\n#importing plotly so that we may create visualizations\nfrom plotly import express as px\n\n\n#creating an iframe folder so that our visualizations may be visible on our blog\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nLet’s get started with our first question.\nHow does the average yearly change in temperature vary within a given country?\nThe best way to tackle this question is through creating a geographic scatter function. This figure will appear as a map but will mark specific stations in a given country and share their temperature measurements. We will start by creating a function called tenperature_coefficient_plot(). Please follow the code written below to create our first visualization. We must be sure to include the necessary parameters and be as detailed as possible with our plots.\n\n#our function will allow us to create out plot\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    Function will create a scatter map box plot that explores the average yearly change in temperature \n\n    Arguments: db_file - the file for our database that we will use to make our plot (in the form of a string)\n                country - string that describes the name of the country that will appear on our plot\n                year_begin - integer value that states the start year for our plot \n                year_end - integer value that states the end year for our plot\n                month - integer value that states the specific month of date we plotting\n\n    Returns: fig - a scatterplot map box that shows the map of a country with the different \n                   measurements placed on the location where they were taken \n    \"\"\"\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    df = df[df.groupby('NAME')['Year'].transform('count') &gt;= min_obs]\n\n    #average tenperature will allow us to use one measurement per station \n    def temperature_average(data):\n        \"\"\"\n        Function will be used to determine the average temperature recorded in a station during a given year\n\n        Arguments: data['Year'] - the integer value from our dataframe that looks at a single year \n                                 in between our year_begin and our year_end\n                   data['Temp'] - the intger value that describes the temperature recorded in the \n                                  given year\n\n        Returns: average - an integer value that will be used to calculate the yearly change\n        \"\"\"\n\n        X = data['Year']\n        y = data['Temp']\n        \n        average = np.polyfit(X, y, deg=1)[0]\n        \n        return average\n\n    #creates columns for our visalization\n    coefs = df.groupby('NAME').apply(temperature_average).reset_index()\n    coefs.columns = ['NAME', 'YearlyChange']\n\n    df = pd.merge(df, coefs, on='NAME')\n\n    #this type of visualization will appear as an interactive map with data points\n    fig = px.scatter_mapbox(df, \n                            lat='LATITUDE', \n                            lon='LONGITUDE',\n                            color='YearlyChange',\n                            hover_data = {'NAME' : True, 'YearlyChange': ':.3f'},\n                            labels = {'YearlyChange': 'Estimated Yearly Increase (Celsius)'}, \n                            title = f\"Estimates of Yearly Increase in Temperature in {pd.to_datetime(month, format='%m').month_name()} for stations in {country}, {year_begin}-{year_end}\",\n                            **kwargs)\n    fig.update_layout(mapbox_style=\"open-street-map\")\n    return fig\n\nWe will now use the following test case of India in January during 1980-2020 to test our function. As you can see, we will create a figure that appears as an interactive map that easily displays the answer to our posed question\n\n#test case \ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"temps.db\", \"India\", 1980, 2020, 1, \n                                   min_obs=10,\n                                   zoom=2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nNow, we can answer our question! For our chosen country of India, we can see that India has not experienced huge increases in temperature during the month of Januray over the period of forty years. Nevertheless, there are some stations along the coasts and borders that have seen significant increases in temperature."
  },
  {
    "objectID": "posts/HW 1/index.html#other-forms-of-visualizations-heatmaps-more-scatter-plots",
    "href": "posts/HW 1/index.html#other-forms-of-visualizations-heatmaps-more-scatter-plots",
    "title": "Exploring Temperatures and Climate of the World!",
    "section": "",
    "text": "While we are programming, we want to ask questions that we can answer using our models. By asking these questions, we can connect our data sets to real world scenarios.\nFor our next question, let’s ask which regions of the world have the highest concentration of stations recording temperature data?\nThe best way to approach this question is to create a heatmap using our knowledge of Plotly. We can start by defining the following function. We can create the figure using specific columns from our database.\n\n#heatmap/density plot will show concentrations using different colors and a key explaining each color's signficance\ndef temperature_coefficient_heatmap(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    Function will create a density/heatmap map box plot that explores the concentration of stations\n\n    Arguments: db_file - the file for our database that we will use to make our plot (in the form of a string)\n                country - string that describes the name of the country that will appear on our plot\n                year_begin - integer value that states the start year for our plot \n                year_end - integer value that states the end year for our plot\n                month - integer value that states the specific month of date we plotting\n\n    Returns: fig - a density map that resembles a map of the world and shows the concentration \n                   of stations throughout the world using colors and our key\n    \"\"\"\n    \n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    df = df[df.groupby('NAME')['Year'].transform('count') &gt;= min_obs]\n\n    def temperature_average(data):\n        \"\"\"\n        Function will be used to determine the average temperature recorded in a station during a given year\n\n        Arguments: data['Year'] - the integer value from our dataframe that looks at a single year \n                                 in between our year_begin and our year_end\n                   data['Temp'] - the intger value that describes the temperature recorded in the \n                                  given year\n\n        Returns: average - an integer value that will be used to calculate the yearly change\n        \"\"\"\n\n        X = data['Year']\n        y = data['Temp']\n        \n        average = np.polyfit(X, y, deg=1)[0]\n        \n        return average\n\n    coefs = df.groupby('NAME').apply(temperature_average).reset_index()\n    coefs.columns = ['NAME', 'YearlyChange']\n\n    df = pd.merge(df, coefs, on='NAME')\n\n   \n#outputs our figure\nfig = px.density_mapbox(stations,\n                        lat = \"LATITUDE\",\n                        lon = \"LONGITUDE\",\n                        hover_data = {'NAME' : True},\n                        radius = 1,\n                        zoom = 0,\n                        height = 300,\n                        title = \"Station Concentration Map\")\n\nfig.update_layout(title = \"Station Concentration Map\")\nfig.update_layout(mapbox_style=\"carto-positron\")\nfig.update_layout(margin={\"r\":20,\"t\":25,\"l\":20,\"b\":20})\nfig.show()\n\n\n\n\nNow that we have our plot, it is important that we understand what it is telling us. The lighter the color gets (or yellower) means that there is a higher concentration of stations recording these temperatures in our data set. In regions with only a few stations, we only note a handful of dots spread around the area.\nIn response to our question, it seems that the United States and parts of Europe have the highest concentration of stations.\nFinally, let’s create one more visualization and learn more about temperature using our data sets. During a twenty-year period, which part of Italy experienced higher average temperatures during the month of August: North Italy, Central Italy, or South Italy?\nAs someone who has lived in Italy, I am very aware of the different lifestyles of the people in each of these three regions. Moreover, I would like to inquire whether climate has played a role in these cultural differences.\nOnce again, I have decided to use a scatterplot to answer my question. However, this scatterplot will differ from the one above as it analyzes the average temperature of each station rather than the change in temperature. Please see the code below.\n\n#uses a scatterplot to explore the temperature changes within a given country\ndef regional_difference_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    Function will create a scatter map box plot that explores how temperatures differ in North \n    Italy compared to South Italy\n\n    Arguments: db_file - the file for our database that we will use to make our plot (in the form of a string)\n                country - string that describes the name of the country that will appear on our plot\n                year_begin - integer value that states the start year for our plot \n                year_end - integer value that states the end year for our plot\n                month - integer value that states the specific month of date we plotting\n\n    Returns: fig - a scatterplot map box that shows the map of a country with the different \n                   measurements placed on the location where they were taken that will make it very \n                   easy to compare the differences between regions of one country\n    \"\"\"\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    df = df[df.groupby('NAME')['Temp'].transform('count') &gt;= min_obs]\n\n    def temperature_average(df):\n        \"\"\"\n        Function will be used to determine the average temperature recorded in a station during a given year\n\n        Arguments: data['Year'] - the integer value from our dataframe that looks at a single year \n                                 in between our year_begin and our year_end\n                   data['Temp'] - the intger value that describes the temperature recorded in the \n                                  given year\n\n        Returns: average - an integer value that will be used to calculate the yearly change\n        \"\"\"\n\n        average = np.mean(df['Temp'])\n        \n        return average\n\n    coefs = df.groupby('NAME').apply(temperature_average).reset_index()\n    coefs.columns = ['NAME', 'Average Temperature']\n\n    df = pd.merge(df, coefs, on='NAME')\n\n    fig = px.scatter_mapbox(df, \n                            lat='LATITUDE', \n                            lon='LONGITUDE',\n                            color='Average Temperature',\n                            hover_data = {'NAME' : True, 'Average Temperature': ':.3f'},\n                            labels = {'Average Temperature': 'Average Temperature (Celsius)'}, \n                            title = f\"Temperatures in {pd.to_datetime(month, format='%m').month_name()} for stations in {country}, {year_begin}-{year_end}\",\n                            **kwargs)\n    fig.update_layout(mapbox_style=\"open-street-map\")\n    return fig\n\nAlthough we can apply this function to any country, month, or year, we will look at Italy in August over a twenty year period to develop our conclusion.\n\n#test case \ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = regional_difference_plot(\"temps.db\", \"Italy\", 1980, 2000, 8, \n                                   min_obs=10,\n                                   zoom=2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nIn our plot above, we can see that South Italian stations have the points with the deepest red. This means that the South of Italy experienced the highest average temperatures compared to both the North and Central regions. I believe that this is very fitting considering the lifestyle and daily practices of people who reside in Southern Italy.\nThus, we have thoroughly reviewed how to create databases and make visualizations using our temperature data sets."
  },
  {
    "objectID": "posts/HW 1/index.html#query-function-2",
    "href": "posts/HW 1/index.html#query-function-2",
    "title": "Exploring Temperatures and Climate of the World!",
    "section": "",
    "text": "Below, we will make a second query function that we will use to further explore the data within our dataframe. Follow the code below to see how we create this new database which replace the year begin and year end columns with a single year.\n\n#importing our new query function from the same file as before\nfrom climate_database import second_query_climate_database\nimport inspect\nprint(inspect.getsource(second_query_climate_database))\n\ndef second_query_climate_database(db_file, country, year, month):\n    \"\"\"\n    Function that uses SQL commands to organize the data from our three CSV files into the created columns\n\n    Arguments:  db_file - the file for our database (in the form of a string)\n                country - string that describes the name of the country \n                year - integer value that describes the year the data was collected \n                month - integer value that states the specific month of date we are working with \n    \n    Returns: df - the result is a pandas data frame that organizes the values from the arguments explained above\n    \"\"\"\n\n    conn = sqlite3.connect(db_file)\n    query = f'''\n        SELECT s.NAME, s.LATITUDE, s.LONGITUDE, c.NAME AS Country, t.Year, t.Month, t.Temp\n        FROM temperatures t\n        LEFT JOIN stations s ON t.ID = s.id\n        LEFT JOIN countries c ON t.ID LIKE c.\"FIPS 10-4\" || '%'\n        WHERE c.Name = '{country}' AND t.Year = '{year}' AND t.Month = '{month}'\n\n\n'''\n    result_df = pd.read_sql_query(query, conn)\n    conn.close()\n    return result_df\n\n\n\n\n#testing with Canada to see that our database is correctly created\nsecond_query_climate_database(db_file = \"temps.db\", \n                       country = \"Canada\", \n                       year = 2000,  \n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nCHEMAINUS\n48.9333\n-123.7500\nCanada\n2000\n1\n3.52\n\n\n1\nCOWICHAN_LAKE_FORESTRY\n48.8167\n-124.1333\nCanada\n2000\n1\n2.21\n\n\n2\nLAKE_COWICHAN\n48.8333\n-124.0500\nCanada\n2000\n1\n1.48\n\n\n3\nDUNCAN_KELVIN_CREEK\n48.7333\n-123.7333\nCanada\n2000\n1\n2.02\n\n\n4\nESQUIMALT_HARBOUR\n48.4333\n-123.4333\nCanada\n2000\n1\n4.58\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1531\nMAKKOVIK_A\n55.0833\n-59.1833\nCanada\n2000\n1\n-15.06\n\n\n1532\nWABUSH_LAKE_A\n52.9333\n-66.8667\nCanada\n2000\n1\n-22.00\n\n\n1533\nCHAPLEAU_ARPTSAWR\n47.6500\n-83.3500\nCanada\n2000\n1\n-15.30\n\n\n1534\nFROBISHER_BAY\n63.8000\n-68.6000\nCanada\n2000\n1\n-27.50\n\n\n1535\nKNOB_LAKE_QUE\n54.8000\n-66.7000\nCanada\n2000\n1\n-23.90\n\n\n\n\n1536 rows × 7 columns\n\n\n\nNow, we can use this new data set in interesting ways!"
  },
  {
    "objectID": "posts/HW 1/index.html#multi-faceted-graphes",
    "href": "posts/HW 1/index.html#multi-faceted-graphes",
    "title": "Exploring Temperatures and Climate of the World!",
    "section": "",
    "text": "Multi-faceted graphes are a great way for us to share more information through out graphes. Facets allow us to break up our graphs so that we have a figure that resembles two plots side by side. For our data set, graphs with facets are great for us to show examine the varying temperatures of two different countries. This leads us to our question, who experienced warmer temperatures in August of 2000: Brazil or Mexico?\nLet’s start by defininng our function! We will be using data from our second query function.\n\ndef month_vs_temp(db_file, country1, country2, year, month, **kwargs): \n    \"\"\"\n    Function will create a basic scatter plot that explores how temperatures differ in Brazil \n    and Mexico during the given month and year\n\n    Arguments: db_file - the file for our database that we will use to make our plot (in the form of a string)\n                country - string that describes the name of the country that will appear on our plot\n                year - integer value that describes the year the data was collected\n                month - integer value that states the specific month of date we plotting\n\n    Returns: fig - a scatterplot that will plot Month vs Temperature for two countries contained in \n                   seperate facets that will make it appear as two graphes plotted on one plane\n    \"\"\"\n\n      #using our second query function with four inputs\n    df1 = second_query_climate_database(db_file, country1, year, month)\n    df2 = second_query_climate_database(db_file, country2, year, month)\n\n      #defining an equation for our coefficients\n    def coef(group):\n          X = group['Year']\n          y = group['Temp']\n          coef = np.polyfit(X, y, deg=1)[0]\n          return coef\n\n      #using our coefficients to combine our two countries into one graph\n    coefs1 = df1.groupby('NAME').apply(coef).reset_index()\n    coefs2 = df2.groupby('NAME').apply(coef).reset_index()\n    coefs1.columns = ['NAME','YearlyChange']\n    coefs1['Country'] = country1\n    coefs2.columns = ['NAME', 'YearlyChange']\n    coefs2['Country'] = country2\n   \n    coefs_combined = pd.concat([coefs1, coefs2])\n    df1 = pd.merge(df1, coefs1, on='NAME') \n    df2 = pd.merge(df2, coefs2, on='NAME')\n    df_combined = pd.concat([df1, df2])\n    \n      #defining our scatterplot function\n    fig = px.scatter(df_combined, x='Month',\n                     y='Temp',\n                     hover_data={'NAME': True, 'Month': True, 'Temp' : True},\n                     labels={'Month': 'Month', 'Temp': 'Temperature (°C)'},\n                     title=f'Differences in Temperatures within a Year for {country1} and {country2} during August', \n                     facet_col='Country_x',\n                     **kwargs)\n    \n    fig.update_xaxes(range=[1, 12])\n    fig.update_yaxes(range=[-10, 40])\n\n    return fig \n\n\n#test case\nmonth_vs_temp('temps.db', \"Brazil\", \"Mexico\", 2000, 8)\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\nc:\\Users\\woulf\\anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:767: RankWarning:\n\nPolyfit may be poorly conditioned\n\n\n\n\n\n\nClearly, we can see that Mexico experienced higher temperatures!\nThank you for reading! Good luck with your next project!!"
  },
  {
    "objectID": "posts/HW 4/index.html",
    "href": "posts/HW 4/index.html",
    "title": "Heat Diffusion in Two Dimensions - Comparing Simulation Methods",
    "section": "",
    "text": "Hi everyone! Welcome to my blog! Today we will be using different methods to examine a simulation that models heat diffusion in two dimensions. Although we have taken the time to explore heat diffusion in one dimension, heat diffusion in two dimensions uses an updated equation and different steps. In addition, we will be able to compare these different methods and best decide which methods are best for us to use on our future projects.\nFirst we must download the necessary packages. Numpy will allow us to work with arrays. Matplotlob will allow us to make visualizations. Jax allows us to complete complex numerical computing projects. The time package will allow us to time how long it takes for the total number of iterations to occur. This will help us decide which method we prefer. The inspect package will allow us to implemenent our code correctly especially those from different files within our folder. Please run the code cell below.\n\n#given in blog post directions\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport jax\nimport jax.numpy as jnp\nimport time\nfrom jax.experimental import sparse\nimport inspect\n\nNow that we have introduced today’s topics and imported the necessary Python packages, let’s explore the different methods of stimulating two dimensional heat diffusion. Please follow along and think of the similarities and differences between each method. By the end of this post, we will be able to identify which methods we prefer.\n\n\n\nFirst we must include the following function which was given to us in the assignment directions. This function allows us to work with the different iterations of the function and use matric multiplication to create our simulation. This function was provided in our homework directions. Please run the code cell below before exploring each of the methods.\nDISCLAIMER: Although it is considered good coding practice to import our functions from our Python files to our Jupyter notebooks, I encountered several errors as I work on a Windows system where some elements of JAX are not completely supported. Please take this into consideration while reading this post.\nGenerally, we would run the following: from heat_equation import advance_time_matvecmul print(inspect.getsource(advance_time_matvecmul))\nAnyways, please continue to the cell below.\n\n@jax.jit  \ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"\n    This function allows us to complete the simulation using matrix multiplication\n    Args:\n        A: two-dimensional matrix, N^2 x N^2 \n        u: N x N grid state at timestep k\n        epsilon: stability constant\n\n    Returns:\n        u: a matrix of the designated size (N x N)\n    \"\"\"\n    #used to determine the matrix size \n    N = u.shape[0]\n    #updates our u value\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N)) \n    return u\n\nNow that we have imported our function, we will use the following intial conditions to define the size and features of our array. We will also define our initial array which will be an array of the designated size with all entries equal to 0. The code cell below will create our intial visualization that we will apply our methods to show heat diffusion.\n\nN = 101\nepsilon = 0.2\n\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n\n\n\n\n\n\n\nNext, we will import the get_A(N) function from our seperate python file (heat_equation.py). This file must be saved in the same folder on your device as this Jupyter notebook file. Our first matrix is similar to the one dimensional matrix that we worked with in lecture. This method will create finite matrix by manually describing the entries. Please run the code cell below.\n\ndef get_A(N): \n    \"\"\"\n    This function creates a finite matrix that can be used to model heat distribution in two-dimensions \n    Args: \n        N: square root of the row and column length for A  \n    Returns: \n        A: a finite matrix of the designated size (N^2 x N^2) \n    \"\"\"\n    n = N * N\n    #inputs our diagonal entries of the matrix\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    \n    #set entries equal to 0\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    \n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\nNow that we have set up the function to create the array for our first method, we can create our visualization and determine the time it takes to generate. We will be running our code for 2700 iterations. After every 300 iterations, the following function will produce a plot to update the process. Please create the visualizations by running the cell below.\n\nsolutions = dict()\nstart_time = time.time()\nA = get_A(N)\n\nfig, axs = plt.subplots(3,3)\nfor i in range(2700):\n    #runs the iteration\n    u0 = advance_time_matvecmul(A, u0, epsilon)\n    solutions[i] = u0\n\n#determine the time it takes to create the simulation    \nend_time = time.time()\ntotal_time = end_time - start_time\n\n#creates the plots for every 300 iterations\nfor i in range(2700):\n    if (i + 1) % 300 == 0:\n        ax = axs[i // 300 // 3, i // 300 % 3]\n        ax.imshow(solutions[i])\n        ax.set_title(f'Iteration {i+1}')\n\n#outputs the elapsed time\nprint(f\"time it took: {total_time:0.2f} sec\")\n\ntime it took: 828.93 sec\n\n\n\n\n\n\n\n\n\nWe have successfully created nine heatmaps to show the progress of our 2700 itertaions! But this function took over 800 seconds to create our simulation. Even though our result was a success, there is no denying that this may not be the most efficient way of showing two dimensional heat diffusion. Let’s continue on to our other methods and see how our results vary.\n\n\n\nIn this secind method, the matrix A will be returned in sparse format. Moreover, we will be utilizing the batched coordinate format (BCOO) and take in a slightly different value than the matrix we created above. Please follow the steps below to create our resulting array. This function resembles the one we defined above but contains a couple key changes. Please run the code cell below to implment our sparse function.\n\ndef get_sparse_A(N): \n    \"\"\"\n    This function creates a finite matrix that is sparse\n    Args:\n        N: square root of the row and column length for A \n    Returns: \n        A: a finite difference matrix that is in sparse format \n    \"\"\"\n    n = N * N\n    #inputs our diagonal entries of the matrix \n    diagonals = [-4 * jnp.ones(n), jnp.ones(n-1), jnp.ones(n-1), jnp.ones(n-N), jnp.ones(n-N)] \n    \n    #set entries equal to 0\n    diagonals[1] = diagonals[1].at[(N-1)::N].set(0)\n    diagonals[2] = diagonals[2].at[(N-1)::N].set(0)\n     \n    A = jnp.diag(diagonals[0]) + jnp.diag(diagonals[1], 1) + jnp.diag(diagonals[2], -1) + jnp.diag(diagonals[3], N) + jnp.diag(diagonals[4], -N)\n    \n    A_sp_matrix = sparse.BCOO.fromdense(A)\n    return A_sp_matrix\n\nNow that we have successfully created our function, we can start with our visulizations and time measurements. However, we must update some of our values (such as u0) to account for the changes we implemented in method 1. After we complete this step, we will create our visulizations using a method that is very similar to method 1. Finally, we will use the same lines of code to determine the elapsed time. Please run the code cell below.\n\n#update your entries\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n#apply the function for method 2\nA_sparse = get_sparse_A(N)\n\nsolutions = dict()\nstart_time = time.time()\n\nfig, axs = plt.subplots(3,3)\nfor i in range(2700):\n    u0 = advance_time_matvecmul(A_sparse, u0, epsilon)\n    solutions[i] = u0\n\n#determine elapsed time\nend_time = time.time()\ntotal_time = end_time - start_time\n\n#create visualizations\nfor i in range(2700):\n    if (i + 1) % 300 == 0:\n        ax = axs[i // 300 // 3, i // 300 % 3]\n        ax.imshow(solutions[i])\n        ax.set_title(f'Iteration {i+1}')\n\nprint(f\"time it took: {total_time:0.2f} sec\")\n\ntime it took: 1.27 sec\n\n\n\n\n\n\n\n\n\nWe have successfully created another simulation! This method was much faster than our first one and took a little more than one second to run. Now, we can move on to our third method.\n\n\n\nFor our third method of creating our array and visualization, we will be able to achieve our result through a different sort of computation. This method works best with Poisson equations and may not be the most efficient implementation of heat diffusion. This function involves lots of Numpy principles which is the Python package that allows us to work with arrays. Please follow the code cell below to see how I created this function.\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"\n    This function allows us to make our simulation using elements of numpy\n    Args:\n        u: a matrix of the designated size (N x N)\n        epsilon: stability constant\n\n    Returns:\n        u_new: a finite grid under the defined conditions\n    \"\"\"\n    N = u.shape[0] \n    u_pad = np.pad(u, pad_width=1, mode='constant', constant_values=0)\n     \n    #use numpy to enter values into our array\n    u_new = u + epsilon * (-4*u_pad[1:-1, 1:-1] + u_pad[:-2, 1:-1] + u_pad[2:, 1:-1] + u_pad[1:-1, :-2] + u_pad[1:-1, 2:])\n    return u_new\n\nAfter importing the function, we will now create our visualization and determined our elapsed time. As we defined this function in a slightly different way than the functions of the first two methods, the lines that we use to create our visualization will also be different. However, we will still be using the time function to determine the length of time from when our iterations start to when we reach iteration 2700. Once again, we will be redefining our intial entries to ensure that running the same simulation mutlitple ways does not create any problems. Please follow the code below.\n\n#updates our entries\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\nsolutions = dict()\nstart_time = time.time()\n\n#run our simulation\nfig, axs = plt.subplots(3,3)\nfor i in range(2700):\n    u0 = advance_time_numpy(u0, epsilon)\n    # store immediate solution\n    solutions[i] = u0\n\n#determine elapsed time\nend_time = time.time()\ntotal_time = end_time - start_time\n\n#create our visualizations\nfor i in range(2700):\n    if (i + 1) % 300 == 0:\n        ax = axs[i // 300 // 3, i // 300 % 3]\n        ax.imshow(solutions[i])\n        ax.set_title(f'Iteration {i+1}')\n\nprint(f\"time it took: {total_time:0.2f} sec\")\n\ntime it took: 0.62 sec\n\n\n\n\n\n\n\n\n\nAs seen in our images above, we have 9 iterations that occur every 300 iterations out of 2700. These iterations are labeled so that a viewer can see and easily understand the two dimensional heat diffusion that occurs. Our elapsed time is 0.62 seconds for all 2700 iterations to run. This means that our method is the fastest yet! We will keep this value in mind so that we may compare it in our conclusion (section 5).\n\n\n\nThe final method we look at will create our array using JAX operations. The Jax package in python allows us to complete complex numerical computations in a more efficient and clear way. In this method, we do not implement the sparse matrix multiplication processes as we have above. Please follow the code cell below and implement the function for our final method.\n\n@jax.jit  #allows us to use certain elements of jax and jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"\n    This function allows us to create our simulation using elements of jax\n    Args:\n        u: a matrix of the designated size (N x N)\n        epsilon: stability constant\n\n    Returns:\n        u_new: a finite grid under the defined conditions\n    \"\"\"\n    N = u.shape[0]\n    u_pad = jnp.pad(u, pad_width=1, mode='constant', constant_values=0)\n    \n    #updates our simulation using elements of the jax package\n    u_new = u + epsilon * (-4*u_pad[1:-1, 1:-1] + u_pad[:-2, 1:-1] + u_pad[2:, 1:-1] + u_pad[1:-1, :-2] + u_pad[1:-1, 2:])\n    return u_new\n\nNow that we have successfully imported our function, we can create our final two-dimensional heat diffusion visualization. This will be created using a function that is very similar to the one we created for method 3. Additionally, we will also be recording the elapsed time for our iterations to occur in the same way as every other method. Please remember that we must reset the intial values for our starting array. Please run the code cell below.\n\n#update our entries\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n#run the simulation\nsolutions = dict()\nstart_time = time.time()\n\nfig, axs = plt.subplots(3,3)\nfor i in range(2700):\n    u0 = advance_time_jax(u0, epsilon)\n    solutions[i] = u0\n\n#determine elapsed time    \nend_time = time.time()\ntotal_time = end_time - start_time\n\n#create our visualization\nfor i in range(2700):\n    if (i + 1) % 300 == 0:\n        ax = axs[i // 300 // 3, i // 300 % 3]\n        ax.imshow(solutions[i])\n        ax.set_title(f'Iteration {i+1}')\n\nprint(f\"time it took: {total_time:0.2f} sec\")\n\ntime it took: 0.32 sec\n\n\n\n\n\n\n\n\n\nWe have successfully created our final set of visualizations!! All nine are identical to the three previous methods which is great. The 2700 iterations took about 0.32 seconds to complete. This is our fastest simulation yet! Now, we can finally look back on our different methods and compare.\n\n\n\nAlthough some methods may be more efficient than others, they are all equally effective at modelling two dimensional heat diffusion. Method 1 took 828.93 seconds for all iterations to occur. Method 2 took 1.27 seconds for all iterations to occur. Method 3 took 0.62 seconds for all iterations to occur. Method 4 took 0.32 seconds for all iterations to occur. Therefore, Method 4 is the fast function we implemented and Method 1 was by far the slowest. In my opinion, Method 1 was the easiest to write as it closely resembled what we completed in lecture. Nevertheless, I would never use Method 1 in a real situation as all the iterations took more than 10 minutes to run. In the future, I would further my understanding of Jax elements and strive to use Method 4. Regardless of which was the easiest or fastest method for this specific situation, it is important to learn and understand all four methods because different methods will be useful for different situations we will encounter in this class.\nThank you for reading!"
  },
  {
    "objectID": "posts/HW 4/index.html#introduction-heat-diffusion-explained-and-basic-set-up",
    "href": "posts/HW 4/index.html#introduction-heat-diffusion-explained-and-basic-set-up",
    "title": "Heat Diffusion in Two Dimensions - Comparing Simulation Methods",
    "section": "",
    "text": "Hi everyone! Welcome to my blog! Today we will be using different methods to examine a simulation that models heat diffusion in two dimensions. Although we have taken the time to explore heat diffusion in one dimension, heat diffusion in two dimensions uses an updated equation and different steps. In addition, we will be able to compare these different methods and best decide which methods are best for us to use on our future projects.\nFirst we must download the necessary packages. Numpy will allow us to work with arrays. Matplotlob will allow us to make visualizations. Jax allows us to complete complex numerical computing projects. The time package will allow us to time how long it takes for the total number of iterations to occur. This will help us decide which method we prefer. The inspect package will allow us to implemenent our code correctly especially those from different files within our folder. Please run the code cell below.\n\n#given in blog post directions\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport jax\nimport jax.numpy as jnp\nimport time\nfrom jax.experimental import sparse\nimport inspect\n\nNow that we have introduced today’s topics and imported the necessary Python packages, let’s explore the different methods of stimulating two dimensional heat diffusion. Please follow along and think of the similarities and differences between each method. By the end of this post, we will be able to identify which methods we prefer."
  },
  {
    "objectID": "posts/HW 4/index.html#method-1-matrix-multiplication",
    "href": "posts/HW 4/index.html#method-1-matrix-multiplication",
    "title": "Heat Diffusion in Two Dimensions - Comparing Simulation Methods",
    "section": "",
    "text": "First we must include the following function which was given to us in the assignment directions. This function allows us to work with the different iterations of the function and use matric multiplication to create our simulation. This function was provided in our homework directions. Please run the code cell below before exploring each of the methods.\nDISCLAIMER: Although it is considered good coding practice to import our functions from our Python files to our Jupyter notebooks, I encountered several errors as I work on a Windows system where some elements of JAX are not completely supported. Please take this into consideration while reading this post.\nGenerally, we would run the following: from heat_equation import advance_time_matvecmul print(inspect.getsource(advance_time_matvecmul))\nAnyways, please continue to the cell below.\n\n@jax.jit  \ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"\n    This function allows us to complete the simulation using matrix multiplication\n    Args:\n        A: two-dimensional matrix, N^2 x N^2 \n        u: N x N grid state at timestep k\n        epsilon: stability constant\n\n    Returns:\n        u: a matrix of the designated size (N x N)\n    \"\"\"\n    #used to determine the matrix size \n    N = u.shape[0]\n    #updates our u value\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N)) \n    return u\n\nNow that we have imported our function, we will use the following intial conditions to define the size and features of our array. We will also define our initial array which will be an array of the designated size with all entries equal to 0. The code cell below will create our intial visualization that we will apply our methods to show heat diffusion.\n\nN = 101\nepsilon = 0.2\n\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n\n\n\n\n\n\n\nNext, we will import the get_A(N) function from our seperate python file (heat_equation.py). This file must be saved in the same folder on your device as this Jupyter notebook file. Our first matrix is similar to the one dimensional matrix that we worked with in lecture. This method will create finite matrix by manually describing the entries. Please run the code cell below.\n\ndef get_A(N): \n    \"\"\"\n    This function creates a finite matrix that can be used to model heat distribution in two-dimensions \n    Args: \n        N: square root of the row and column length for A  \n    Returns: \n        A: a finite matrix of the designated size (N^2 x N^2) \n    \"\"\"\n    n = N * N\n    #inputs our diagonal entries of the matrix\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    \n    #set entries equal to 0\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    \n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\nNow that we have set up the function to create the array for our first method, we can create our visualization and determine the time it takes to generate. We will be running our code for 2700 iterations. After every 300 iterations, the following function will produce a plot to update the process. Please create the visualizations by running the cell below.\n\nsolutions = dict()\nstart_time = time.time()\nA = get_A(N)\n\nfig, axs = plt.subplots(3,3)\nfor i in range(2700):\n    #runs the iteration\n    u0 = advance_time_matvecmul(A, u0, epsilon)\n    solutions[i] = u0\n\n#determine the time it takes to create the simulation    \nend_time = time.time()\ntotal_time = end_time - start_time\n\n#creates the plots for every 300 iterations\nfor i in range(2700):\n    if (i + 1) % 300 == 0:\n        ax = axs[i // 300 // 3, i // 300 % 3]\n        ax.imshow(solutions[i])\n        ax.set_title(f'Iteration {i+1}')\n\n#outputs the elapsed time\nprint(f\"time it took: {total_time:0.2f} sec\")\n\ntime it took: 828.93 sec\n\n\n\n\n\n\n\n\n\nWe have successfully created nine heatmaps to show the progress of our 2700 itertaions! But this function took over 800 seconds to create our simulation. Even though our result was a success, there is no denying that this may not be the most efficient way of showing two dimensional heat diffusion. Let’s continue on to our other methods and see how our results vary."
  },
  {
    "objectID": "posts/HW 4/index.html#method-2-sparse-matrix-jax",
    "href": "posts/HW 4/index.html#method-2-sparse-matrix-jax",
    "title": "Heat Diffusion in Two Dimensions - Comparing Simulation Methods",
    "section": "",
    "text": "In this secind method, the matrix A will be returned in sparse format. Moreover, we will be utilizing the batched coordinate format (BCOO) and take in a slightly different value than the matrix we created above. Please follow the steps below to create our resulting array. This function resembles the one we defined above but contains a couple key changes. Please run the code cell below to implment our sparse function.\n\ndef get_sparse_A(N): \n    \"\"\"\n    This function creates a finite matrix that is sparse\n    Args:\n        N: square root of the row and column length for A \n    Returns: \n        A: a finite difference matrix that is in sparse format \n    \"\"\"\n    n = N * N\n    #inputs our diagonal entries of the matrix \n    diagonals = [-4 * jnp.ones(n), jnp.ones(n-1), jnp.ones(n-1), jnp.ones(n-N), jnp.ones(n-N)] \n    \n    #set entries equal to 0\n    diagonals[1] = diagonals[1].at[(N-1)::N].set(0)\n    diagonals[2] = diagonals[2].at[(N-1)::N].set(0)\n     \n    A = jnp.diag(diagonals[0]) + jnp.diag(diagonals[1], 1) + jnp.diag(diagonals[2], -1) + jnp.diag(diagonals[3], N) + jnp.diag(diagonals[4], -N)\n    \n    A_sp_matrix = sparse.BCOO.fromdense(A)\n    return A_sp_matrix\n\nNow that we have successfully created our function, we can start with our visulizations and time measurements. However, we must update some of our values (such as u0) to account for the changes we implemented in method 1. After we complete this step, we will create our visulizations using a method that is very similar to method 1. Finally, we will use the same lines of code to determine the elapsed time. Please run the code cell below.\n\n#update your entries\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n#apply the function for method 2\nA_sparse = get_sparse_A(N)\n\nsolutions = dict()\nstart_time = time.time()\n\nfig, axs = plt.subplots(3,3)\nfor i in range(2700):\n    u0 = advance_time_matvecmul(A_sparse, u0, epsilon)\n    solutions[i] = u0\n\n#determine elapsed time\nend_time = time.time()\ntotal_time = end_time - start_time\n\n#create visualizations\nfor i in range(2700):\n    if (i + 1) % 300 == 0:\n        ax = axs[i // 300 // 3, i // 300 % 3]\n        ax.imshow(solutions[i])\n        ax.set_title(f'Iteration {i+1}')\n\nprint(f\"time it took: {total_time:0.2f} sec\")\n\ntime it took: 1.27 sec\n\n\n\n\n\n\n\n\n\nWe have successfully created another simulation! This method was much faster than our first one and took a little more than one second to run. Now, we can move on to our third method."
  },
  {
    "objectID": "posts/HW 4/index.html#method-3-direct-operation-numpy",
    "href": "posts/HW 4/index.html#method-3-direct-operation-numpy",
    "title": "Heat Diffusion in Two Dimensions - Comparing Simulation Methods",
    "section": "",
    "text": "For our third method of creating our array and visualization, we will be able to achieve our result through a different sort of computation. This method works best with Poisson equations and may not be the most efficient implementation of heat diffusion. This function involves lots of Numpy principles which is the Python package that allows us to work with arrays. Please follow the code cell below to see how I created this function.\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"\n    This function allows us to make our simulation using elements of numpy\n    Args:\n        u: a matrix of the designated size (N x N)\n        epsilon: stability constant\n\n    Returns:\n        u_new: a finite grid under the defined conditions\n    \"\"\"\n    N = u.shape[0] \n    u_pad = np.pad(u, pad_width=1, mode='constant', constant_values=0)\n     \n    #use numpy to enter values into our array\n    u_new = u + epsilon * (-4*u_pad[1:-1, 1:-1] + u_pad[:-2, 1:-1] + u_pad[2:, 1:-1] + u_pad[1:-1, :-2] + u_pad[1:-1, 2:])\n    return u_new\n\nAfter importing the function, we will now create our visualization and determined our elapsed time. As we defined this function in a slightly different way than the functions of the first two methods, the lines that we use to create our visualization will also be different. However, we will still be using the time function to determine the length of time from when our iterations start to when we reach iteration 2700. Once again, we will be redefining our intial entries to ensure that running the same simulation mutlitple ways does not create any problems. Please follow the code below.\n\n#updates our entries\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\nsolutions = dict()\nstart_time = time.time()\n\n#run our simulation\nfig, axs = plt.subplots(3,3)\nfor i in range(2700):\n    u0 = advance_time_numpy(u0, epsilon)\n    # store immediate solution\n    solutions[i] = u0\n\n#determine elapsed time\nend_time = time.time()\ntotal_time = end_time - start_time\n\n#create our visualizations\nfor i in range(2700):\n    if (i + 1) % 300 == 0:\n        ax = axs[i // 300 // 3, i // 300 % 3]\n        ax.imshow(solutions[i])\n        ax.set_title(f'Iteration {i+1}')\n\nprint(f\"time it took: {total_time:0.2f} sec\")\n\ntime it took: 0.62 sec\n\n\n\n\n\n\n\n\n\nAs seen in our images above, we have 9 iterations that occur every 300 iterations out of 2700. These iterations are labeled so that a viewer can see and easily understand the two dimensional heat diffusion that occurs. Our elapsed time is 0.62 seconds for all 2700 iterations to run. This means that our method is the fastest yet! We will keep this value in mind so that we may compare it in our conclusion (section 5)."
  },
  {
    "objectID": "posts/HW 4/index.html#method-4-jax",
    "href": "posts/HW 4/index.html#method-4-jax",
    "title": "Heat Diffusion in Two Dimensions - Comparing Simulation Methods",
    "section": "",
    "text": "The final method we look at will create our array using JAX operations. The Jax package in python allows us to complete complex numerical computations in a more efficient and clear way. In this method, we do not implement the sparse matrix multiplication processes as we have above. Please follow the code cell below and implement the function for our final method.\n\n@jax.jit  #allows us to use certain elements of jax and jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"\n    This function allows us to create our simulation using elements of jax\n    Args:\n        u: a matrix of the designated size (N x N)\n        epsilon: stability constant\n\n    Returns:\n        u_new: a finite grid under the defined conditions\n    \"\"\"\n    N = u.shape[0]\n    u_pad = jnp.pad(u, pad_width=1, mode='constant', constant_values=0)\n    \n    #updates our simulation using elements of the jax package\n    u_new = u + epsilon * (-4*u_pad[1:-1, 1:-1] + u_pad[:-2, 1:-1] + u_pad[2:, 1:-1] + u_pad[1:-1, :-2] + u_pad[1:-1, 2:])\n    return u_new\n\nNow that we have successfully imported our function, we can create our final two-dimensional heat diffusion visualization. This will be created using a function that is very similar to the one we created for method 3. Additionally, we will also be recording the elapsed time for our iterations to occur in the same way as every other method. Please remember that we must reset the intial values for our starting array. Please run the code cell below.\n\n#update our entries\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n#run the simulation\nsolutions = dict()\nstart_time = time.time()\n\nfig, axs = plt.subplots(3,3)\nfor i in range(2700):\n    u0 = advance_time_jax(u0, epsilon)\n    solutions[i] = u0\n\n#determine elapsed time    \nend_time = time.time()\ntotal_time = end_time - start_time\n\n#create our visualization\nfor i in range(2700):\n    if (i + 1) % 300 == 0:\n        ax = axs[i // 300 // 3, i // 300 % 3]\n        ax.imshow(solutions[i])\n        ax.set_title(f'Iteration {i+1}')\n\nprint(f\"time it took: {total_time:0.2f} sec\")\n\ntime it took: 0.32 sec\n\n\n\n\n\n\n\n\n\nWe have successfully created our final set of visualizations!! All nine are identical to the three previous methods which is great. The 2700 iterations took about 0.32 seconds to complete. This is our fastest simulation yet! Now, we can finally look back on our different methods and compare."
  },
  {
    "objectID": "posts/HW 4/index.html#comparison-of-methods",
    "href": "posts/HW 4/index.html#comparison-of-methods",
    "title": "Heat Diffusion in Two Dimensions - Comparing Simulation Methods",
    "section": "",
    "text": "Although some methods may be more efficient than others, they are all equally effective at modelling two dimensional heat diffusion. Method 1 took 828.93 seconds for all iterations to occur. Method 2 took 1.27 seconds for all iterations to occur. Method 3 took 0.62 seconds for all iterations to occur. Method 4 took 0.32 seconds for all iterations to occur. Therefore, Method 4 is the fast function we implemented and Method 1 was by far the slowest. In my opinion, Method 1 was the easiest to write as it closely resembled what we completed in lecture. Nevertheless, I would never use Method 1 in a real situation as all the iterations took more than 10 minutes to run. In the future, I would further my understanding of Jax elements and strive to use Method 4. Regardless of which was the easiest or fastest method for this specific situation, it is important to learn and understand all four methods because different methods will be useful for different situations we will encounter in this class.\nThank you for reading!"
  },
  {
    "objectID": "posts/HW2/index.html",
    "href": "posts/HW2/index.html",
    "title": "Web Scraping",
    "section": "",
    "text": "Let’s start of today’s blog post by asking the following question: what movie or TV shows share actors with your favorite movie or show?\nHave you ever finished a great movie just to immediately look up every behind the scenes detail on the internet? I know that I definitely have. Often times, I find that if I enjoy one movie I am more likely to enjoy a second movie created by the same director or starring the same actor. Rather than going down a tedious “Google rabbit hole”, we can create a program that takes a movie and shows us which projects we can see the same actors in.\nWe will complete this project using the Python package called scrapy. This package allows us to extract data from a specific website and use it in our own way. We must start our project by importing all the necessary Python packages.\n\nimport scrapy\nimport pandas as pd\nimport numpy as py\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nScrapy allows us to webscrape and gather data from an online database. Pandas and Numpy enables us to create out own data frame with the information we extracted through webscraping and manipulate them. With Plotly, we can create visualizations using the elements of our dataframe. For our demo, we will use the 2020 movie “Emma.” starring Anya Joy Taylor. This personal favorite of mine stars very notable actors and actresses and is widely known as it is based off a book by Jane Austen. Many of these actors have overlapping projects with one another. Moreover, we can assume that someone who enjoys the movie “Emma.” will enjoy movies with a similar cast. This leads us to our question! The best way to answer our question and make some excellent movie recommendations will be to create a data frame and a visualization.\n\n\n\nWe created our scraper using two seperate Python files and the Scrapy package. The ‘settings.py’ file was autofilled with all of the necessary lines of code to enable our scraper. In this file, it is very important to change the user agent in order to avoid encountering errors while interacting with different websites.\nIn the ‘tmdb_spiderr.py’ file we start by importing scrapy and defining our class. Within our class, we will have three functions. These functions will all work together to extract specific information from our online database.\nThe first function ‘def init’ is set. The only thing that changes in this function is the exact url for our specific movie.\nOur next function is ‘def parse’. In this function, we attach more elements to our original url so that it now links to the cast page from the movie overview page. We pass the result of this function to the next function so that we may start extracting cast information from our website. Our first function is listed in the following code cell.\n\n#parse function definition \ndef parse(self, response): \n        \"\"\"\n        Function will help us identify the specific urls for the cast and crew members\n        \n        Arguments:\n            self : instance of our class and our function\n            response : our output of our function (will return the cast url)\n\n        Returns: \n            the url of our cast members so that we can see other films that they have worked on \n        \"\"\"\n        \n        #identifies the url for our cast and crew list for the movie we selected\n        cast_url = response.url + '/cast/'  \n        \n        #yields parse_full_credits using scrapy request\n        yield scrapy.Request(url = cast_url, callback = self.parse_full_credits)\n\nNext, we will define our ‘parse_full_credits’ function. We will use the call “response.xpath” along with specific source code from the website itself to identify each member of our movie’s cast. We will then use the url for each actor to pull information from their personal page and determine which movies each actor has worked in. The code for this function is written in the cell below.\n\n#parse_full_credits function definition\ndef parse_full_credits(self, response): \n        \"\"\"\n         Function will help us extract the urls for each actor and their individual page\n        \n        Arguments:\n            self : instance of our class and our function\n            response : our output of our function (will return the cast url)\n\n        Returns: \n            the url of our cast members so that we can see other films that they have worked on \n        \"\"\"\n        \n        #identifies cast\n        table1 = response.xpath('//h3[contains(., \"Cast\")]/following-sibling::ol[1]')\n        \n        #finds each specific actor's page\n        urls_actors = table1.xpath('.//li//div[contains(@class, \"info\")]/descendant-or-self::*/p/a/@href').getall()\n        \n        #yields our list of actor urls\n        for link in urls_actors: \n            yield scrapy.Request(url = response.urljoin(link), callback = self.parse_actor_page) \n\nFinally, we will create one more function called ‘def parse_actor_page’. This function will actually pull the movie and TV show names from the list of credits for each actor. It is important that we select the correct part of the website to use in the ‘response.css’. After implementing the code from the cell below, we will be able to create our CSV file.\n\n#parse_actor_page function defintion \ndef parse_actor_page(self, response): \n        \"\"\"\n        Function will help us organize and display each actor's name and their other movies and TV shows\n        \n        Arguments:\n            self : instance of our class and our function\n            response : our output of our function (will return the cast url)\n\n        Returns: \n            all of the names of specific actors in our movie and the other projects they have worked on  \n        \"\"\"\n        \n        #finds actor names\n        actor_name = response.css('h2.title &gt; a::text').get()\n        \n        #find lists of actor projects \n        table = response.xpath('//h3[text()=\"Acting\"]/following-sibling::table[1]')\n        \n        #find names of actor movies and TV shows within the list of actor projects  \n        movie_names = table.css('td.role.true.account_adult_false.item_adult_false &gt; a.tooltip &gt; bdi::text').getall()\n        \n        #yields our results\n        for movie_or_TV_name in movie_names: \n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name} \n\nNow that we have created all of our functions, we will now be able to create a CSV file that contains both actor names and movie/show names. Run the following line in the terminal: #### scrapy crawl tmdb_spider -o results3.csv -a subdir=556678-emma We will now have a CSV file in our ‘TMDB_scraper’ file. After completing all of the above steps, we can now use our extracted data to create a dataframe and a visualization.\n\n\n\nNow that we have gathered our data using our webscraper, we will now organzie it into a database. We will start by simporting our csv file that we filled using our scraper. Please ensure that this CSV file is saved in the same location as your Jupyter notebook.\n\n#creating our initial database using out CSV file\nfilename = \"results.csv\"\ndf = pd.read_csv(filename)\ndf\n\nFileNotFoundError: [Errno 2] No such file or directory: 'results.csv'\n\n\nAfter printing our dataframe, we can see that it looks extremely similar to the table in our csv file. This is great! However, in order to find movies that share the same actors, we must know which of the films in our “movie_or_TV_name” column have multiple actors from “Emma.”. We can do this by creating a sorted list and adding a new column to our dataframe. We can implement this using the code below.\n\n#identifies the movies that have more that stars more than one of our 50 actors in \"Emma.\":\nactor_counts_per_movie = df.groupby('movie_or_TV_name')['actor'].nunique()\nsorted_actor_counts = actor_counts_per_movie.sort_values(ascending=False)\n\n#here we will make our above list a column within our dataframe: \ndf['actor_counts_per_movie'] = df['movie_or_TV_name'].map(actor_counts_per_movie)\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nactor_counts_per_movie\n\n\n\n\n0\nAnya Taylor-Joy\nSplit\n1\n\n\n1\nAnya Taylor-Joy\nThe Witch\n1\n\n\n2\nAnya Taylor-Joy\nThe Menu\n1\n\n\n3\nAnya Taylor-Joy\nThe Queen's Gambit\n2\n\n\n4\nAnya Taylor-Joy\nThe Super Mario Bros. Movie\n1\n\n\n...\n...\n...\n...\n\n\n1146\nMia Goth\nNymphomaniac: Vol. II\n1\n\n\n1147\nMia Goth\nMaXXXine\n1\n\n\n1148\nMia Goth\nPearl\n1\n\n\n1149\nMia Goth\nDisappear Into the Blue\n1\n\n\n1150\nMia Goth\nPearl\n1\n\n\n\n\n1151 rows × 3 columns\n\n\n\nNow we can better use our data to make recommendations. After creating our second column, let’s test if everything worked. After doing some exploring the Movie Database, I noticed that three members of the “Emma.” cast also had roles in the hit show “Peaky Blinders”. However, our database has thousands of rows of movies. We will use the following cell of code to ensure that our database was created correctly.\n\n#we will pick a random movie that we know multiple actors have starred in \n#for this demo, we will use \"Peaky Blinders\"\nspecific_movie = \"Peaky Blinders\"\n#we will run the line to determine the number of actors using a specific movie title\nactor_count = sorted_actor_counts.get(specific_movie)\n\n#we will now use loops to determine the number of actors who are in \"Emma.\" and \"Peaky Blinders\"\nif actor_count is not None:\n    print(f\"The number of actors in '{specific_movie}' is {actor_count}.\")\nelse:\n    print(\"Error!\")\n\nThe number of actors in 'Peaky Blinders' is 3.\n\n\nOur test worked! Therefore, we can confirm that we have successfully created a dataframe that contains actor names, movies and shows for each actor, and the number of actors from our list that are in each project.\nWe need to make a few more adjustments to our dataset before we can create a readable graph. The first step is to our sort our dataframe. In order to make my dataset and my graph easier to read, I am going to put the rows in descending order by starting with the movies that share the most actors and ending with the ones that share the least.\n\n#creating our sorted dataframe:\nsorted_df = df.sort_values(by='actor_counts_per_movie', ascending=False)\n\n#as every actor in this film is in \"Emma.\", this movie should be in the first few rows\n#display our updated dataframe:\nsorted_df\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nactor_counts_per_movie\n\n\n\n\n552\nEdward Davis\nEmma.\n50\n\n\n148\nJohnny Flynn\nEmma.\n50\n\n\n316\nAnna Francolini\nEmma.\n50\n\n\n377\nLeigh Daniels\nEmma.\n50\n\n\n58\nCallum Turner\nEmma.\n50\n\n\n...\n...\n...\n...\n\n\n461\nNicholas Burns\nGhost Stories\n1\n\n\n462\nNicholas Burns\nCensor\n1\n\n\n465\nNicholas Burns\nBenidorm\n1\n\n\n467\nNicholas Burns\nThe Lady in the Van\n1\n\n\n1150\nMia Goth\nPearl\n1\n\n\n\n\n1151 rows × 3 columns\n\n\n\nAs we are assuming that the best recommendation has the greatest number of actors from “Emma.”, the last couple rows (the movies with only one actor from our movie) are not great recommendations for our users. Thus, we can make our plot simpler and easier to read by only plotting the the best recommendations. In the next cell, I will create a new dataframe that only includes the top 300 rows of sorted data frame.\n\n#selecting our top choices with the most shared actors\ntop_movies = sorted_df.head(150)\ntop_movies\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nactor_counts_per_movie\n\n\n\n\n552\nEdward Davis\nEmma.\n50\n\n\n148\nJohnny Flynn\nEmma.\n50\n\n\n316\nAnna Francolini\nEmma.\n50\n\n\n377\nLeigh Daniels\nEmma.\n50\n\n\n58\nCallum Turner\nEmma.\n50\n\n\n...\n...\n...\n...\n\n\n884\nBill Nighy\nSalting the Battlefield\n2\n\n\n262\nLucy Briers\nUnnatural Causes\n2\n\n\n258\nLucy Briers\nBeast\n2\n\n\n386\nRose Shalloo\nCall the Midwife\n2\n\n\n259\nLucy Briers\nMidsomer Murders\n2\n\n\n\n\n150 rows × 3 columns\n\n\n\nFinally, we can create our visualization and determine the best recommendations. The best way to model the answer to our question is through creating a bar graph. We will plot the movie names and the number of actors from “Emma.” in each film. The tall the bar, the better fit the film will be for fans of “Emma.”. As every actor discussed has played a role in “Emma.”, we will expect this film to have the tallest bar that accounts for the entire cast of 50 people. Use Plotly in the cell below to create a graph.\n\n#creating our bar graph with the given conditions\nfig = px.bar(top_movies, \n             x='movie_or_TV_name', \n             y='actor_counts_per_movie', \n             title='Movies with the \"Emma\" Actors')\n\n#make customizations to make our plot more readable\nfig.update_xaxes(title='Movie/TV Show')\nfig.update_yaxes(title='Number of Shared Actors')\nfig.update_layout(yaxis=dict(range=[0, 50]))\nfig.update_layout(title = 'Movie Recommendations for \"Emma.\" Fans')\n\n#output our figure\nfig.show()\n\n\n\n\nNow that we have created out graph, we can make educated recommendations. People who enjoy “Emma.” and its cast will most likely enjoy the shows “The Crown” and “Doctor Who”.\nAs we can see, we can use webscraping to answer a variety of questions and study a variety of topics! I hope that this post helps you use webscraping, data manipulation, and plotting in your assignments. Thanks for reading!"
  },
  {
    "objectID": "posts/HW2/index.html#using-scrapy-and-the-film-emma.-2020",
    "href": "posts/HW2/index.html#using-scrapy-and-the-film-emma.-2020",
    "title": "Web Scraping",
    "section": "",
    "text": "Let’s start of today’s blog post by asking the following question: what movie or TV shows share actors with your favorite movie or show?\nHave you ever finished a great movie just to immediately look up every behind the scenes detail on the internet? I know that I definitely have. Often times, I find that if I enjoy one movie I am more likely to enjoy a second movie created by the same director or starring the same actor. Rather than going down a tedious “Google rabbit hole”, we can create a program that takes a movie and shows us which projects we can see the same actors in.\nWe will complete this project using the Python package called scrapy. This package allows us to extract data from a specific website and use it in our own way. We must start our project by importing all the necessary Python packages.\n\nimport scrapy\nimport pandas as pd\nimport numpy as py\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nScrapy allows us to webscrape and gather data from an online database. Pandas and Numpy enables us to create out own data frame with the information we extracted through webscraping and manipulate them. With Plotly, we can create visualizations using the elements of our dataframe. For our demo, we will use the 2020 movie “Emma.” starring Anya Joy Taylor. This personal favorite of mine stars very notable actors and actresses and is widely known as it is based off a book by Jane Austen. Many of these actors have overlapping projects with one another. Moreover, we can assume that someone who enjoys the movie “Emma.” will enjoy movies with a similar cast. This leads us to our question! The best way to answer our question and make some excellent movie recommendations will be to create a data frame and a visualization."
  },
  {
    "objectID": "posts/HW2/index.html#creating-our-scraper",
    "href": "posts/HW2/index.html#creating-our-scraper",
    "title": "Web Scraping",
    "section": "",
    "text": "We created our scraper using two seperate Python files and the Scrapy package. The ‘settings.py’ file was autofilled with all of the necessary lines of code to enable our scraper. In this file, it is very important to change the user agent in order to avoid encountering errors while interacting with different websites.\nIn the ‘tmdb_spiderr.py’ file we start by importing scrapy and defining our class. Within our class, we will have three functions. These functions will all work together to extract specific information from our online database.\nThe first function ‘def init’ is set. The only thing that changes in this function is the exact url for our specific movie.\nOur next function is ‘def parse’. In this function, we attach more elements to our original url so that it now links to the cast page from the movie overview page. We pass the result of this function to the next function so that we may start extracting cast information from our website. Our first function is listed in the following code cell.\n\n#parse function definition \ndef parse(self, response): \n        \"\"\"\n        Function will help us identify the specific urls for the cast and crew members\n        \n        Arguments:\n            self : instance of our class and our function\n            response : our output of our function (will return the cast url)\n\n        Returns: \n            the url of our cast members so that we can see other films that they have worked on \n        \"\"\"\n        \n        #identifies the url for our cast and crew list for the movie we selected\n        cast_url = response.url + '/cast/'  \n        \n        #yields parse_full_credits using scrapy request\n        yield scrapy.Request(url = cast_url, callback = self.parse_full_credits)\n\nNext, we will define our ‘parse_full_credits’ function. We will use the call “response.xpath” along with specific source code from the website itself to identify each member of our movie’s cast. We will then use the url for each actor to pull information from their personal page and determine which movies each actor has worked in. The code for this function is written in the cell below.\n\n#parse_full_credits function definition\ndef parse_full_credits(self, response): \n        \"\"\"\n         Function will help us extract the urls for each actor and their individual page\n        \n        Arguments:\n            self : instance of our class and our function\n            response : our output of our function (will return the cast url)\n\n        Returns: \n            the url of our cast members so that we can see other films that they have worked on \n        \"\"\"\n        \n        #identifies cast\n        table1 = response.xpath('//h3[contains(., \"Cast\")]/following-sibling::ol[1]')\n        \n        #finds each specific actor's page\n        urls_actors = table1.xpath('.//li//div[contains(@class, \"info\")]/descendant-or-self::*/p/a/@href').getall()\n        \n        #yields our list of actor urls\n        for link in urls_actors: \n            yield scrapy.Request(url = response.urljoin(link), callback = self.parse_actor_page) \n\nFinally, we will create one more function called ‘def parse_actor_page’. This function will actually pull the movie and TV show names from the list of credits for each actor. It is important that we select the correct part of the website to use in the ‘response.css’. After implementing the code from the cell below, we will be able to create our CSV file.\n\n#parse_actor_page function defintion \ndef parse_actor_page(self, response): \n        \"\"\"\n        Function will help us organize and display each actor's name and their other movies and TV shows\n        \n        Arguments:\n            self : instance of our class and our function\n            response : our output of our function (will return the cast url)\n\n        Returns: \n            all of the names of specific actors in our movie and the other projects they have worked on  \n        \"\"\"\n        \n        #finds actor names\n        actor_name = response.css('h2.title &gt; a::text').get()\n        \n        #find lists of actor projects \n        table = response.xpath('//h3[text()=\"Acting\"]/following-sibling::table[1]')\n        \n        #find names of actor movies and TV shows within the list of actor projects  \n        movie_names = table.css('td.role.true.account_adult_false.item_adult_false &gt; a.tooltip &gt; bdi::text').getall()\n        \n        #yields our results\n        for movie_or_TV_name in movie_names: \n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name} \n\nNow that we have created all of our functions, we will now be able to create a CSV file that contains both actor names and movie/show names. Run the following line in the terminal: #### scrapy crawl tmdb_spider -o results3.csv -a subdir=556678-emma We will now have a CSV file in our ‘TMDB_scraper’ file. After completing all of the above steps, we can now use our extracted data to create a dataframe and a visualization."
  },
  {
    "objectID": "posts/HW2/index.html#creating-our-database",
    "href": "posts/HW2/index.html#creating-our-database",
    "title": "Web Scraping",
    "section": "",
    "text": "Now that we have gathered our data using our webscraper, we will now organzie it into a database. We will start by simporting our csv file that we filled using our scraper. Please ensure that this CSV file is saved in the same location as your Jupyter notebook.\n\n#creating our initial database using out CSV file\nfilename = \"results.csv\"\ndf = pd.read_csv(filename)\ndf\n\nFileNotFoundError: [Errno 2] No such file or directory: 'results.csv'\n\n\nAfter printing our dataframe, we can see that it looks extremely similar to the table in our csv file. This is great! However, in order to find movies that share the same actors, we must know which of the films in our “movie_or_TV_name” column have multiple actors from “Emma.”. We can do this by creating a sorted list and adding a new column to our dataframe. We can implement this using the code below.\n\n#identifies the movies that have more that stars more than one of our 50 actors in \"Emma.\":\nactor_counts_per_movie = df.groupby('movie_or_TV_name')['actor'].nunique()\nsorted_actor_counts = actor_counts_per_movie.sort_values(ascending=False)\n\n#here we will make our above list a column within our dataframe: \ndf['actor_counts_per_movie'] = df['movie_or_TV_name'].map(actor_counts_per_movie)\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nactor_counts_per_movie\n\n\n\n\n0\nAnya Taylor-Joy\nSplit\n1\n\n\n1\nAnya Taylor-Joy\nThe Witch\n1\n\n\n2\nAnya Taylor-Joy\nThe Menu\n1\n\n\n3\nAnya Taylor-Joy\nThe Queen's Gambit\n2\n\n\n4\nAnya Taylor-Joy\nThe Super Mario Bros. Movie\n1\n\n\n...\n...\n...\n...\n\n\n1146\nMia Goth\nNymphomaniac: Vol. II\n1\n\n\n1147\nMia Goth\nMaXXXine\n1\n\n\n1148\nMia Goth\nPearl\n1\n\n\n1149\nMia Goth\nDisappear Into the Blue\n1\n\n\n1150\nMia Goth\nPearl\n1\n\n\n\n\n1151 rows × 3 columns\n\n\n\nNow we can better use our data to make recommendations. After creating our second column, let’s test if everything worked. After doing some exploring the Movie Database, I noticed that three members of the “Emma.” cast also had roles in the hit show “Peaky Blinders”. However, our database has thousands of rows of movies. We will use the following cell of code to ensure that our database was created correctly.\n\n#we will pick a random movie that we know multiple actors have starred in \n#for this demo, we will use \"Peaky Blinders\"\nspecific_movie = \"Peaky Blinders\"\n#we will run the line to determine the number of actors using a specific movie title\nactor_count = sorted_actor_counts.get(specific_movie)\n\n#we will now use loops to determine the number of actors who are in \"Emma.\" and \"Peaky Blinders\"\nif actor_count is not None:\n    print(f\"The number of actors in '{specific_movie}' is {actor_count}.\")\nelse:\n    print(\"Error!\")\n\nThe number of actors in 'Peaky Blinders' is 3.\n\n\nOur test worked! Therefore, we can confirm that we have successfully created a dataframe that contains actor names, movies and shows for each actor, and the number of actors from our list that are in each project.\nWe need to make a few more adjustments to our dataset before we can create a readable graph. The first step is to our sort our dataframe. In order to make my dataset and my graph easier to read, I am going to put the rows in descending order by starting with the movies that share the most actors and ending with the ones that share the least.\n\n#creating our sorted dataframe:\nsorted_df = df.sort_values(by='actor_counts_per_movie', ascending=False)\n\n#as every actor in this film is in \"Emma.\", this movie should be in the first few rows\n#display our updated dataframe:\nsorted_df\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nactor_counts_per_movie\n\n\n\n\n552\nEdward Davis\nEmma.\n50\n\n\n148\nJohnny Flynn\nEmma.\n50\n\n\n316\nAnna Francolini\nEmma.\n50\n\n\n377\nLeigh Daniels\nEmma.\n50\n\n\n58\nCallum Turner\nEmma.\n50\n\n\n...\n...\n...\n...\n\n\n461\nNicholas Burns\nGhost Stories\n1\n\n\n462\nNicholas Burns\nCensor\n1\n\n\n465\nNicholas Burns\nBenidorm\n1\n\n\n467\nNicholas Burns\nThe Lady in the Van\n1\n\n\n1150\nMia Goth\nPearl\n1\n\n\n\n\n1151 rows × 3 columns\n\n\n\nAs we are assuming that the best recommendation has the greatest number of actors from “Emma.”, the last couple rows (the movies with only one actor from our movie) are not great recommendations for our users. Thus, we can make our plot simpler and easier to read by only plotting the the best recommendations. In the next cell, I will create a new dataframe that only includes the top 300 rows of sorted data frame.\n\n#selecting our top choices with the most shared actors\ntop_movies = sorted_df.head(150)\ntop_movies\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\nactor_counts_per_movie\n\n\n\n\n552\nEdward Davis\nEmma.\n50\n\n\n148\nJohnny Flynn\nEmma.\n50\n\n\n316\nAnna Francolini\nEmma.\n50\n\n\n377\nLeigh Daniels\nEmma.\n50\n\n\n58\nCallum Turner\nEmma.\n50\n\n\n...\n...\n...\n...\n\n\n884\nBill Nighy\nSalting the Battlefield\n2\n\n\n262\nLucy Briers\nUnnatural Causes\n2\n\n\n258\nLucy Briers\nBeast\n2\n\n\n386\nRose Shalloo\nCall the Midwife\n2\n\n\n259\nLucy Briers\nMidsomer Murders\n2\n\n\n\n\n150 rows × 3 columns\n\n\n\nFinally, we can create our visualization and determine the best recommendations. The best way to model the answer to our question is through creating a bar graph. We will plot the movie names and the number of actors from “Emma.” in each film. The tall the bar, the better fit the film will be for fans of “Emma.”. As every actor discussed has played a role in “Emma.”, we will expect this film to have the tallest bar that accounts for the entire cast of 50 people. Use Plotly in the cell below to create a graph.\n\n#creating our bar graph with the given conditions\nfig = px.bar(top_movies, \n             x='movie_or_TV_name', \n             y='actor_counts_per_movie', \n             title='Movies with the \"Emma\" Actors')\n\n#make customizations to make our plot more readable\nfig.update_xaxes(title='Movie/TV Show')\nfig.update_yaxes(title='Number of Shared Actors')\nfig.update_layout(yaxis=dict(range=[0, 50]))\nfig.update_layout(title = 'Movie Recommendations for \"Emma.\" Fans')\n\n#output our figure\nfig.show()\n\n\n\n\nNow that we have created out graph, we can make educated recommendations. People who enjoy “Emma.” and its cast will most likely enjoy the shows “The Crown” and “Doctor Who”.\nAs we can see, we can use webscraping to answer a variety of questions and study a variety of topics! I hope that this post helps you use webscraping, data manipulation, and plotting in your assignments. Thanks for reading!"
  },
  {
    "objectID": "posts/HW5/index.html",
    "href": "posts/HW5/index.html",
    "title": "Cats vs. Dogs: Image Classification",
    "section": "",
    "text": "Today, we will use Keras and Tensorflow to create a machine learning algorithm that can distinguish images of dog from images of cats through Image Classification. Image Classification is a very useful skill that can be used in real world projects. Additionally, it is a great exercise to help hone your understanding of machine learning. We will implement 4 different models and compare the accuracy of each. By doing so, we will learn about multiple methods but also see which methods are the most successful. Please follow along with the code in this blog post to create your own algorithms!\nAs always, we will start out by importing the following packages so that we can successfully implement these models and make plots that allow us to compare results. Please run the code cell below to import each of the packages.\n\n#packages given in blog post\nimport os\nfrom keras import utils\nimport tensorflow_datasets as tfds\n\n#more required packages\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import datasets, layers, models\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\n\nNext, we must upload our data set. Our data set will be comprised of different pictures of both cats and dogs from Kaggle. Please run the following code cell to import our data.\n\n#uploads images (code given in blog post)\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nWe have successfully accessed thousands photos that we can use in our various models for training, validation, and testing! Now, we must ensure that all of ours images are the same size. Please run the code cell below and resize teh images to a set size of 150 by 150.\n\n#resize code from blog post\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nThe next code cell will allow us to rapidly read data and determine how many data points are gathered from the directory at once. Please run the following code cell before proceeding.\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\nIn the next few steps, we will be better organizing and understanding our dataset.\nWe will now use the take method to retrieve an image from a batch of the training data. I selected one image of a dog and another of a cat by implementing the following two cells of code. Please run the cells below.\n\n#selecting dog image from training batch\nfor batch in train_ds.take(1):\n    single_image = batch[0].numpy().astype(\"uint8\")\n\nsingle_image = single_image[0]\n\n#display image on our marked grid\nplt.imshow(single_image)\nplt.show()\n\n\n\n\n\n\n\n\n\n#selecting cat image from training\nfor batch in train_ds.take(81):\n    single_image = batch[0].numpy().astype(\"uint8\")\n\nsingle_image = single_image[0]\n\n#display image on our marked grid\nplt.imshow(single_image)\nplt.show()\n\n\n\n\n\n\n\n\nWe will now write a function that organizes our photos into two rows. This will simply make it easier to visualize and work with our data. The first row will display three random photos of cats and the second row will show three random photos of dogs. Please run the cell below to implement our function.\n\n#allows us to organize our images by the name of the animal\nclass_names = { 0: \"Cat\", 1: \"Dog\"}\n\n#function that organizes the dog and cat photos into two given rows with appropriate labels\ndef visualize_dataset(dataset, class_names):\n  plt.figure(figsize=(15, 8))\n  for subset in dataset.take(1):\n    images = batch[0].numpy().astype(\"uint8\")\n    labels = batch[1].numpy()\n\n    i = 0\n    d_count = 0\n    c_count = 0\n    cats = []\n    dogs = []\n    while c_count &lt; 3:\n      if labels [i] == 0:\n        cats.append(images[i])\n        c_count += 1\n      i += 1\n\n    i = 0\n    while d_count &lt; 3:\n      if labels [i] == 1:\n        dogs.append(images[i])\n        d_count += 1\n      i += 1\n\n    for i in range(3):\n      ax = plt.subplot(2, 3, i + 1)\n      plt.imshow(cats[i])\n      plt.title(class_names[0])\n      plt.axis(\"off\")\n\n    for i in range(3):\n      ax = plt.subplot(2, 3, 3 + i + 1)\n      plt.imshow(dogs[i])\n      plt.title(class_names[1])\n      plt.axis(\"off\")\n\n  plt.show()\n\n#display our two rows\nvisualize_dataset(train_ds, class_names)\n\n\n\n\n\n\n\n\nAfter successfully organizing our photos into rows, we will now run the following cell to create an iterator. Please run the code cell below to implement this step.\n\n#creates an iterator (given in blog post)\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\nWe will create the following dictionary so that we may differentiate between the strings “Cat” and “Dog”.\n\nclass_names = { 0: \"Cat\", 1: \"Dog\"}\n\nIn order to determine the baseline for our machine learning model, we must know the number of images displaying cats and the number of images displaying dogs. In the following cell, we will use our iterator to compute these numbers. Please run the following code cell.\n\n#compute the number of images in training data set\n\n#sets our image count to zero to start\ncat_count = 0\ndog_count = 0\n\n#loop will count images containing dogs and images containing cats\nfor label_value in labels_iterator:\n    if label_value == 0:\n        cat_count += 1\n    elif label_value == 1:\n        dog_count += 1\n\n#displays our results\nprint(\"Number of images displaying cats:\", cat_count)\nprint(\"Number of images displaying dogs:\", dog_count)\n\nNumber of images displaying cats: 4637\nNumber of images displaying dogs: 4668\n\n\nAs we can see above, the number of images displaying dogs and the number of images displaying cats in our dataset are both very similar. Each range between 4600 and 4700 total images. This means that we are working with a training dataset of 9305 photos. Now we can calculate our baseline accuracy. Our baseline machine learning model will guess the most frequent label. There are more dog images than cat images so we can infer that our baseline model would sort the images as “dog”. We can determine our baseline accuracy by dividing the total number of dog images by the total number of images. This is equivalent to 4668 divided by 9305. Thus, our baseline accuracy is about 0.5017. Logically, this makes sense as almost half of the total photos are photos of dogs. In order for our models to be considered accurate and successful, we should see accuracy results that are greater than about 0.50. Now that we have organized our dataset and determined our baseline accuracy, we are ready to start implemementing our four different models.\n\n\n\nThis first model is anticipated to be the least accurate. In this method, we will use layers that we defined in class to train our machine to sort our different images. We will include Conv2D layers, MaxPooling2D layers, Flatten layers, Dense layers, and Dropout layers. Please run the following code cell to define this model.\n\n#model1 definition\nmodel1 = keras.Sequential([\n    #our required layers:\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(10, activation='softmax')\n])\n\nNow that we have used the keras.Sequential to create our model and define all of the necessary layers, we can train our model using the code in the following cell. Please note that this may take a few minutes to completely finish running. Please run the code cell and train our model.\n\n#allows model1 to compile\nmodel1.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n#given in blog post instructions.\nhistory_model_1 = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 14s 55ms/step - loss: 16.6655 - accuracy: 0.5071 - val_loss: 0.7008 - val_accuracy: 0.5636\nEpoch 2/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.8050 - accuracy: 0.5531 - val_loss: 0.7235 - val_accuracy: 0.6036\nEpoch 3/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.7339 - accuracy: 0.5963 - val_loss: 0.6713 - val_accuracy: 0.5890\nEpoch 4/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.6774 - accuracy: 0.6547 - val_loss: 0.6842 - val_accuracy: 0.6264\nEpoch 5/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.6099 - accuracy: 0.7025 - val_loss: 0.7092 - val_accuracy: 0.6225\nEpoch 6/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.5599 - accuracy: 0.7280 - val_loss: 0.7178 - val_accuracy: 0.6328\nEpoch 7/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.4879 - accuracy: 0.7755 - val_loss: 0.9081 - val_accuracy: 0.6221\nEpoch 8/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.4251 - accuracy: 0.8101 - val_loss: 0.9148 - val_accuracy: 0.6144\nEpoch 9/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.4117 - accuracy: 0.8192 - val_loss: 0.9019 - val_accuracy: 0.6135\nEpoch 10/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.3669 - accuracy: 0.8572 - val_loss: 0.9012 - val_accuracy: 0.6217\nEpoch 11/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.3177 - accuracy: 0.8692 - val_loss: 1.0913 - val_accuracy: 0.6156\nEpoch 12/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.2667 - accuracy: 0.8955 - val_loss: 1.3066 - val_accuracy: 0.6255\nEpoch 13/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.2635 - accuracy: 0.9015 - val_loss: 1.1932 - val_accuracy: 0.6333\nEpoch 14/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.2101 - accuracy: 0.9243 - val_loss: 1.1485 - val_accuracy: 0.6384\nEpoch 15/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.1909 - accuracy: 0.9348 - val_loss: 1.3259 - val_accuracy: 0.6285\nEpoch 16/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.1743 - accuracy: 0.9390 - val_loss: 1.3392 - val_accuracy: 0.6328\nEpoch 17/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.1924 - accuracy: 0.9340 - val_loss: 1.4034 - val_accuracy: 0.6260\nEpoch 18/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.1727 - accuracy: 0.9421 - val_loss: 1.2522 - val_accuracy: 0.6187\nEpoch 19/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.1519 - accuracy: 0.9458 - val_loss: 1.3135 - val_accuracy: 0.6277\nEpoch 20/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.1589 - accuracy: 0.9456 - val_loss: 1.2940 - val_accuracy: 0.6225\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n\n\nNow that we have trained our model, we will plot the history of our model’s accuracy using both the training and validation sets. We will create a line graph using the package “Matplotlib” (imported in the beginning of this post). We will use this plot to further understand how the accuracy of our first model. Please run the code below.\n\nplt.plot(history_model_1.history['accuracy'], label='TRAINING ACCURACY')\nplt.plot(history_model_1.history['val_accuracy'], label='VALIDATION ACCURACY')\nplt.xlabel('EPOCHS')\nplt.ylabel('ACCURACY')\nplt.title('MODEL 1: ACCURACY COMPARSION')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe validation accuracy of model 1 is about 64% (in between 60% and 65%).\nI found the measurement bylooking at my graph and estimating the highest accuracy value (y-axis) for the Validation accuracy line (orange plot).\nIn comparison to the baseline accuracy, we can conclude that my model 1 is more accurate (about 10% more successful). This makes sense but leads me to believe that this still may not be the best model overall.\nOverfitting can occur at points where the training accuracy is much higher than the validation accuracy. There are signs of overfitting as the training model produces an accuracy over 90% while validation accuracy for model 1 never experiences an accuracy that exceeds 65%. By adjusting our model, we may be able to avoid overfitting in the future.\n\n\n\nIn this next model, we are going to use data augmentation layers. This will involve us including layers that include modified copies of the same image in the training set. For our purposes, we will include data augmentation layers that rotate the images in hopes of seeing a higher accuracy. These slight changes in the versions of our images are called invariant features. They will help our model further understand the differences between the cat and dog images.\nWe will start by defining a function that allows us to complete data augmentation. This function will allow us to output a 2 by 2 grid of the same photo. Each photo should have a slightly different rotation. Please run the code cell below.\n\n#function allows us to implement data augmentation and outputs images with the appropriate labels\ndef augmented(train_ds, data_augmentation):\n  plt.figure(figsize=(10, 10))\n  for image, _ in train_ds.take(1):\n      original_image = image[0]\n      ax = plt.subplot(2, 2, 1)\n      plt.imshow(original_image / 255)\n      plt.axis('off')\n      ax.set_title('Original Image')\n\n  #display original and augemented images in a 2 by 2 grid\n  for i in range(2, 5):\n      ax = plt.subplot(2, 2, i)\n      augmented_image = data_augmentation(tf.expand_dims(original_image, 0))\n      plt.imshow(augmented_image[0] / 255)\n      plt.axis('off')\n      ax.set_title(f'Augmented Image {i - 1}')\n\n  #displays results\n  plt.show()\n\nNow that we have defined our function, we can implement different forms of data augmentation onto our images. First, we will create a keras.layer.RandomFlip() that takes the original image and then flips it for a few instances in our grid. The image will appear as a mirrored image of our original photo. Please run the code cell below to create this first data augmentation layer.\n\n#first data augmentation layer\ndata_augmentation_1 = tf.keras.Sequential([\n  tf.keras.layers.RandomFlip('horizontal'),\n])\naugmented(train_ds, data_augmentation_1)\n\n\n\n\n\n\n\n\nAs we can see in our grid of images above, the original image and second image show the original photo. However, the bottom first and third images appear as a mirrored version of the original.\nFor our second data augmentation layer, we will create a keras.layers.RandomRotation() layer that rotates the entire image in either the clockwise or counterclockwise direction. Please run the code cell below to create this second layer.\n\n#second data augmentation layer\ndata_augmentation_2 = tf.keras.Sequential([\n  tf.keras.layers.RandomRotation(0.2),\n])\naugmented(train_ds, data_augmentation_2)\n\n\n\n\n\n\n\n\nNow that we have sucessfully created two different data augmentation layers, we can train our model and create our visualization. We will define model 2 using keras.Sequential just as we did for model 1. The main difference here will be the two new layers that we will incorporate into our model. Please run the code cell below to define our secind model.\n\nmodel2 = keras.models.Sequential([\n    #new layers (defined above)\n    layers.RandomFlip(\"horizontal\", input_shape=(150, 150, 3)),\n    layers.RandomRotation(0.2),\n\n    #extra layers (model1)\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(10, activation='softmax')\n])\n\nNow that we have included our new layers in model 2, we can run the code cell below to compile and train our model while also extracting the information we need to determine the accuracy. Please run the code below.\n\n#allows model2 to compile\nmodel2.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n\n#given in blog post instructions\nhistory_model2 = model2.fit(train_ds,\n                            epochs=20,\n                            validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 7s 35ms/step - loss: 11.5206 - accuracy: 0.5083 - val_loss: 0.7474 - val_accuracy: 0.5486\nEpoch 2/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.8137 - accuracy: 0.5362 - val_loss: 0.7817 - val_accuracy: 0.5907\nEpoch 3/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.7655 - accuracy: 0.5370 - val_loss: 0.6930 - val_accuracy: 0.6105\nEpoch 4/20\n146/146 [==============================] - 6s 38ms/step - loss: 0.7324 - accuracy: 0.5686 - val_loss: 0.6570 - val_accuracy: 0.6195\nEpoch 5/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.7105 - accuracy: 0.5851 - val_loss: 0.6499 - val_accuracy: 0.6509\nEpoch 6/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.7004 - accuracy: 0.5922 - val_loss: 0.6346 - val_accuracy: 0.6509\nEpoch 7/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.6769 - accuracy: 0.6169 - val_loss: 0.6230 - val_accuracy: 0.6870\nEpoch 8/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.6719 - accuracy: 0.6159 - val_loss: 0.6213 - val_accuracy: 0.6707\nEpoch 9/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6547 - accuracy: 0.6376 - val_loss: 0.6542 - val_accuracy: 0.6066\nEpoch 10/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.6598 - accuracy: 0.6243 - val_loss: 0.6001 - val_accuracy: 0.6991\nEpoch 11/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6482 - accuracy: 0.6428 - val_loss: 0.6273 - val_accuracy: 0.6522\nEpoch 12/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6463 - accuracy: 0.6443 - val_loss: 0.6163 - val_accuracy: 0.6698\nEpoch 13/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6460 - accuracy: 0.6402 - val_loss: 0.6158 - val_accuracy: 0.6917\nEpoch 14/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6377 - accuracy: 0.6509 - val_loss: 0.6059 - val_accuracy: 0.6943\nEpoch 15/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.6275 - accuracy: 0.6644 - val_loss: 0.6010 - val_accuracy: 0.6917\nEpoch 16/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6280 - accuracy: 0.6629 - val_loss: 0.5989 - val_accuracy: 0.6917\nEpoch 17/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6211 - accuracy: 0.6689 - val_loss: 0.5989 - val_accuracy: 0.6905\nEpoch 18/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6227 - accuracy: 0.6672 - val_loss: 0.6063 - val_accuracy: 0.6823\nEpoch 19/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6188 - accuracy: 0.6693 - val_loss: 0.5964 - val_accuracy: 0.6991\nEpoch 20/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6134 - accuracy: 0.6740 - val_loss: 0.5959 - val_accuracy: 0.7038\n\n\nWe will create another line graph using Matplotlib and the information for model 2. Please run the code cell below to create our visualization.\n\nplt.plot(history_model2.history['accuracy'], label='TRAINING ACCURACY')\nplt.plot(history_model2.history['val_accuracy'], label='VALIDATION ACCURACY')\nplt.xlabel('EPOCHS')\nplt.ylabel('ACCURACY')\nplt.title('MODEL 2_ ACCURACY COMPARISON')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe validation accuracy of model 2 is about 68% (in between 66 and 70%).\nI found the measurement by looking at my graph and estimating the highest accuracy value (y-axis) for the Validation accuracy line (orange plot).\nIn comparison to the model 1 validation accuracy, we can conclude that my model 2 is more accurate (about 4% more successful). This makes sense as my second model was more detailed but still very similar to model 1.\nOverfitting does not seem to occur with this model. The validation accuracy seems to always exceed the training accuracy. Even though this conclusion surprised me, it is a good sign that my model is improving and becoming more accurate.\n\n\n\nFor our third model, we will include data preprocessing layers. The data is containeded in scale described by pixels with RGB values. As of right now, our RGB values are in between 0 and 255 which causes the model to take longer to train. These layers will allows us to focus more of our algorithm’s training energy on the data and less on the scale.\nWe will include the following code from the blog post instructions to create a preprocessing layer called preprocessor. Please run the following code cell.\n\n#given in instructions\n\ni = keras.Input(shape=(150, 150, 3))\n#the pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n#outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\nWe can now define our model3 using our new preprocessing layer as well as the layers we defined for model2. Please run the code cell below.\n\nmodel3 = keras.Sequential([\n    #new layer (defined above)\n    preprocessor,\n\n    #data augmentation layer (model2)\n    layers.RandomFlip(\"horizontal\", input_shape=(150, 150, 3)),\n    layers.RandomRotation(0.2),\n\n    #updated original layers (model1)\n    layers.Conv2D(64, (3, 3), activation='LeakyReLU'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(256, (3, 3), activation='LeakyReLU'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(512, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(1024, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(512, activation='LeakyReLU'),\n    layers.Dropout(0.3),\n    layers.Dense(10, activation='softmax')\n])\n\nNow that we have included our new layers in model 3, we can run the code cell below to compile and train our model while also extracting the information we need to determine the accuracy. Please run the code below.\n\n#allows model3 to compile\nmodel3.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n\n#given in blog post instructions\nhistory_model_3 = model3.fit(train_ds,\n                            epochs=20,\n                            validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 28s 142ms/step - loss: 0.7791 - accuracy: 0.5318 - val_loss: 0.6546 - val_accuracy: 0.6543\nEpoch 2/20\n146/146 [==============================] - 18s 121ms/step - loss: 0.6626 - accuracy: 0.5999 - val_loss: 0.5969 - val_accuracy: 0.6887\nEpoch 3/20\n146/146 [==============================] - 18s 121ms/step - loss: 0.6060 - accuracy: 0.6676 - val_loss: 0.5720 - val_accuracy: 0.7072\nEpoch 4/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.5772 - accuracy: 0.7030 - val_loss: 0.5285 - val_accuracy: 0.7485\nEpoch 5/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.5381 - accuracy: 0.7290 - val_loss: 0.4879 - val_accuracy: 0.7730\nEpoch 6/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.5149 - accuracy: 0.7487 - val_loss: 0.4506 - val_accuracy: 0.7889\nEpoch 7/20\n146/146 [==============================] - 18s 121ms/step - loss: 0.4734 - accuracy: 0.7742 - val_loss: 0.4162 - val_accuracy: 0.8151\nEpoch 8/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.4544 - accuracy: 0.7857 - val_loss: 0.3987 - val_accuracy: 0.8117\nEpoch 9/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.4330 - accuracy: 0.8042 - val_loss: 0.4069 - val_accuracy: 0.8143\nEpoch 10/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.4013 - accuracy: 0.8146 - val_loss: 0.3874 - val_accuracy: 0.8151\nEpoch 11/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3773 - accuracy: 0.8382 - val_loss: 0.3650 - val_accuracy: 0.8392\nEpoch 12/20\n146/146 [==============================] - 19s 129ms/step - loss: 0.3699 - accuracy: 0.8389 - val_loss: 0.3363 - val_accuracy: 0.8521\nEpoch 13/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3569 - accuracy: 0.8440 - val_loss: 0.3306 - val_accuracy: 0.8504\nEpoch 14/20\n146/146 [==============================] - 18s 121ms/step - loss: 0.3483 - accuracy: 0.8457 - val_loss: 0.3202 - val_accuracy: 0.8577\nEpoch 15/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3296 - accuracy: 0.8556 - val_loss: 0.3029 - val_accuracy: 0.8706\nEpoch 16/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3111 - accuracy: 0.8676 - val_loss: 0.3026 - val_accuracy: 0.8693\nEpoch 17/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3179 - accuracy: 0.8621 - val_loss: 0.2885 - val_accuracy: 0.8805\nEpoch 18/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3043 - accuracy: 0.8732 - val_loss: 0.2718 - val_accuracy: 0.8852\nEpoch 19/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.2844 - accuracy: 0.8769 - val_loss: 0.2690 - val_accuracy: 0.8792\nEpoch 20/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.2784 - accuracy: 0.8821 - val_loss: 0.2775 - val_accuracy: 0.8813\n\n\nWe will now create our visualization using the code below.\n\nplt.plot(history_model_3.history['accuracy'], label='TRAINING ACCURACY')\nplt.plot(history_model_3.history['val_accuracy'], label='VALIDATION ACCURACY')\nplt.xlabel('EPOCHS')\nplt.ylabel('ACCURACY')\nplt.title('MODEL 3: ACCURACY COMPARISON')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe validation accuracy of model 3 is about 87% (in between 87% and 90%).\nI found the measurement by looking at my graph and estimating the highest accuracy value (y-axis) for the Validation accuracy line (orange plot).\nIn comparison to the model 1 validation accuracy, we can conclude that my model 3 is signficantly more accurate (about 20%-25% more successful). This makes sense as my third model is the most detailed one so far out of the three.\nOverfitting does not really occur in the plot of this model’s accuracy as the validation accuracy is constantly greater than the training accuracy.\n\n\n\nThere are many models rgar have already been trained to do related tasks. Even if the intended task is not exactly the same, there is a high probability that they have been trained to notice similar patterns and output similar results. For our fourth model, we will access a pre-existing “base model” to complete our current task of sorting images of dogs and cats. Please run the following code that was given in the blog post instructions.\n\n#from blog post instructions\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n12683000/12683000 [==============================] - 2s 0us/step\n\n\nNow that we have a defined “base model”, we can create a new model using some of layers from our existing models. For our fourth and final model, we will include the layers defined above as well as the data augmentation layers that were included in both model 2 and model 3. We will also need to include some of the starter layers that we first introduced in model 1.\n\nmodel4 = models.Sequential([\n    # augmentation layers\n    layers.RandomFlip(\"horizontal\", input_shape=(150, 150, 3)),\n    layers.RandomRotation(0.2),\n\n    base_model_layer,\n    layers.GlobalMaxPooling2D(),\n    layers.Dropout(0.2),\n    layers.Dense(2, activation='softmax'),  # outputs the final classification\n])\n\nNow that we have our model, use the code cell below to determine the summary of the model’s accuracy.\n\n# Compile the model\nmodel4.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\nmodel4.summary()\n\nModel: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_flip_3 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_3 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n model_1 (Functional)        (None, 5, 5, 960)         2996352   \n                                                                 \n global_max_pooling2d (Glob  (None, 960)               0         \n alMaxPooling2D)                                                 \n                                                                 \n dropout_6 (Dropout)         (None, 960)               0         \n                                                                 \n dense_9 (Dense)             (None, 2)                 1922      \n                                                                 \n=================================================================\nTotal params: 2998274 (11.44 MB)\nTrainable params: 1922 (7.51 KB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\n\n\nUse the following code cell to determine history just as we have done for the previous three models.\n\nhistory_model_4 = model4.fit(train_ds,\n                            epochs=20,\n                            validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 15s 64ms/step - loss: 1.4940 - accuracy: 0.8417 - val_loss: 0.2753 - val_accuracy: 0.9669\nEpoch 2/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.7269 - accuracy: 0.9140 - val_loss: 0.3333 - val_accuracy: 0.9592\nEpoch 3/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.5694 - accuracy: 0.9256 - val_loss: 0.2105 - val_accuracy: 0.9712\nEpoch 4/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.5304 - accuracy: 0.9283 - val_loss: 0.1809 - val_accuracy: 0.9703\nEpoch 5/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4488 - accuracy: 0.9341 - val_loss: 0.2300 - val_accuracy: 0.9600\nEpoch 6/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4191 - accuracy: 0.9318 - val_loss: 0.1424 - val_accuracy: 0.9733\nEpoch 7/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3847 - accuracy: 0.9334 - val_loss: 0.2245 - val_accuracy: 0.9617\nEpoch 8/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3594 - accuracy: 0.9382 - val_loss: 0.1431 - val_accuracy: 0.9695\nEpoch 9/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3263 - accuracy: 0.9362 - val_loss: 0.1258 - val_accuracy: 0.9729\nEpoch 10/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3442 - accuracy: 0.9337 - val_loss: 0.1430 - val_accuracy: 0.9725\nEpoch 11/20\n146/146 [==============================] - 6s 42ms/step - loss: 0.3117 - accuracy: 0.9341 - val_loss: 0.1328 - val_accuracy: 0.9746\nEpoch 12/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.2531 - accuracy: 0.9422 - val_loss: 0.1193 - val_accuracy: 0.9721\nEpoch 13/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.2852 - accuracy: 0.9332 - val_loss: 0.1046 - val_accuracy: 0.9742\nEpoch 14/20\n146/146 [==============================] - 6s 42ms/step - loss: 0.2595 - accuracy: 0.9390 - val_loss: 0.1241 - val_accuracy: 0.9665\nEpoch 15/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.2654 - accuracy: 0.9370 - val_loss: 0.1161 - val_accuracy: 0.9695\nEpoch 16/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.2700 - accuracy: 0.9353 - val_loss: 0.1018 - val_accuracy: 0.9742\nEpoch 17/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.2752 - accuracy: 0.9341 - val_loss: 0.1429 - val_accuracy: 0.9643\nEpoch 18/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.2929 - accuracy: 0.9352 - val_loss: 0.1529 - val_accuracy: 0.9652\nEpoch 19/20\n146/146 [==============================] - 6s 42ms/step - loss: 0.2891 - accuracy: 0.9311 - val_loss: 0.1177 - val_accuracy: 0.9725\nEpoch 20/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3265 - accuracy: 0.9304 - val_loss: 0.1335 - val_accuracy: 0.9729\n\n\nFinally, we can plot the validation accuracy and the training accuracy.\n\nplt.plot(history_model_4.history['accuracy'], label='TRAINING ACCURACY')\nplt.plot(history_model_4.history['val_accuracy'], label='VALIDATION ACCURACY')\nplt.xlabel('EPOCHS')\nplt.ylabel('ACCURACY')\nplt.title('MODEL 4: ACCURACY COMPARISON')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe validation accuracy of model 3 is about 97% (in between 96% and 98%).\nI found the measurement by looking at my graph and estimating the highest accuracy value (y-axis) for the Validation accuracy line (orange plot).\nIn comparison to all of our models, this is the model with the best accuracy as the validation accuracy is constantly above 96%.\nOverfitting does not really occur in the plot of this model’s accuracy as the validation accuracy is constantly greater than the training accuracy.\nWe have successfully implemented our fourth and final model and gathered the information regarding model accuracy.\n\n\n\nAs model 4 is clearly the most accurate out of all four models, with a 97% validation accuracy rate, we will use test_ds to evaluate the accuracy. Please complete this step by running the code cell below.\n\ntest_loss, test_accuracy = model4.evaluate(test_ds)\nprint(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n\n37/37 [==============================] - 3s 68ms/step - loss: 0.1688 - accuracy: 0.9695\nTest Accuracy: 96.95%\n\n\nThe test we ran above is almost exactly the same as the validation accuracy we see in our plot for model 4. This is great indication that our model was both successful and accurate in classifying our images in our dataset.\nThank you for reading this post about image classification!"
  },
  {
    "objectID": "posts/HW5/index.html#introduction-and-dataset-set-up",
    "href": "posts/HW5/index.html#introduction-and-dataset-set-up",
    "title": "Cats vs. Dogs: Image Classification",
    "section": "",
    "text": "Today, we will use Keras and Tensorflow to create a machine learning algorithm that can distinguish images of dog from images of cats through Image Classification. Image Classification is a very useful skill that can be used in real world projects. Additionally, it is a great exercise to help hone your understanding of machine learning. We will implement 4 different models and compare the accuracy of each. By doing so, we will learn about multiple methods but also see which methods are the most successful. Please follow along with the code in this blog post to create your own algorithms!\nAs always, we will start out by importing the following packages so that we can successfully implement these models and make plots that allow us to compare results. Please run the code cell below to import each of the packages.\n\n#packages given in blog post\nimport os\nfrom keras import utils\nimport tensorflow_datasets as tfds\n\n#more required packages\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import datasets, layers, models\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\n\nNext, we must upload our data set. Our data set will be comprised of different pictures of both cats and dogs from Kaggle. Please run the following code cell to import our data.\n\n#uploads images (code given in blog post)\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nWe have successfully accessed thousands photos that we can use in our various models for training, validation, and testing! Now, we must ensure that all of ours images are the same size. Please run the code cell below and resize teh images to a set size of 150 by 150.\n\n#resize code from blog post\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nThe next code cell will allow us to rapidly read data and determine how many data points are gathered from the directory at once. Please run the following code cell before proceeding.\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\nIn the next few steps, we will be better organizing and understanding our dataset.\nWe will now use the take method to retrieve an image from a batch of the training data. I selected one image of a dog and another of a cat by implementing the following two cells of code. Please run the cells below.\n\n#selecting dog image from training batch\nfor batch in train_ds.take(1):\n    single_image = batch[0].numpy().astype(\"uint8\")\n\nsingle_image = single_image[0]\n\n#display image on our marked grid\nplt.imshow(single_image)\nplt.show()\n\n\n\n\n\n\n\n\n\n#selecting cat image from training\nfor batch in train_ds.take(81):\n    single_image = batch[0].numpy().astype(\"uint8\")\n\nsingle_image = single_image[0]\n\n#display image on our marked grid\nplt.imshow(single_image)\nplt.show()\n\n\n\n\n\n\n\n\nWe will now write a function that organizes our photos into two rows. This will simply make it easier to visualize and work with our data. The first row will display three random photos of cats and the second row will show three random photos of dogs. Please run the cell below to implement our function.\n\n#allows us to organize our images by the name of the animal\nclass_names = { 0: \"Cat\", 1: \"Dog\"}\n\n#function that organizes the dog and cat photos into two given rows with appropriate labels\ndef visualize_dataset(dataset, class_names):\n  plt.figure(figsize=(15, 8))\n  for subset in dataset.take(1):\n    images = batch[0].numpy().astype(\"uint8\")\n    labels = batch[1].numpy()\n\n    i = 0\n    d_count = 0\n    c_count = 0\n    cats = []\n    dogs = []\n    while c_count &lt; 3:\n      if labels [i] == 0:\n        cats.append(images[i])\n        c_count += 1\n      i += 1\n\n    i = 0\n    while d_count &lt; 3:\n      if labels [i] == 1:\n        dogs.append(images[i])\n        d_count += 1\n      i += 1\n\n    for i in range(3):\n      ax = plt.subplot(2, 3, i + 1)\n      plt.imshow(cats[i])\n      plt.title(class_names[0])\n      plt.axis(\"off\")\n\n    for i in range(3):\n      ax = plt.subplot(2, 3, 3 + i + 1)\n      plt.imshow(dogs[i])\n      plt.title(class_names[1])\n      plt.axis(\"off\")\n\n  plt.show()\n\n#display our two rows\nvisualize_dataset(train_ds, class_names)\n\n\n\n\n\n\n\n\nAfter successfully organizing our photos into rows, we will now run the following cell to create an iterator. Please run the code cell below to implement this step.\n\n#creates an iterator (given in blog post)\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\nWe will create the following dictionary so that we may differentiate between the strings “Cat” and “Dog”.\n\nclass_names = { 0: \"Cat\", 1: \"Dog\"}\n\nIn order to determine the baseline for our machine learning model, we must know the number of images displaying cats and the number of images displaying dogs. In the following cell, we will use our iterator to compute these numbers. Please run the following code cell.\n\n#compute the number of images in training data set\n\n#sets our image count to zero to start\ncat_count = 0\ndog_count = 0\n\n#loop will count images containing dogs and images containing cats\nfor label_value in labels_iterator:\n    if label_value == 0:\n        cat_count += 1\n    elif label_value == 1:\n        dog_count += 1\n\n#displays our results\nprint(\"Number of images displaying cats:\", cat_count)\nprint(\"Number of images displaying dogs:\", dog_count)\n\nNumber of images displaying cats: 4637\nNumber of images displaying dogs: 4668\n\n\nAs we can see above, the number of images displaying dogs and the number of images displaying cats in our dataset are both very similar. Each range between 4600 and 4700 total images. This means that we are working with a training dataset of 9305 photos. Now we can calculate our baseline accuracy. Our baseline machine learning model will guess the most frequent label. There are more dog images than cat images so we can infer that our baseline model would sort the images as “dog”. We can determine our baseline accuracy by dividing the total number of dog images by the total number of images. This is equivalent to 4668 divided by 9305. Thus, our baseline accuracy is about 0.5017. Logically, this makes sense as almost half of the total photos are photos of dogs. In order for our models to be considered accurate and successful, we should see accuracy results that are greater than about 0.50. Now that we have organized our dataset and determined our baseline accuracy, we are ready to start implemementing our four different models."
  },
  {
    "objectID": "posts/HW5/index.html#first-model",
    "href": "posts/HW5/index.html#first-model",
    "title": "Cats vs. Dogs: Image Classification",
    "section": "",
    "text": "This first model is anticipated to be the least accurate. In this method, we will use layers that we defined in class to train our machine to sort our different images. We will include Conv2D layers, MaxPooling2D layers, Flatten layers, Dense layers, and Dropout layers. Please run the following code cell to define this model.\n\n#model1 definition\nmodel1 = keras.Sequential([\n    #our required layers:\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(10, activation='softmax')\n])\n\nNow that we have used the keras.Sequential to create our model and define all of the necessary layers, we can train our model using the code in the following cell. Please note that this may take a few minutes to completely finish running. Please run the code cell and train our model.\n\n#allows model1 to compile\nmodel1.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n#given in blog post instructions.\nhistory_model_1 = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 14s 55ms/step - loss: 16.6655 - accuracy: 0.5071 - val_loss: 0.7008 - val_accuracy: 0.5636\nEpoch 2/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.8050 - accuracy: 0.5531 - val_loss: 0.7235 - val_accuracy: 0.6036\nEpoch 3/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.7339 - accuracy: 0.5963 - val_loss: 0.6713 - val_accuracy: 0.5890\nEpoch 4/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.6774 - accuracy: 0.6547 - val_loss: 0.6842 - val_accuracy: 0.6264\nEpoch 5/20\n146/146 [==============================] - 5s 32ms/step - loss: 0.6099 - accuracy: 0.7025 - val_loss: 0.7092 - val_accuracy: 0.6225\nEpoch 6/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.5599 - accuracy: 0.7280 - val_loss: 0.7178 - val_accuracy: 0.6328\nEpoch 7/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.4879 - accuracy: 0.7755 - val_loss: 0.9081 - val_accuracy: 0.6221\nEpoch 8/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.4251 - accuracy: 0.8101 - val_loss: 0.9148 - val_accuracy: 0.6144\nEpoch 9/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.4117 - accuracy: 0.8192 - val_loss: 0.9019 - val_accuracy: 0.6135\nEpoch 10/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.3669 - accuracy: 0.8572 - val_loss: 0.9012 - val_accuracy: 0.6217\nEpoch 11/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.3177 - accuracy: 0.8692 - val_loss: 1.0913 - val_accuracy: 0.6156\nEpoch 12/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.2667 - accuracy: 0.8955 - val_loss: 1.3066 - val_accuracy: 0.6255\nEpoch 13/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.2635 - accuracy: 0.9015 - val_loss: 1.1932 - val_accuracy: 0.6333\nEpoch 14/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.2101 - accuracy: 0.9243 - val_loss: 1.1485 - val_accuracy: 0.6384\nEpoch 15/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.1909 - accuracy: 0.9348 - val_loss: 1.3259 - val_accuracy: 0.6285\nEpoch 16/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.1743 - accuracy: 0.9390 - val_loss: 1.3392 - val_accuracy: 0.6328\nEpoch 17/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.1924 - accuracy: 0.9340 - val_loss: 1.4034 - val_accuracy: 0.6260\nEpoch 18/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.1727 - accuracy: 0.9421 - val_loss: 1.2522 - val_accuracy: 0.6187\nEpoch 19/20\n146/146 [==============================] - 5s 33ms/step - loss: 0.1519 - accuracy: 0.9458 - val_loss: 1.3135 - val_accuracy: 0.6277\nEpoch 20/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.1589 - accuracy: 0.9456 - val_loss: 1.2940 - val_accuracy: 0.6225\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n\n\nNow that we have trained our model, we will plot the history of our model’s accuracy using both the training and validation sets. We will create a line graph using the package “Matplotlib” (imported in the beginning of this post). We will use this plot to further understand how the accuracy of our first model. Please run the code below.\n\nplt.plot(history_model_1.history['accuracy'], label='TRAINING ACCURACY')\nplt.plot(history_model_1.history['val_accuracy'], label='VALIDATION ACCURACY')\nplt.xlabel('EPOCHS')\nplt.ylabel('ACCURACY')\nplt.title('MODEL 1: ACCURACY COMPARSION')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe validation accuracy of model 1 is about 64% (in between 60% and 65%).\nI found the measurement bylooking at my graph and estimating the highest accuracy value (y-axis) for the Validation accuracy line (orange plot).\nIn comparison to the baseline accuracy, we can conclude that my model 1 is more accurate (about 10% more successful). This makes sense but leads me to believe that this still may not be the best model overall.\nOverfitting can occur at points where the training accuracy is much higher than the validation accuracy. There are signs of overfitting as the training model produces an accuracy over 90% while validation accuracy for model 1 never experiences an accuracy that exceeds 65%. By adjusting our model, we may be able to avoid overfitting in the future."
  },
  {
    "objectID": "posts/HW5/index.html#second-model",
    "href": "posts/HW5/index.html#second-model",
    "title": "Cats vs. Dogs: Image Classification",
    "section": "",
    "text": "In this next model, we are going to use data augmentation layers. This will involve us including layers that include modified copies of the same image in the training set. For our purposes, we will include data augmentation layers that rotate the images in hopes of seeing a higher accuracy. These slight changes in the versions of our images are called invariant features. They will help our model further understand the differences between the cat and dog images.\nWe will start by defining a function that allows us to complete data augmentation. This function will allow us to output a 2 by 2 grid of the same photo. Each photo should have a slightly different rotation. Please run the code cell below.\n\n#function allows us to implement data augmentation and outputs images with the appropriate labels\ndef augmented(train_ds, data_augmentation):\n  plt.figure(figsize=(10, 10))\n  for image, _ in train_ds.take(1):\n      original_image = image[0]\n      ax = plt.subplot(2, 2, 1)\n      plt.imshow(original_image / 255)\n      plt.axis('off')\n      ax.set_title('Original Image')\n\n  #display original and augemented images in a 2 by 2 grid\n  for i in range(2, 5):\n      ax = plt.subplot(2, 2, i)\n      augmented_image = data_augmentation(tf.expand_dims(original_image, 0))\n      plt.imshow(augmented_image[0] / 255)\n      plt.axis('off')\n      ax.set_title(f'Augmented Image {i - 1}')\n\n  #displays results\n  plt.show()\n\nNow that we have defined our function, we can implement different forms of data augmentation onto our images. First, we will create a keras.layer.RandomFlip() that takes the original image and then flips it for a few instances in our grid. The image will appear as a mirrored image of our original photo. Please run the code cell below to create this first data augmentation layer.\n\n#first data augmentation layer\ndata_augmentation_1 = tf.keras.Sequential([\n  tf.keras.layers.RandomFlip('horizontal'),\n])\naugmented(train_ds, data_augmentation_1)\n\n\n\n\n\n\n\n\nAs we can see in our grid of images above, the original image and second image show the original photo. However, the bottom first and third images appear as a mirrored version of the original.\nFor our second data augmentation layer, we will create a keras.layers.RandomRotation() layer that rotates the entire image in either the clockwise or counterclockwise direction. Please run the code cell below to create this second layer.\n\n#second data augmentation layer\ndata_augmentation_2 = tf.keras.Sequential([\n  tf.keras.layers.RandomRotation(0.2),\n])\naugmented(train_ds, data_augmentation_2)\n\n\n\n\n\n\n\n\nNow that we have sucessfully created two different data augmentation layers, we can train our model and create our visualization. We will define model 2 using keras.Sequential just as we did for model 1. The main difference here will be the two new layers that we will incorporate into our model. Please run the code cell below to define our secind model.\n\nmodel2 = keras.models.Sequential([\n    #new layers (defined above)\n    layers.RandomFlip(\"horizontal\", input_shape=(150, 150, 3)),\n    layers.RandomRotation(0.2),\n\n    #extra layers (model1)\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(10, activation='softmax')\n])\n\nNow that we have included our new layers in model 2, we can run the code cell below to compile and train our model while also extracting the information we need to determine the accuracy. Please run the code below.\n\n#allows model2 to compile\nmodel2.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n\n#given in blog post instructions\nhistory_model2 = model2.fit(train_ds,\n                            epochs=20,\n                            validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 7s 35ms/step - loss: 11.5206 - accuracy: 0.5083 - val_loss: 0.7474 - val_accuracy: 0.5486\nEpoch 2/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.8137 - accuracy: 0.5362 - val_loss: 0.7817 - val_accuracy: 0.5907\nEpoch 3/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.7655 - accuracy: 0.5370 - val_loss: 0.6930 - val_accuracy: 0.6105\nEpoch 4/20\n146/146 [==============================] - 6s 38ms/step - loss: 0.7324 - accuracy: 0.5686 - val_loss: 0.6570 - val_accuracy: 0.6195\nEpoch 5/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.7105 - accuracy: 0.5851 - val_loss: 0.6499 - val_accuracy: 0.6509\nEpoch 6/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.7004 - accuracy: 0.5922 - val_loss: 0.6346 - val_accuracy: 0.6509\nEpoch 7/20\n146/146 [==============================] - 5s 34ms/step - loss: 0.6769 - accuracy: 0.6169 - val_loss: 0.6230 - val_accuracy: 0.6870\nEpoch 8/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.6719 - accuracy: 0.6159 - val_loss: 0.6213 - val_accuracy: 0.6707\nEpoch 9/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6547 - accuracy: 0.6376 - val_loss: 0.6542 - val_accuracy: 0.6066\nEpoch 10/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.6598 - accuracy: 0.6243 - val_loss: 0.6001 - val_accuracy: 0.6991\nEpoch 11/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6482 - accuracy: 0.6428 - val_loss: 0.6273 - val_accuracy: 0.6522\nEpoch 12/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6463 - accuracy: 0.6443 - val_loss: 0.6163 - val_accuracy: 0.6698\nEpoch 13/20\n146/146 [==============================] - 5s 36ms/step - loss: 0.6460 - accuracy: 0.6402 - val_loss: 0.6158 - val_accuracy: 0.6917\nEpoch 14/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6377 - accuracy: 0.6509 - val_loss: 0.6059 - val_accuracy: 0.6943\nEpoch 15/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.6275 - accuracy: 0.6644 - val_loss: 0.6010 - val_accuracy: 0.6917\nEpoch 16/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6280 - accuracy: 0.6629 - val_loss: 0.5989 - val_accuracy: 0.6917\nEpoch 17/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6211 - accuracy: 0.6689 - val_loss: 0.5989 - val_accuracy: 0.6905\nEpoch 18/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6227 - accuracy: 0.6672 - val_loss: 0.6063 - val_accuracy: 0.6823\nEpoch 19/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6188 - accuracy: 0.6693 - val_loss: 0.5964 - val_accuracy: 0.6991\nEpoch 20/20\n146/146 [==============================] - 5s 35ms/step - loss: 0.6134 - accuracy: 0.6740 - val_loss: 0.5959 - val_accuracy: 0.7038\n\n\nWe will create another line graph using Matplotlib and the information for model 2. Please run the code cell below to create our visualization.\n\nplt.plot(history_model2.history['accuracy'], label='TRAINING ACCURACY')\nplt.plot(history_model2.history['val_accuracy'], label='VALIDATION ACCURACY')\nplt.xlabel('EPOCHS')\nplt.ylabel('ACCURACY')\nplt.title('MODEL 2_ ACCURACY COMPARISON')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe validation accuracy of model 2 is about 68% (in between 66 and 70%).\nI found the measurement by looking at my graph and estimating the highest accuracy value (y-axis) for the Validation accuracy line (orange plot).\nIn comparison to the model 1 validation accuracy, we can conclude that my model 2 is more accurate (about 4% more successful). This makes sense as my second model was more detailed but still very similar to model 1.\nOverfitting does not seem to occur with this model. The validation accuracy seems to always exceed the training accuracy. Even though this conclusion surprised me, it is a good sign that my model is improving and becoming more accurate."
  },
  {
    "objectID": "posts/HW5/index.html#third-model",
    "href": "posts/HW5/index.html#third-model",
    "title": "Cats vs. Dogs: Image Classification",
    "section": "",
    "text": "For our third model, we will include data preprocessing layers. The data is containeded in scale described by pixels with RGB values. As of right now, our RGB values are in between 0 and 255 which causes the model to take longer to train. These layers will allows us to focus more of our algorithm’s training energy on the data and less on the scale.\nWe will include the following code from the blog post instructions to create a preprocessing layer called preprocessor. Please run the following code cell.\n\n#given in instructions\n\ni = keras.Input(shape=(150, 150, 3))\n#the pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n#outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\nWe can now define our model3 using our new preprocessing layer as well as the layers we defined for model2. Please run the code cell below.\n\nmodel3 = keras.Sequential([\n    #new layer (defined above)\n    preprocessor,\n\n    #data augmentation layer (model2)\n    layers.RandomFlip(\"horizontal\", input_shape=(150, 150, 3)),\n    layers.RandomRotation(0.2),\n\n    #updated original layers (model1)\n    layers.Conv2D(64, (3, 3), activation='LeakyReLU'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(256, (3, 3), activation='LeakyReLU'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(512, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(1024, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(512, activation='LeakyReLU'),\n    layers.Dropout(0.3),\n    layers.Dense(10, activation='softmax')\n])\n\nNow that we have included our new layers in model 3, we can run the code cell below to compile and train our model while also extracting the information we need to determine the accuracy. Please run the code below.\n\n#allows model3 to compile\nmodel3.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n\n#given in blog post instructions\nhistory_model_3 = model3.fit(train_ds,\n                            epochs=20,\n                            validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 28s 142ms/step - loss: 0.7791 - accuracy: 0.5318 - val_loss: 0.6546 - val_accuracy: 0.6543\nEpoch 2/20\n146/146 [==============================] - 18s 121ms/step - loss: 0.6626 - accuracy: 0.5999 - val_loss: 0.5969 - val_accuracy: 0.6887\nEpoch 3/20\n146/146 [==============================] - 18s 121ms/step - loss: 0.6060 - accuracy: 0.6676 - val_loss: 0.5720 - val_accuracy: 0.7072\nEpoch 4/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.5772 - accuracy: 0.7030 - val_loss: 0.5285 - val_accuracy: 0.7485\nEpoch 5/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.5381 - accuracy: 0.7290 - val_loss: 0.4879 - val_accuracy: 0.7730\nEpoch 6/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.5149 - accuracy: 0.7487 - val_loss: 0.4506 - val_accuracy: 0.7889\nEpoch 7/20\n146/146 [==============================] - 18s 121ms/step - loss: 0.4734 - accuracy: 0.7742 - val_loss: 0.4162 - val_accuracy: 0.8151\nEpoch 8/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.4544 - accuracy: 0.7857 - val_loss: 0.3987 - val_accuracy: 0.8117\nEpoch 9/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.4330 - accuracy: 0.8042 - val_loss: 0.4069 - val_accuracy: 0.8143\nEpoch 10/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.4013 - accuracy: 0.8146 - val_loss: 0.3874 - val_accuracy: 0.8151\nEpoch 11/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3773 - accuracy: 0.8382 - val_loss: 0.3650 - val_accuracy: 0.8392\nEpoch 12/20\n146/146 [==============================] - 19s 129ms/step - loss: 0.3699 - accuracy: 0.8389 - val_loss: 0.3363 - val_accuracy: 0.8521\nEpoch 13/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3569 - accuracy: 0.8440 - val_loss: 0.3306 - val_accuracy: 0.8504\nEpoch 14/20\n146/146 [==============================] - 18s 121ms/step - loss: 0.3483 - accuracy: 0.8457 - val_loss: 0.3202 - val_accuracy: 0.8577\nEpoch 15/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3296 - accuracy: 0.8556 - val_loss: 0.3029 - val_accuracy: 0.8706\nEpoch 16/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3111 - accuracy: 0.8676 - val_loss: 0.3026 - val_accuracy: 0.8693\nEpoch 17/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3179 - accuracy: 0.8621 - val_loss: 0.2885 - val_accuracy: 0.8805\nEpoch 18/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.3043 - accuracy: 0.8732 - val_loss: 0.2718 - val_accuracy: 0.8852\nEpoch 19/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.2844 - accuracy: 0.8769 - val_loss: 0.2690 - val_accuracy: 0.8792\nEpoch 20/20\n146/146 [==============================] - 18s 120ms/step - loss: 0.2784 - accuracy: 0.8821 - val_loss: 0.2775 - val_accuracy: 0.8813\n\n\nWe will now create our visualization using the code below.\n\nplt.plot(history_model_3.history['accuracy'], label='TRAINING ACCURACY')\nplt.plot(history_model_3.history['val_accuracy'], label='VALIDATION ACCURACY')\nplt.xlabel('EPOCHS')\nplt.ylabel('ACCURACY')\nplt.title('MODEL 3: ACCURACY COMPARISON')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe validation accuracy of model 3 is about 87% (in between 87% and 90%).\nI found the measurement by looking at my graph and estimating the highest accuracy value (y-axis) for the Validation accuracy line (orange plot).\nIn comparison to the model 1 validation accuracy, we can conclude that my model 3 is signficantly more accurate (about 20%-25% more successful). This makes sense as my third model is the most detailed one so far out of the three.\nOverfitting does not really occur in the plot of this model’s accuracy as the validation accuracy is constantly greater than the training accuracy."
  },
  {
    "objectID": "posts/HW5/index.html#fourth-model",
    "href": "posts/HW5/index.html#fourth-model",
    "title": "Cats vs. Dogs: Image Classification",
    "section": "",
    "text": "There are many models rgar have already been trained to do related tasks. Even if the intended task is not exactly the same, there is a high probability that they have been trained to notice similar patterns and output similar results. For our fourth model, we will access a pre-existing “base model” to complete our current task of sorting images of dogs and cats. Please run the following code that was given in the blog post instructions.\n\n#from blog post instructions\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n12683000/12683000 [==============================] - 2s 0us/step\n\n\nNow that we have a defined “base model”, we can create a new model using some of layers from our existing models. For our fourth and final model, we will include the layers defined above as well as the data augmentation layers that were included in both model 2 and model 3. We will also need to include some of the starter layers that we first introduced in model 1.\n\nmodel4 = models.Sequential([\n    # augmentation layers\n    layers.RandomFlip(\"horizontal\", input_shape=(150, 150, 3)),\n    layers.RandomRotation(0.2),\n\n    base_model_layer,\n    layers.GlobalMaxPooling2D(),\n    layers.Dropout(0.2),\n    layers.Dense(2, activation='softmax'),  # outputs the final classification\n])\n\nNow that we have our model, use the code cell below to determine the summary of the model’s accuracy.\n\n# Compile the model\nmodel4.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\nmodel4.summary()\n\nModel: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_flip_3 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_3 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n model_1 (Functional)        (None, 5, 5, 960)         2996352   \n                                                                 \n global_max_pooling2d (Glob  (None, 960)               0         \n alMaxPooling2D)                                                 \n                                                                 \n dropout_6 (Dropout)         (None, 960)               0         \n                                                                 \n dense_9 (Dense)             (None, 2)                 1922      \n                                                                 \n=================================================================\nTotal params: 2998274 (11.44 MB)\nTrainable params: 1922 (7.51 KB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\n\n\nUse the following code cell to determine history just as we have done for the previous three models.\n\nhistory_model_4 = model4.fit(train_ds,\n                            epochs=20,\n                            validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 15s 64ms/step - loss: 1.4940 - accuracy: 0.8417 - val_loss: 0.2753 - val_accuracy: 0.9669\nEpoch 2/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.7269 - accuracy: 0.9140 - val_loss: 0.3333 - val_accuracy: 0.9592\nEpoch 3/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.5694 - accuracy: 0.9256 - val_loss: 0.2105 - val_accuracy: 0.9712\nEpoch 4/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.5304 - accuracy: 0.9283 - val_loss: 0.1809 - val_accuracy: 0.9703\nEpoch 5/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4488 - accuracy: 0.9341 - val_loss: 0.2300 - val_accuracy: 0.9600\nEpoch 6/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4191 - accuracy: 0.9318 - val_loss: 0.1424 - val_accuracy: 0.9733\nEpoch 7/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3847 - accuracy: 0.9334 - val_loss: 0.2245 - val_accuracy: 0.9617\nEpoch 8/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3594 - accuracy: 0.9382 - val_loss: 0.1431 - val_accuracy: 0.9695\nEpoch 9/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3263 - accuracy: 0.9362 - val_loss: 0.1258 - val_accuracy: 0.9729\nEpoch 10/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3442 - accuracy: 0.9337 - val_loss: 0.1430 - val_accuracy: 0.9725\nEpoch 11/20\n146/146 [==============================] - 6s 42ms/step - loss: 0.3117 - accuracy: 0.9341 - val_loss: 0.1328 - val_accuracy: 0.9746\nEpoch 12/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.2531 - accuracy: 0.9422 - val_loss: 0.1193 - val_accuracy: 0.9721\nEpoch 13/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.2852 - accuracy: 0.9332 - val_loss: 0.1046 - val_accuracy: 0.9742\nEpoch 14/20\n146/146 [==============================] - 6s 42ms/step - loss: 0.2595 - accuracy: 0.9390 - val_loss: 0.1241 - val_accuracy: 0.9665\nEpoch 15/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.2654 - accuracy: 0.9370 - val_loss: 0.1161 - val_accuracy: 0.9695\nEpoch 16/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.2700 - accuracy: 0.9353 - val_loss: 0.1018 - val_accuracy: 0.9742\nEpoch 17/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.2752 - accuracy: 0.9341 - val_loss: 0.1429 - val_accuracy: 0.9643\nEpoch 18/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.2929 - accuracy: 0.9352 - val_loss: 0.1529 - val_accuracy: 0.9652\nEpoch 19/20\n146/146 [==============================] - 6s 42ms/step - loss: 0.2891 - accuracy: 0.9311 - val_loss: 0.1177 - val_accuracy: 0.9725\nEpoch 20/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3265 - accuracy: 0.9304 - val_loss: 0.1335 - val_accuracy: 0.9729\n\n\nFinally, we can plot the validation accuracy and the training accuracy.\n\nplt.plot(history_model_4.history['accuracy'], label='TRAINING ACCURACY')\nplt.plot(history_model_4.history['val_accuracy'], label='VALIDATION ACCURACY')\nplt.xlabel('EPOCHS')\nplt.ylabel('ACCURACY')\nplt.title('MODEL 4: ACCURACY COMPARISON')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe validation accuracy of model 3 is about 97% (in between 96% and 98%).\nI found the measurement by looking at my graph and estimating the highest accuracy value (y-axis) for the Validation accuracy line (orange plot).\nIn comparison to all of our models, this is the model with the best accuracy as the validation accuracy is constantly above 96%.\nOverfitting does not really occur in the plot of this model’s accuracy as the validation accuracy is constantly greater than the training accuracy.\nWe have successfully implemented our fourth and final model and gathered the information regarding model accuracy."
  },
  {
    "objectID": "posts/HW5/index.html#test-accuracy",
    "href": "posts/HW5/index.html#test-accuracy",
    "title": "Cats vs. Dogs: Image Classification",
    "section": "",
    "text": "As model 4 is clearly the most accurate out of all four models, with a 97% validation accuracy rate, we will use test_ds to evaluate the accuracy. Please complete this step by running the code cell below.\n\ntest_loss, test_accuracy = model4.evaluate(test_ds)\nprint(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n\n37/37 [==============================] - 3s 68ms/step - loss: 0.1688 - accuracy: 0.9695\nTest Accuracy: 96.95%\n\n\nThe test we ran above is almost exactly the same as the validation accuracy we see in our plot for model 4. This is great indication that our model was both successful and accurate in classifying our images in our dataset.\nThank you for reading this post about image classification!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]