<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.547">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Isabella Woulfe">
<meta name="dcterms.date" content="2024-03-11">

<title>Isabella Woulfe: My Blog - Fact or Fiction? Fake News Classification Using Keras</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Isabella Woulfe: My Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Isabella Woulfe</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/woulfeig"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Fact or Fiction? Fake News Classification Using Keras</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Homework</div>
                <div class="quarto-category">code</div>
                <div class="quarto-category">Week 10</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Isabella Woulfe </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 11, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="fact-or-fiction" class="level1">
<h1>Fact or Fiction?</h1>
<section id="fake-news-classification-using-keras" class="level2">
<h2 class="anchored" data-anchor-id="fake-news-classification-using-keras">Fake News Classification using Keras</h2>
<p>The spread of false information, also reffered to as “fake news”, is a contant plague on modern life. Fake news can be extremely harmful to our society as a whole. But with today’s technology, we have tools that could combat the force that is fake news. In today’s blog post, we will use Keras and Machine Learning to classify articles as being truthful or not.</p>
<p>As always, we must start by importing the necassary packages and upgrading our Keras. Please run the following two code cells before we start uploading our dataset and defining our model.</p>
<div id="cell-2" class="cell" data-outputid="c7312b9d-c83a-46a9-ee7c-0b3aa306607c" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install keras <span class="op">--</span>upgrade</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)
Collecting keras
  Downloading keras-3.0.5-py3-none-any.whl (1.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 9.9 MB/s eta 0:00:00
Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)
Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)
Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)
Collecting namex (from keras)
  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)
Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)
Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)
Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)
Requirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)
Requirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)
Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)
Installing collected packages: namex, keras
  Attempting uninstall: keras
    Found existing installation: keras 2.15.0
    Uninstalling keras-2.15.0:
      Successfully uninstalled keras-2.15.0
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
tensorflow 2.15.0 requires keras&lt;2.16,&gt;=2.15.0, but you have keras 3.0.5 which is incompatible.
Successfully installed keras-3.0.5 namex-0.0.7</code></pre>
</div>
</div>
<div id="cell-3" class="cell" data-outputid="b5b5397a-ed34-465d-bed7-4162629a98c9" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install nltk</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)
Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)
Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)
Requirement already satisfied: regex&gt;=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)
Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)</code></pre>
</div>
</div>
<div id="cell-4" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">#import all of the necessary packages</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow_datasets <span class="im">as</span> tfds</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers, losses</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> TextVectorization</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> utils</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that we have imported all of our packages, we can start by acquiring our training data. Our data is in the form of different articles that we accessed from Kaggle. By running the code cell below, we can upload the link to the data and organize it into a dataframe using pandas. Please run the code cell below and see the resulting table for our</p>
<div id="cell-6" class="cell" data-outputid="d65e79fa-32e0-4836-e953-c7b405246652" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>train_url <span class="op">=</span> <span class="st">"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">#csv file into dataframe</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(train_url)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">

  <div id="df-87bfc545-427a-4184-8c88-a3fcf78bc9b2" class="colab-df-container">
    <div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Unnamed: 0</th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">fake</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>17366</td>
<td>Merkel: Strong result for Austria's FPO 'big c...</td>
<td>German Chancellor Angela Merkel said on Monday...</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>5634</td>
<td>Trump says Pence will lead voter fraud panel</td>
<td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>17487</td>
<td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
<td>On December 5, 2017, Circa s Sara Carter warne...</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>12217</td>
<td>Thyssenkrupp has offered help to Argentina ove...</td>
<td>Germany s Thyssenkrupp, has offered assistance...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5535</td>
<td>Trump say appeals court decision on travel ban...</td>
<td>President Donald Trump on Thursday called the ...</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-87bfc545-427a-4184-8c88-a3fcf78bc9b2')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-87bfc545-427a-4184-8c88-a3fcf78bc9b2 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-87bfc545-427a-4184-8c88-a3fcf78bc9b2');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-69365074-3ac7-4ea6-bd37-3251f783e9f7">
  <button class="colab-df-quickchart" onclick="quickchart('df-69365074-3ac7-4ea6-bd37-3251f783e9f7')" title="Suggest charts" style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-69365074-3ac7-4ea6-bd37-3251f783e9f7 button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>

    </div>
  </div>
</div>
</div>
<p>We now have successfully accessed our data that we will use to train our model! Before we continue, we must organize this data into a dataset. Using the following function, we will change all text to lowercase letters, remove stopwords, and create a dataset with inputs title and text and outputs a fake column. It is very important to have all of the necessary packages in order for us to filter out stop words and make a dataframe.</p>
<p>Please run the code cell below to define the make_dataset function.</p>
<div id="cell-8" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_dataset(df):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Function will create a dataset of our data and organize it with the given conditions</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">      df : pandas DataFrame containing columns: title, text, and fake</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co">      dataset : the dataset we created with the specified conditions (TensorFlow)</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">#lowercase text</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'text'</span>] <span class="op">=</span> df[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: x.lower())</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">#remove stopwords</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    nltk.download(<span class="st">'stopwords'</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    stop <span class="op">=</span> stopwords.words(<span class="st">'english'</span>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'text'</span>] <span class="op">=</span> df[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">' '</span>.join([word <span class="cf">for</span> word <span class="kw">in</span> x.split() <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> (stop)]))</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    title_tensor <span class="op">=</span> tf.constant(df[<span class="st">'title'</span>].values, dtype<span class="op">=</span>tf.string)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    text_tensor <span class="op">=</span> tf.constant(df[<span class="st">'text'</span>].values, dtype<span class="op">=</span>tf.string)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    fake_tensor <span class="op">=</span> tf.constant(df[<span class="st">'fake'</span>].values, dtype<span class="op">=</span>tf.int32)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    dataset <span class="op">=</span> tf.data.Dataset.from_tensor_slices(({<span class="st">"title"</span>: title_tensor, <span class="st">"text"</span>: text_tensor}, fake_tensor))</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    dataset <span class="op">=</span> dataset.batch(<span class="dv">100</span>)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">#outputs our dataset</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that our function is defined, we can use our training dataframe to output a dataset. Please run the code cell below to create a dataset from our training dataframe.</p>
<div id="cell-10" class="cell" data-outputid="bc4fd7df-74c1-4003-d517-33e6637b21b0" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">#create dataset using training data (see above)</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>ds <span class="op">=</span> make_dataset(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.</code></pre>
</div>
</div>
<p>For our next step, we will work with the validation set. This will involve taking 20% of our training data set to use for the validation.</p>
<div id="cell-12" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">#create validation set</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>ds <span class="op">=</span> ds.shuffle(buffer_size <span class="op">=</span> <span class="bu">len</span>(ds), reshuffle_each_iteration<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.2</span> <span class="op">*</span> <span class="bu">len</span>(ds))</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>val <span class="op">=</span> ds.take(val_size)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> ds.skip(val_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-13" class="cell" data-outputid="5e7d2f44-aca4-4138-863a-7a87c5053193" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(train), <span class="bu">len</span>(val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>(180, 45)</code></pre>
</div>
</div>
<p>The base rate is used to determine the accuracy of the model. It allows us to find a value to compare the results of our model to. Please run the code cell below to determine the base rate for our dataset.</p>
<div id="cell-15" class="cell" data-outputid="b10bf4f1-7969-4726-a6ef-32fae935a022" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">#determine the base rate for our training dataset</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>fake_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>total_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, labels <span class="kw">in</span> train:</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    fake_count <span class="op">+=</span> tf.reduce_sum(labels).numpy()</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    total_count <span class="op">+=</span> <span class="bu">len</span>(labels)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>base_rate <span class="op">=</span> fake_count <span class="op">/</span> total_count</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Base rate:"</span>, base_rate)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Base rate: 0.523928909688562</code></pre>
</div>
</div>
<p>After printing the base rate, ppreapre the text vectorization for the model. The following code cell was given in the Homework 6 blog post. Please run the code cell below before proceeding with the models.</p>
<div id="cell-17" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">#preparing a text vectorization layer for tf model</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>size_vocabulary <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> standardization(input_data):</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    lowercase <span class="op">=</span> tf.strings.lower(input_data)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    no_punctuation <span class="op">=</span> tf.strings.regex_replace(lowercase,</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>                                  <span class="st">'[</span><span class="sc">%s</span><span class="st">]'</span> <span class="op">%</span> re.escape(string.punctuation),<span class="st">''</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> no_punctuation</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>title_vectorize_layer <span class="op">=</span> TextVectorization(</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    standardize<span class="op">=</span>standardization,</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    max_tokens<span class="op">=</span>size_vocabulary, <span class="co"># only consider this many words</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    output_mode<span class="op">=</span><span class="st">'int'</span>,</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    output_sequence_length<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>title_vectorize_layer.adapt(train.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: x[<span class="st">"title"</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The following cell will also add an embedding layer that will be used in our models. This layer will access the word embeddings that we will use to determine whether or not our articles can be classified as fake news. Please run the code cell below.</p>
<div id="cell-19" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>shared_embedding_layer <span class="op">=</span> layers.Embedding(size_vocabulary, <span class="dv">3</span>, name<span class="op">=</span><span class="st">"embedding"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now start on defining our three models that will be used to determine the validity of different news sources.</p>
</section>
<section id="model-1" class="level2">
<h2 class="anchored" data-anchor-id="model-1">Model 1</h2>
<p>In our first model, our only input will be the article title. This means that the article title will be used by our model to determine how we classify the given article. We will define the following layers below to create our model. Please run the code cell below to create the first model.</p>
<div id="cell-22" class="cell" data-outputid="961ffe4d-b146-4c43-f763-7402541b181b" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">#defining the model when the input is our article title</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>title_input <span class="op">=</span> tf.keras.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span>tf.string, name<span class="op">=</span><span class="st">"title"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> title_vectorize_layer(title_input)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> shared_embedding_layer(title_features)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Dropout(<span class="fl">0.5</span>)(title_features)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.GlobalAveragePooling1D()(title_features)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(title_features)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Dropout(<span class="fl">0.3</span>)(title_features)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(title_features)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>title_output <span class="op">=</span> layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>, name<span class="op">=</span><span class="st">"title_output"</span>)(title_features)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co">#create model 1</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>model_1 <span class="op">=</span> tf.keras.Model(inputs<span class="op">=</span>title_input, outputs<span class="op">=</span>title_output)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>model_1.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional_1"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                         </span>┃<span style="font-weight: bold"> Output Shape                </span>┃<span style="font-weight: bold">         Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ title (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)                   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                   │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ text_vectorization                   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">500</span>)                 │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">TextVectorization</span>)                  │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ embedding (<span style="color: #0087ff; text-decoration-color: #0087ff">Embedding</span>)                │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">500</span>, <span style="color: #00af00; text-decoration-color: #00af00">3</span>)              │           <span style="color: #00af00; text-decoration-color: #00af00">6,000</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)                    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">500</span>, <span style="color: #00af00; text-decoration-color: #00af00">3</span>)              │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ global_average_pooling1d             │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">3</span>)                   │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">GlobalAveragePooling1D</span>)             │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                        │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)                  │             <span style="color: #00af00; text-decoration-color: #00af00">256</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)                  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)                  │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)                  │           <span style="color: #00af00; text-decoration-color: #00af00">2,080</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ title_output (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                   │              <span style="color: #00af00; text-decoration-color: #00af00">33</span> │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">8,369</span> (32.69 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">8,369</span> (32.69 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
<p>Now that we have created Model 1, we can visualize and further understand the structure of the model by creating a flowchart. The following code was provided in the blog post instructions to complete this step. Please run the code below.</p>
<div id="cell-24" class="cell" data-outputid="6eb15431-f9a4-4c7d-bb79-1c09fb2018e8" data-execution_count="13">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co">#model visualization (flowchart)</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>utils.plot_model(model_1, <span class="st">"model_1.png"</span>,</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>                       show_shapes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>                       show_layer_names<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Please run the code cell below to compile our data.</p>
<div id="cell-26" class="cell" data-outputid="5b1a4079-4c50-4cde-850b-c5f5d676c59c" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co">#compile with training and valiation data</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>model_1.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>, loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>, metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>history_1 <span class="op">=</span> model_1.fit(train, epochs<span class="op">=</span><span class="dv">20</span>, validation_data<span class="op">=</span>val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 8ms/step - accuracy: 0.5261 - loss: 0.6920 - val_accuracy: 0.5191 - val_loss: 0.6913
Epoch 2/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 7ms/step - accuracy: 0.5428 - loss: 0.6875 - val_accuracy: 0.6278 - val_loss: 0.6445
Epoch 3/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.6460 - loss: 0.6297 - val_accuracy: 0.8084 - val_loss: 0.4680
Epoch 4/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 8ms/step - accuracy: 0.7717 - loss: 0.4842 - val_accuracy: 0.7813 - val_loss: 0.4412
Epoch 5/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.8210 - loss: 0.4007 - val_accuracy: 0.9036 - val_loss: 0.2753
Epoch 6/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8376 - loss: 0.3631 - val_accuracy: 0.9196 - val_loss: 0.2366
Epoch 7/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8710 - loss: 0.3065 - val_accuracy: 0.9033 - val_loss: 0.2439
Epoch 8/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8675 - loss: 0.3050 - val_accuracy: 0.9218 - val_loss: 0.2059
Epoch 9/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8793 - loss: 0.2902 - val_accuracy: 0.9351 - val_loss: 0.1832
Epoch 10/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8772 - loss: 0.2814 - val_accuracy: 0.9296 - val_loss: 0.1879
Epoch 11/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 7ms/step - accuracy: 0.8736 - loss: 0.2933 - val_accuracy: 0.9307 - val_loss: 0.1823
Epoch 12/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.8933 - loss: 0.2590 - val_accuracy: 0.9427 - val_loss: 0.1629
Epoch 13/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8951 - loss: 0.2500 - val_accuracy: 0.9371 - val_loss: 0.1650
Epoch 14/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9071 - loss: 0.2300 - val_accuracy: 0.9444 - val_loss: 0.1604
Epoch 15/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8929 - loss: 0.2572 - val_accuracy: 0.9202 - val_loss: 0.1992
Epoch 16/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9019 - loss: 0.2425 - val_accuracy: 0.9460 - val_loss: 0.1492
Epoch 17/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9067 - loss: 0.2249 - val_accuracy: 0.9251 - val_loss: 0.1852
Epoch 18/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9080 - loss: 0.2199 - val_accuracy: 0.9484 - val_loss: 0.1470
Epoch 19/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.9089 - loss: 0.2169 - val_accuracy: 0.9438 - val_loss: 0.1496
Epoch 20/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 9ms/step - accuracy: 0.9110 - loss: 0.2197 - val_accuracy: 0.9442 - val_loss: 0.1469</code></pre>
</div>
</div>
<p>Now that we have completed all of work to run and populate our model with our training data, we can create a visualization to the accuracy of our model. Similar to last blog post, we will use the package matplotlib to create line graphs that compare the validation accuracy with the model accuracy. Please run the code cell below to create our plot for Model 1.</p>
<div id="cell-28" class="cell" data-outputid="69dc4f81-f719-4c53-a788-a5531c35fccc" data-execution_count="15">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co">#plot accuracy of training data vs. validation data</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history_1.history[<span class="st">'accuracy'</span>], label<span class="op">=</span><span class="st">'Training Accuracy'</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>plt.plot(history_1.history[<span class="st">'val_accuracy'</span>], label<span class="op">=</span><span class="st">'Validation Accuracy'</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Model 1: Article Titles'</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The validation accuracy is consistantly higher than the training accuracy. The validation accuract flucutates for periods but never experiences dramatic drops. In all, the accuracy never seems to exceed a rate of 95%</p>
</section>
<section id="model-2" class="level2">
<h2 class="anchored" data-anchor-id="model-2">Model 2</h2>
<p>In our second model, our only input will be the article text. This means that the text of each article will be used by our model to determine how we classify the given article. We will define the following layers below to create our model. Please run the code cell below to create the second model.</p>
<div id="cell-31" class="cell" data-outputid="6cddae3a-eb75-4acb-cd92-ba201a515e6c" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co">#defining our model when our input is the text of the article</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>text_input <span class="op">=</span> tf.keras.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span>tf.string, name<span class="op">=</span><span class="st">"text"</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> title_vectorize_layer(text_input)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> shared_embedding_layer(text_features)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Dropout(<span class="fl">0.5</span>)(text_features)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.GlobalAveragePooling1D()(text_features)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(text_features)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Dropout(<span class="fl">0.3</span>)(text_features)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(text_features)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>text_output <span class="op">=</span> layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>, name<span class="op">=</span><span class="st">"text_output"</span>)(text_features)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a><span class="co">#create nodel 2</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>model_2 <span class="op">=</span> tf.keras.Model(inputs<span class="op">=</span>text_input, outputs<span class="op">=</span>text_output)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>model_2.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional_3"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                         </span>┃<span style="font-weight: bold"> Output Shape                </span>┃<span style="font-weight: bold">         Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ text (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)                    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                   │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ text_vectorization                   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">500</span>)                 │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">TextVectorization</span>)                  │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ embedding (<span style="color: #0087ff; text-decoration-color: #0087ff">Embedding</span>)                │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">500</span>, <span style="color: #00af00; text-decoration-color: #00af00">3</span>)              │           <span style="color: #00af00; text-decoration-color: #00af00">6,000</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)                  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">500</span>, <span style="color: #00af00; text-decoration-color: #00af00">3</span>)              │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ global_average_pooling1d_1           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">3</span>)                   │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">GlobalAveragePooling1D</span>)             │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)                  │             <span style="color: #00af00; text-decoration-color: #00af00">256</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)                  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)                  │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)                  │           <span style="color: #00af00; text-decoration-color: #00af00">2,080</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ text_output (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                   │              <span style="color: #00af00; text-decoration-color: #00af00">33</span> │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">8,369</span> (32.69 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">8,369</span> (32.69 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
<p>Now that we have created Model 2, we can visualize and further understand the structure of the model by creating a flowchart. The following code was provided in the blog post instructions to complete this step. Please run the code below.</p>
<div id="cell-33" class="cell" data-outputid="479a68c9-64bd-4092-e763-f7d6bde9a064" data-execution_count="17">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co">#visualize model 2 (flowchart)</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>utils.plot_model(model_2, <span class="st">"model_2.png"</span>,</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>                       show_shapes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>                       show_layer_names<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Please run the code cell below to compile our data.</p>
<div id="cell-35" class="cell" data-outputid="4971b129-3292-4dd8-8b0d-0b03e12d0d82" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co">#compile with the training and validation data</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>model_2.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>, loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>, metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>history_2 <span class="op">=</span> model_2.fit(train, epochs<span class="op">=</span><span class="dv">20</span>, validation_data<span class="op">=</span>val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 14ms/step - accuracy: 0.6953 - loss: 0.6114 - val_accuracy: 0.9042 - val_loss: 0.2832
Epoch 2/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 12ms/step - accuracy: 0.8953 - loss: 0.2754 - val_accuracy: 0.9267 - val_loss: 0.2092
Epoch 3/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9153 - loss: 0.2213 - val_accuracy: 0.9402 - val_loss: 0.1799
Epoch 4/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 28ms/step - accuracy: 0.9259 - loss: 0.1953 - val_accuracy: 0.9500 - val_loss: 0.1598
Epoch 5/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9361 - loss: 0.1719 - val_accuracy: 0.9549 - val_loss: 0.1477
Epoch 6/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9362 - loss: 0.1719 - val_accuracy: 0.9584 - val_loss: 0.1402
Epoch 7/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9419 - loss: 0.1598 - val_accuracy: 0.9551 - val_loss: 0.1362
Epoch 8/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9447 - loss: 0.1481 - val_accuracy: 0.9618 - val_loss: 0.1278
Epoch 9/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9456 - loss: 0.1497 - val_accuracy: 0.9604 - val_loss: 0.1225
Epoch 10/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9480 - loss: 0.1448 - val_accuracy: 0.9629 - val_loss: 0.1250
Epoch 11/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9502 - loss: 0.1345 - val_accuracy: 0.9649 - val_loss: 0.1199
Epoch 12/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9486 - loss: 0.1391 - val_accuracy: 0.9651 - val_loss: 0.1178
Epoch 13/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9529 - loss: 0.1293 - val_accuracy: 0.9662 - val_loss: 0.1116
Epoch 14/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9452 - loss: 0.1419 - val_accuracy: 0.9664 - val_loss: 0.1073
Epoch 15/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9590 - loss: 0.1173 - val_accuracy: 0.9671 - val_loss: 0.1042
Epoch 16/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9562 - loss: 0.1210 - val_accuracy: 0.9662 - val_loss: 0.1043
Epoch 17/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9591 - loss: 0.1123 - val_accuracy: 0.9691 - val_loss: 0.1014
Epoch 18/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9602 - loss: 0.1101 - val_accuracy: 0.9678 - val_loss: 0.1067
Epoch 19/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9610 - loss: 0.1082 - val_accuracy: 0.9684 - val_loss: 0.0988
Epoch 20/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9555 - loss: 0.1207 - val_accuracy: 0.9707 - val_loss: 0.0963</code></pre>
</div>
</div>
<p>Now that we have completed all of work to run and populate our model with our training data, we can create a visualization to the accuracy of our model. Similar to last blog post, we will use the package matplotlib to create line graphs that compare the validation accuracy with the model accuracy. Please run the code cell below to create our plot for Model 2.</p>
<div id="cell-37" class="cell" data-outputid="a9a62c26-231c-4055-f961-b9a2d4f48491" data-execution_count="19">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co">#plot validation accuract vs. training accuracy of modele 2</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history_2.history[<span class="st">'accuracy'</span>], label<span class="op">=</span><span class="st">'Training Accuracy'</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>plt.plot(history_2.history[<span class="st">'val_accuracy'</span>], label<span class="op">=</span><span class="st">'Validation Accuracy'</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Model 2: Article Texts'</span>)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The validation accuracy for model 2 was consistently higher than the training accuracy. In general, we can see rates under about 96%.</p>
</section>
<section id="model-3" class="level2">
<h2 class="anchored" data-anchor-id="model-3">Model 3</h2>
<p>In our third model, our inputs will be both the article text and the article title. This will work as a sort of combination of the first and second model. In theory, we should expect this model to be the most accurate as it takes the most information as inputs and allows the model to make the best decision. Please run the code cell below to create the third model.</p>
<div id="cell-40" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>main <span class="op">=</span> layers.concatenate([title_features, text_features], axis <span class="op">=</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-41" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>main <span class="op">=</span> layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(main)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>main <span class="op">=</span> layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(main)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>main <span class="op">=</span> layers.Dropout(<span class="fl">0.3</span>)(main)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> layers.Dense(<span class="dv">1</span>, name <span class="op">=</span> <span class="st">"genre"</span>)(main)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-42" class="cell" data-outputid="ad3c8aea-15f2-4346-d1cc-c8a9b44da482" data-execution_count="22">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co">#define our model so that our inputs are both article title and article text</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>model_3 <span class="op">=</span> keras.Model(</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> [title_input, text_input],</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> output</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>model_3.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional_5"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)              </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">        Param # </span>┃<span style="font-weight: bold"> Connected to           </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩
│ title (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)        │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)              │              <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                      │
├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤
│ text (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)              │              <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                      │
├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤
│ text_vectorization        │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">500</span>)            │              <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ title[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>],           │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">TextVectorization</span>)       │                        │                │ text[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]             │
├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤
│ embedding (<span style="color: #0087ff; text-decoration-color: #0087ff">Embedding</span>)     │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">500</span>, <span style="color: #00af00; text-decoration-color: #00af00">3</span>)         │          <span style="color: #00af00; text-decoration-color: #00af00">6,000</span> │ text_vectorization[<span style="color: #00af00; text-decoration-color: #00af00">0</span>]… │
│                           │                        │                │ text_vectorization[<span style="color: #00af00; text-decoration-color: #00af00">1</span>]… │
├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤
│ dropout (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">500</span>, <span style="color: #00af00; text-decoration-color: #00af00">3</span>)         │              <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ embedding[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]        │
├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤
│ dropout_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)       │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">500</span>, <span style="color: #00af00; text-decoration-color: #00af00">3</span>)         │              <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ embedding[<span style="color: #00af00; text-decoration-color: #00af00">1</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]        │
├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤
│ global_average_pooling1d  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">3</span>)              │              <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ dropout[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]          │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">GlobalAveragePooling1D</span>)  │                        │                │                        │
├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤
│ global_average_pooling1d… │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">3</span>)              │              <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ dropout_2[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]        │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">GlobalAveragePooling1D</span>)  │                        │                │                        │
├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤
│ dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)             │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)             │            <span style="color: #00af00; text-decoration-color: #00af00">256</span> │ global_average_poolin… │
├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤
│ dense_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)             │            <span style="color: #00af00; text-decoration-color: #00af00">256</span> │ global_average_poolin… │
├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤
│ dropout_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)       │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)             │              <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ dense[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]            │
├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤
│ dropout_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)       │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)             │              <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ dense_2[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]          │
├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤
│ dense_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)             │          <span style="color: #00af00; text-decoration-color: #00af00">2,080</span> │ dropout_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]        │
├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤
│ dense_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)             │          <span style="color: #00af00; text-decoration-color: #00af00">2,080</span> │ dropout_3[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]        │
├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤
│ concatenate (<span style="color: #0087ff; text-decoration-color: #0087ff">Concatenate</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)             │              <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ dense_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>],         │
│                           │                        │                │ dense_3[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]          │
├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤
│ dense_4 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)             │          <span style="color: #00af00; text-decoration-color: #00af00">2,080</span> │ concatenate[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]      │
├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤
│ dense_5 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)             │          <span style="color: #00af00; text-decoration-color: #00af00">1,056</span> │ dense_4[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]          │
├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤
│ dropout_4 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)       │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)             │              <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ dense_5[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]          │
├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤
│ genre (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)             │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)              │             <span style="color: #00af00; text-decoration-color: #00af00">33</span> │ dropout_4[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]        │
└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">13,841</span> (54.07 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">13,841</span> (54.07 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
<p>Now that we have created Model 3, we can visualize and further understand the structure of the model by creating a flowchart. The following code was provided in the blog post instructions to complete this step. Please run the code below.</p>
<div id="cell-44" class="cell" data-outputid="555da90f-5963-4efc-94b2-32ddc4a88e0e" data-execution_count="23">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co">#create visualization of model 3 (flowchart)</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>utils.plot_model(model_3, <span class="st">"model_3.png"</span>,</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>                       show_shapes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>                       show_layer_names<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-24-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Please run the code cell below to compile our data.</p>
<div id="cell-46" class="cell" data-outputid="8348f3d9-69b5-4434-8ac9-e3934bb22d55" data-execution_count="24">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co">#compile with the training and validation data</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>model_3.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>, loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>, metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>history_3 <span class="op">=</span> model_3.fit(train, epochs<span class="op">=</span><span class="dv">20</span>, validation_data<span class="op">=</span>val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 18ms/step - accuracy: 0.8375 - loss: 0.7682 - val_accuracy: 0.9740 - val_loss: 0.1477
Epoch 2/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9620 - loss: 0.1800 - val_accuracy: 0.9367 - val_loss: 0.1902
Epoch 3/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9531 - loss: 0.1808 - val_accuracy: 0.9756 - val_loss: 0.1384
Epoch 4/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 28ms/step - accuracy: 0.9733 - loss: 0.1579 - val_accuracy: 0.9764 - val_loss: 0.1114
Epoch 5/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 23ms/step - accuracy: 0.9621 - loss: 0.1465 - val_accuracy: 0.9773 - val_loss: 0.1230
Epoch 6/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9718 - loss: 0.1283 - val_accuracy: 0.9796 - val_loss: 0.1080
Epoch 7/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9519 - loss: 0.2628 - val_accuracy: 0.7851 - val_loss: 0.5011
Epoch 8/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.7019 - loss: 0.6588 - val_accuracy: 0.9351 - val_loss: 0.2822
Epoch 9/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 16ms/step - accuracy: 0.8409 - loss: 0.4528 - val_accuracy: 0.7669 - val_loss: 0.5394
Epoch 10/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.7886 - loss: 0.5026 - val_accuracy: 0.9316 - val_loss: 0.2635
Epoch 11/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9362 - loss: 0.2705 - val_accuracy: 0.9724 - val_loss: 0.1719
Epoch 12/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 19ms/step - accuracy: 0.9053 - loss: 0.3286 - val_accuracy: 0.7907 - val_loss: 0.4918
Epoch 13/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.7677 - loss: 0.5278 - val_accuracy: 0.8847 - val_loss: 0.3257
Epoch 14/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.8703 - loss: 0.3216 - val_accuracy: 0.7642 - val_loss: 0.4942
Epoch 15/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 28ms/step - accuracy: 0.8745 - loss: 0.3119 - val_accuracy: 0.9333 - val_loss: 0.2088
Epoch 16/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 31ms/step - accuracy: 0.9337 - loss: 0.2126 - val_accuracy: 0.9693 - val_loss: 0.1838
Epoch 17/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9498 - loss: 0.2347 - val_accuracy: 0.9673 - val_loss: 0.1607
Epoch 18/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9525 - loss: 0.2044 - val_accuracy: 0.9704 - val_loss: 0.1487
Epoch 19/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 18ms/step - accuracy: 0.9604 - loss: 0.1740 - val_accuracy: 0.9727 - val_loss: 0.1721
Epoch 20/20
180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9647 - loss: 0.1816 - val_accuracy: 0.9711 - val_loss: 0.1413</code></pre>
</div>
</div>
<p>Now that we have completed all of work to run and populate our model with our training data, we can create a visualization to the accuracy of our model. Similar to last blog post, we will use the package matplotlib to create line graphs that compare the validation accuracy with the model accuracy. Please run the code cell below to create our plot for Model 3.</p>
<div id="cell-48" class="cell" data-outputid="0a075f96-5d90-4a0d-d957-7ace93d11d84" data-execution_count="25">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co">#plot validation accuracy vs. training accuracy for model 3</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history_3.history[<span class="st">'accuracy'</span>], label<span class="op">=</span><span class="st">'Training Accuracy'</span>)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>plt.plot(history_3.history[<span class="st">'val_accuracy'</span>], label<span class="op">=</span><span class="st">'Validation Accuracy'</span>)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Model 3: Titles and Text'</span>)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This model experienced some overfitting and lots of flucuations. However, it did see a very high accuracy and regularly reached a rate that exceeded 97%.</p>
</section>
<section id="testing-with-new-data" class="level2">
<h2 class="anchored" data-anchor-id="testing-with-new-data">Testing with New Data</h2>
<p>We have successfully created three different models! Using the model that performed the best, model 3, we will use a new dataset and classify these articles as fake news or not. This dataset will be different than the one we used to train our three models.</p>
<p>To start off, please run the code cell below to import our new dataset.</p>
<div id="cell-51" class="cell" data-outputid="8f57b609-2346-4e24-eeb9-8daef92d2bda" data-execution_count="26">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>test_url <span class="op">=</span> <span class="st">"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>df_test <span class="op">=</span> pd.read_csv(test_url)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>df_test.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">

  <div id="df-76f5081d-40a3-461b-a654-e4c08bc5f388" class="colab-df-container">
    <div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Unnamed: 0</th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">fake</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>420</td>
<td>CNN And MSNBC Destroy Trump, Black Out His Fa...</td>
<td>Donald Trump practically does something to cri...</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>14902</td>
<td>Exclusive: Kremlin tells companies to deliver ...</td>
<td>The Kremlin wants good news. The Russian lead...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>322</td>
<td>Golden State Warriors Coach Just WRECKED Trum...</td>
<td>On Saturday, the man we re forced to call Pre...</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>16108</td>
<td>Putin opens monument to Stalin's victims, diss...</td>
<td>President Vladimir Putin inaugurated a monumen...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>10304</td>
<td>BREAKING: DNC HACKER FIRED For Bank Fraud…Blam...</td>
<td>Apparently breaking the law and scamming the g...</td>
<td>1</td>
</tr>
</tbody>
</table>

</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-76f5081d-40a3-461b-a654-e4c08bc5f388')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-76f5081d-40a3-461b-a654-e4c08bc5f388 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-76f5081d-40a3-461b-a654-e4c08bc5f388');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-b0554b76-c5fe-4543-b34b-42cb63c3937d">
  <button class="colab-df-quickchart" onclick="quickchart('df-b0554b76-c5fe-4543-b34b-42cb63c3937d')" title="Suggest charts" style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-b0554b76-c5fe-4543-b34b-42cb63c3937d button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>

    </div>
  </div>
</div>
</div>
<p>Now that we have uploaded our new data, we will organize it into our dataset by using the same function we defined for our training dataset.</p>
<div id="cell-53" class="cell" data-outputid="33b62a99-1aa5-4e14-9e0e-aaacd80645c4" data-execution_count="27">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>test_ds <span class="op">=</span> make_dataset(df_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!</code></pre>
</div>
</div>
<p>Finally, we will test the accuracy of model 3 using this data.</p>
<div id="cell-55" class="cell" data-outputid="1bcf244f-cd4e-48a5-9f3b-743e06f0f403" data-execution_count="28">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>test_loss, test_accuracy <span class="op">=</span> model_3.evaluate(test_ds)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Test Accuracy: </span><span class="sc">{</span>test_accuracy <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>225/225 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.9701 - loss: 0.1552
Test Accuracy: 97.06%</code></pre>
</div>
</div>
<p>This model had an accuracy of about 97% for this new set of data.</p>
</section>
<section id="visual-embedding" class="level2">
<h2 class="anchored" data-anchor-id="visual-embedding">Visual Embedding</h2>
<p>In the last section of today’s blog post, we will visualize the embedding of our third model. This requires us to use PCA from sklearn. Please run the code cell below.</p>
<div id="cell-58" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> model_3.get_layer(<span class="st">'embedding'</span>).get_weights()[<span class="dv">0</span>] <span class="co"># get the weights from the embedding layer</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> title_vectorize_layer.get_vocabulary()                <span class="co"># get the vocabulary from our data prep for later</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> pca.fit_transform(weights)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>embedding_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'word'</span> : vocab,</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'x0'</span>   : weights[:,<span class="dv">0</span>],</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'x1'</span>   : weights[:,<span class="dv">1</span>]</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, import our plotly package and make a visualization in the form of the scatterplot</p>
<div id="cell-60" class="cell" data-outputid="0b1762ca-0d51-474b-e9cf-e120ad4533b0" data-execution_count="30">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co">#necessary packages</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="co">#create visualization</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.scatter(embedding_df,</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>                 x <span class="op">=</span> <span class="st">"x0"</span>,</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>                 y <span class="op">=</span> <span class="st">"x1"</span>,</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>                 size <span class="op">=</span> <span class="bu">list</span>(np.ones(<span class="bu">len</span>(embedding_df))),</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>                 size_max <span class="op">=</span> <span class="dv">5</span>,</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>                 hover_name <span class="op">=</span> <span class="st">"word"</span>,</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>                 title<span class="op">=</span><span class="st">'Word Embeddings Visualization'</span>)</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>fig.update_layout(</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>    xaxis<span class="op">=</span><span class="bu">dict</span>(title<span class="op">=</span><span class="st">'Principal Component 1'</span>),</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>    yaxis<span class="op">=</span><span class="bu">dict</span>(title<span class="op">=</span><span class="st">'Principal Component 2'</span>),</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<meta charset="utf-8">

    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.24.1.min.js"></script>                <div id="8498206e-34b4-4bad-80b7-dab2797befdf" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("8498206e-34b4-4bad-80b7-dab2797befdf")) {                    Plotly.newPlot(                        "8498206e-34b4-4bad-80b7-dab2797befdf",                        [{"hovertemplate":"\u003cb\u003e%{hovertext}\u003c\u002fb\u003e\u003cbr\u003e\u003cbr\u003ex0=%{x}\u003cbr\u003ex1=%{y}\u003cbr\u003esize=%{marker.size}\u003cextra\u003e\u003c\u002fextra\u003e","hovertext":["","[UNK]","to","trump","in","video","of","for","on","the","us","a","and","with","says","is","after","obama","at","over","from","as","house","about","watch","by","his","hillary","new","not","will","clinton","trump\u2019s","white","president","just","he","be","it","out","this","bill","who","are","russia","state","that","north","republican","up","against","her","court","was","him","has","senate","election","you","media","breaking","news","how","vote","korea","donald","calls","black","no","police","why","republicans","have","\u2013","muslim","tax","more","gop","down","campaign","trumps","may","deal","what","gets","china","one","obama\u2019s","if","democrats","party","into","back","they","tweets","iran","former","pm","government","people","first","fbi","eu","talks","un","their","off","during","syria","speech","america","chief","attack","russian","top","fox","but","security","cnn","tells","leader","ban","congress","all","senator","plan","an","judge","war","minister","law","democrat","could","she","man","twitter","should","makes","sanders","when","military","would","it\u2019s","make","factbox","say","report","liberal","can","south","probe","brexit","shows","racist","goes","million","like","get","being","presidential","official","hillary\u2019s","supreme","nuclear","host","two","putin","foreign","before","your","sanctions","governor","healthcare","cruz","americans","wow","take","our","american","wants","tweet","gun","them","go","woman","support","islamic","time","or","border","than","rally","day","most","fight","obamacare","show","political","want","under","help","debate","because","women","we","so","trade","race","next","attacks","meet","illegal","urges","lawmakers","he\u2019s","uk","rights","poll","national","do","call","supporters","german","while","response","syrian","school","saudi","claims","bernie","win","ryan","conservative","killed","world","visit","big","travel","room","policy","must","meeting","leaders","don\u2019t","crisis","budget","gives","asks","turkey","see","did","ted","old","group","won\u2019t","takes","states","refugees","panel","mexico","defense","now","sources","right","plans","email","details","department","candidate","warns","ties","antitrump","administration","death","still","got","going","fake","emails","stop","only","democratic","secretary","mayor","end","climate","way","students","press","opposition","officials","immigration","hilarious","arrested","air","pick","huge","free","city","tillerson","years","protesters","open","left","head","general","supporter","seeks","money","kill","exclusive","wall","texas","major","john","reason","cops","caught","own","myanmar","i","comey","work","voter","reporter","leftist","iraq","health","give","voters","justice","interview","washington","violence","shocking","here\u2019s","again","year","really","move","case","truth","reform","live","use","threatens","presidency","lives","federal","can\u2019t","boiler","rules","last","secret","paul","high","had","conservatives","aid","need","made","lawyer","business","its","york","times","sexual","said","image","does","director","change","britain","ahead","speaker","social","order","key","dead","college","some","puerto","post","office","family","ep","coalition","team","list","latest","investigation","home","florida","bomb","push","nominee","merkel","lawmaker","dnc","british","boom","army","tv","run","never","lie","isis","himself","george","fire","discuss","decision","threat","keep","ever","shooting","protest","jerusalem","face","been","attorney","told","job","bid","2016","were","very","release","pence","jobs","forces","control","billion","terrorists","rohingya","peace","germany","catalan","ruling","rico","muslims","mccain","lies","japan","flag","used","lol","funding","even","doesn\u2019t","chair","amid","story","senators","legal","great","another","week","trying","terrorist","seek","macron","korean","intelligence","destroys","busted","cut","charges","blasts","behind","statement","israel","independence","force","bush","ready","know","chicago","california","source","protests","power","adviser","rep","parliament","msnbc","kremlin","france","cuba","called","bad","ad","3","using","terror","set","march","fraud","cop","bombshell","aide","votes","violent","tries","three","slams","sign","scandal","radical","other","hate","fired","didn\u2019t","agency","\u201cthe","start","sean","rule","real","nfl","kids","clinton\u2019s","children","best","admits","\u201ci","voting","public","nyc","message","hit","arrest","shut","shot","saying","matter","benghazi","any","action","proves","missile","london","lead","iraqi","destroy","cia","calling","too","son","reveals","needs","inauguration","images","hurricane","hold","hack","full","flynn","defends","backs","special","questions","pay","my","let","leave","kills","good","facebook","drops","asked","service","seen","refugee","rape","michelle","likely","internet","hollywood","committee","claim","chinas","awesome","audio","ambassador","yet","working","wife","turkish","trip","thing","question","pressure","ny","much","kurdish","exposes","epic","despite","10","young","victims","rubio","near","making","ivanka","french","cuts","congressman","blames","water","these","spokesman","rant","program","groups","found","brutal","believe","uses","transgender","senior","sarah","repeal","possible","king","joe","girl","final","erdogan","comments","away","1","warren","moore","men","envoy","disgusting","debt","dc","biden","between","street","sex","russias","reports","released","orders","oil","massive","look","little","hits","gave","dem","chris","announces","agree","\u201cwe","wins","whoa","threats","tell","sees","rejects","refuses","mike","jr","jail","history","hilariously","faces","evidence","days","convention","catalonia","battle","barack","assault","al","wikileaks","vows","strike","role","review","pentagon","passes","middle","melania","liberals","kellyanne","issue","immigrants","gay","friday","executive","demands","country","citizens","break","approves","angry","act","workers","venezuela","united","target","summit","stunning","strikes","puts","put","nato","murder","least","issues","illegals","effort","conway","close","activists","abortion","words","where","victory","think","she\u2019s","sessions","schools","running","planned","paris","offers","fed","every","5","they\u2019re","spain","rips","return","private","photo","pelosi","part","migrants","getting","four","flashback","five","epa","energy","doj","denies","boy","announcement","4","warning","university","terrorism","talk","pope","member","lying","join","hell","hearing","hard","guy","corruption","child","cabinet","brilliant","agenda","activist","911","2018","yemen","without","viral","suspected","second","responds","releases","picks","oops","loses","laws","labor","global","find","far","elections","east","destroyed","dems","criminal","alien","accuses","vp","through","thinks","test","something","protect","philippines","michael","killing","hopes","future","fund","finally","fck","elizabeth","cyber","challenge","breaks","ben","thousands","taking","student","star","spending","reelection","perfect","percent","offer","names","mcconnell","intel","immigrant","guest","forced","fighting","fans","exposed","dangerous","crowd","civil","bank","arms","arabia","xi","wont","sunday","stage","spicer","ria","parties","oregon","navy","nancy","nafta","michigan","mattis","manager","letter","lebanon","kelly","female","daughter","cnn\u2019s","church","blame","around","union","trial","staff","since","signs","sht","resign","night","name","millions","members","johnson","hannity","deputy","crooked","car","canada","ask","asia","anchor","yr","wrong","weapons","ukraine","turkeys","troops","transition","system","speak","same","request","politics","lose","frances","defend","council","conference","ceo","bathroom","australia","allow","allies","afghan","waters","try","tried","town","tough","rich","points","mueller","militants","mass","mark","life","insane","highlights","happened","hacking","explains","chinese","chairman","block","better","absolutely","20","west","urge","there","then","stand","sheriff","revealed","prison","prince","ohio","northern","might","megyn","irma","henningsen","guns","funds","foundation","firm","everyone","embarrassing","economy","economic","doing","demand","company","christmas","antifa","allegations","actually","\u2018the","uks","tucker","sue","remarks","prosecutor","picture","pass","palin","pakistan","number","moscow","jeanine","james","israeli","ireland","finance","fear","crime","congressional","confederate","come","cities","christie","care","accused","access","2","worst","update","turn","totally","things","telling","taxes","taiwan","steps","step","stay","soros","red","moves","mom","mexican","me","literally","leaves","friend","food","fires","employees","embassy","education","criticism","crazy","confirms","changes","carson","capital","bundy","boost","bangladesh","anthem","alabama","8","vietnam","videos","syrias","seth","reporters","referendum","palestinian","o\u2019reilly","owner","movie","mocks","meets","lost","lawsuit","launches","important","heads","giving","front","flint","europe","declares","coal","britains","boycott","aliens","15","zimbabwe","women\u2019s","well","unhinged","streets","steve","speaks","sick","shuts","reutersipsos","residents","record","raise","professor","perfectly","mugabe","maxine","market","line","journalist","irish","hurt","finds","financial","event","drug","dad","communist","choice","carolina","carlson","become","baltimore","african","zimbabwes","wearing","vs","voted","victim","until","train","term","study","spy","sea","screenshots","racism","progress","praises","place","parents","officer","nations","journalists","hope","girls","germanys","firing","feds","eyes","due","desperate","comment","comes","christian","bring","book","bans","asking","already","100","you\u2019re","virginia","veteran","trump\u201d","sends","send","save","returns","quit","proof","play","patrick","parenthood","meltdown","iowa","industry","illinois","human","held","hacked","green","experts","ends","drop","dispute","conspiracy","charged","brutally","afghanistan","actor","6","13","worse","soldiers","results","raises","qatar","problem","potential","pastor","obamas","months","moment","missing","looks","leadership","joy","hariri","harassment","furious","freedom","eus","efforts","crackdown","cover","continue","companies","charge","central","board","baby","appeals","amnesty","aides","ads","won","tuesday","today","thursday","threatened","testimony","spanish","soon","sent","rnc","prime","overhaul","opens","news\u2019","migrant","long","leaks","hundreds","hot","here","golf","early","dollars","disaster","detroit","data","coming","check","candidates","borders","billionaire","arrests","answer","america\u2019s","agencies","abuse","50","unreal","turns","treasury","thug","throws","teen","taxpayer","targets","super","suicide","strong","somali","socialist","seeking","sanctuary","san","roy","resigns","relations","reaction","powerful","popular","nothing","mother","love","launch","koreas","kim","joins","jailed","information","husband","hotel","he\u2019ll","game","failed","expects","done","disturbing","community","armed","anyone","agents","\u201cyou","worker","who\u2019s","went","wanted","veterans","undercover","total","took","six","remove","quits","posts","paid","pact","nbc","nation","murdered","maher","libya","iranian","international","injured","india","hysterical","humiliates","ground","gov","gas","facts","elected","egypt","corporate","condemns","class","center","brother","broke","blow","blast","avoid","attacked","appeal","ally","airport","abe","2017","watchdog","warned","w","visits","vice","unity","trumprussia","thugs","thought","tensions","surprise","russians","rightwing","replace","religious","pushes","protester","promises","position","officers","nyt","mays","many","loss","less","leads","kurds","kkk","jeff","isn\u2019t","inside","father","false","ethics","endorses","endorsement","documents","diplomatic","die","december","cites","bus","brazil","biggest","base","asylum","assad","winning","whining","usa","threaten","third","testify","suspect","supporting","station","sets","reach","process","point","players","phone","mosque","month","lied","legislation","leaked","leading","lavrov","hypocrisy","homeland","hispanic","hands","gowdy","gingrich","focus","explain","exit","enough","emergency","draws","detained","dept","concerns","concerned","chuck","bannon","attempt","advisor","accept","12","wisconsin","waiting","visa","vegas","trey","threatening","teacher","talking","stupid","storm","spying","seven","sales","romney","risk","reuters","region","read","ratings","prove","proposes","presidents","plane","once","nra","nomination","netanyahu","mi","memorial","mainstream","longer","kenya","japans","idea","humiliated","hosts","happen","hammers","half","fix","farright","expert","european","electoral","donations","dialogue","deals","credit","controversial","chaos","campus","blocks","blacklivesmatter","bills","april","alert","address","across","\u201cthis","welfare","weighs","wage","taxpayers","suspends","supports","small","shooter","serious","sec","scam","rush","road","ridiculous","removed","relief","regime","reforms","recount","pulls","polls","philippine","person","outrageous","nails","met","lynch","losing","living","lady","kerry","kasich","kansas","jimmy","investment","independent","historic","forward","fan","entire","eastern","drive","defending","dallas","crimes","counsel","continues","cannot","buy","brings","bizarre","berkeley","australian","audience","attend","attacking","apart","amazing","allowed","ago","africas","admit","actions","abc","\u201cracist\u201d","\u2014","you\u2019ll","wednesday","swedish","sweden","strategy","stephen","speaking","snl","silence","showing","scott","safe","rise","reportedly","remain","relationship","reality","propaganda","probes","primary","plot","pledge","pathetic","past","outside","others","nuts","nominate","newspaper","ministry","militant","meddling","marriage","looms","info","hospital","helping","hear","hand","guilty","guess","gorsuch","google","gonna","fit","files","families","falls","fail","eric","duterte","dossier","delivers","decide","countries","cooperation","conflict","committed","commission","commerce","collusion","clintons","clash","citizen","christians","brilliantly","both","body","banned","banks","backing","attempts","aren\u2019t","appears","alleged","agent","30","18","17","11","\u201cnot","\u201cif","word","whether","van","trust","tim","those","tech","taps","susan","sudan","store","starts","stance","someone","shutdown","sell","search","rice","rebels","reasons","radio","policies","owned","options","opposes","movement","monday","mock","missiles","marco","manafort","low","libyan","kushner","jersey","jeb","irans","insurance","influence","indonesian","hunt","hopeful","giuliani","floor","expected","exactly","easy","discussed","dirty","cost","consider","completely","closed","charlottesville","chance","cash","camp","ca","burn","brussels","brags","blocked","aim","agrees","account","\u201ci\u2019m","\u201che","wire","whines","truck","tpp","tower","that\u2019s","suggests","straight","sports","situation","sharpton","sentence","rock","robert","riots","planning","phony","outrage","oklahoma","needed","nearly","nazi","mind","militia","local","leaving","lawyers","las","kid","keeps","judges","japanese","i\u2019m","italy","islamist","islam","irs","including","hypocrite","hateful","graft","god","feel","donors","dies","delay","defeat","deep","david","cuomo","criticizes","courts","course","corrupt","compares","commander","charity","card","cancels","brazils","blaming","accidentally","7","25","\u201cit\u2019s","zika","worked","witch","which","weeks","viewers","view","unlikely","trumpcare","trudeau","treatment","thanks","taken","stealing","shreds","shares","server","sending","schumer","respond","resignation","records","raqqa","protrump","proposal","project","problems","priceless","price","president\u201d","poor","photos","orlando","numbers","nightmare","newt","mnuchin","memo","listen","latino","la","kimmel","interior","infrastructure","indian","huckabee","holds","hezbollah","harvey","handed","gang","games","expose","environmental","domestic","disabled","dark","critical","couple","considering","confident","complete","collapse","camera","building","blacks","beating","amendment","agreement","wealthy","warrant","usbacked","trash","themselves","tests","tears","teachers","targeted","steal","standing","spent","shoot","scalia","resume","replacement","regional","putting","progressive","ordered","neil","mean","maine","lower","killer","joke","it\u201d","incident","hitler","hiding","halt","gone","gold","gains","fuel","flight","firms","fighters","fellow","fcking","famous","failure","everything","endorse","egypts","drone","doctor","diplomats","diplomacy","dhs","deadly","costs","cold","clear","classified","citizenship","chelsea","challenges","canadas","build","bombing","bombers","becoming","becomes","beaten","ballot","baghdad","austrian","arizona","answers","allowing","actress","9","\u2018white","\u2018fake","zone","willing","wh","western","welcomes","warming","vet","va","trolls","toward","toll","stunned","stands","southern","soldier","slam","silent","shouldn\u2019t","sen","saturday","safety","runs","ross","resolve","reject","reid","regulations","refuse","queen","protesting","promote","presses","pledges","plays","playing","pennsylvania","paying","parade","ousted","openly","online","nsa","november","multiple","mn","majority","links","leftists","land","kurdistan","kidding","kenyan","june","jones","increase","illegally","idlib","holding","heart","having","happy","happens","haley","guests","glorious","gift","gaza","funded","friends","form","facing","episode","enforcement","door","direct","dinner","deported","deir","declaration","deadline","cuban","coup","coulter","convicted","colbert","civilians","caucus","career","burns","breitbart","boss","blows","believes","behavior","beautiful","bar","bail","article","argentina","appearance","antigay","among","america\u201d","alzor","aims","accusations","\u2018i","zuma","writer","works","website"],"legendgroup":"","marker":{"color":"#636efa","size":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],"sizemode":"area","sizeref":0.04,"symbol":"circle"},"mode":"markers","name":"","showlegend":false,"x":[0.4183650612831116,0.29851800203323364,0.5046107172966003,-0.18324273824691772,1.1494860649108887,-8.971181869506836,-0.02745526283979416,-0.07946132123470306,1.3217103481292725,1.0283533334732056,2.543292999267578,0.19700653851032257,-1.5211821794509888,-0.5023969411849976,0.6826772689819336,-3.1402297019958496,0.9470446109771729,-3.0561084747314453,-2.1949331760406494,0.13439613580703735,0.0589313805103302,2.1836183071136475,1.8488776683807373,-1.690679907798767,-6.287734031677246,-0.8596047759056091,-2.0986640453338623,-4.740410804748535,0.8581207990646362,0.658229410648346,-0.17503713071346283,0.2781895697116852,-5.389088153839111,0.2693619430065155,0.7675637602806091,-3.649035692214966,1.026154637336731,-0.11674406379461288,0.36694076657295227,-1.1141163110733032,-0.648769199848175,0.6358306407928467,-1.8512449264526367,-2.5966596603393555,-0.32784295082092285,0.21075086295604706,-1.7534958124160767,0.4894549250602722,2.6159801483154297,-0.9007144570350647,-0.27590373158454895,-1.6504230499267578,1.5195786952972412,-2.187680244445801,-1.6615371704101562,-0.29151827096939087,2.5195648670196533,2.0953242778778076,-1.6783703565597534,0.12053296715021133,-4.397099018096924,-1.2398769855499268,-1.6697016954421997,0.44672176241874695,1.1167268753051758,0.20299293100833893,1.2607207298278809,-2.104297161102295,0.8519436717033386,1.0619373321533203,-2.332322597503662,-0.6465045809745789,-0.9857044219970703,-0.35675501823425293,-2.6839523315429688,2.5453879833221436,-3.629906177520752,-9.564024925231934,-0.6856582164764404,1.1708143949508667,10.678877830505371,0.6472796201705933,1.2799863815307617,-0.41368618607521057,-2.8944668769836426,3.1315500736236572,-0.9489842057228088,-4.1657819747924805,3.5356836318969727,0.4547744393348694,0.5832882523536682,-1.1386414766311646,-1.289144515991211,-0.49069511890411377,-2.10461163520813,-0.08347123861312866,1.6358031034469604,2.378885507583618,1.9655333757400513,0.6824579238891602,0.9132249355316162,-1.7782899141311646,3.1029744148254395,4.200555324554443,1.2777798175811768,-2.011836051940918,-0.4085277020931244,-1.6013692617416382,-0.5131795406341553,-0.06118230149149895,-3.6549339294433594,2.403904438018799,0.15288104116916656,-0.356009304523468,0.8583025932312012,-2.993393898010254,2.488490104675293,0.7180407643318176,-2.079761266708374,-0.22043530642986298,1.7734521627426147,0.887597918510437,0.6801325082778931,-1.197637677192688,1.3283746242523193,0.206672802567482,-0.011450007557868958,-0.7013970017433167,-0.29082128405570984,3.8727760314941406,0.6979905962944031,0.252522736787796,0.7665945291519165,-0.09791309386491776,-1.2748652696609497,2.0686304569244385,0.4121811091899872,-2.242004156112671,0.02472902648150921,0.28221362829208374,0.1572800725698471,0.5097598433494568,-2.6881601810455322,-0.16877157986164093,4.23174524307251,1.4738109111785889,-1.06987726688385,-2.6052026748657227,-0.2113208919763565,2.7565114498138428,2.944359064102173,3.259899377822876,-1.49021577835083,-3.759626626968384,-3.3423831462860107,0.8784463405609131,-2.6251678466796875,-0.6785905361175537,-1.8235942125320435,3.6991689205169678,0.6880819201469421,-2.83685040473938,-0.9391372203826904,0.34189164638519287,-1.9920035600662231,0.9145130515098572,0.5122940540313721,0.8430989980697632,-0.36716532707214355,-1.2340881824493408,1.7215988636016846,1.186309814453125,3.746885061264038,-0.1139313280582428,-1.7717533111572266,-3.417728900909424,-0.1897585391998291,-0.4121703803539276,-3.0120432376861572,-0.5433932542800903,-0.5465713739395142,-1.161030650138855,-1.065423846244812,-1.8985562324523926,-1.144208550453186,0.5725096464157104,2.0571131706237793,-1.0564733743667603,-1.8386266231536865,0.7913278937339783,-0.952529788017273,0.906086802482605,-0.657193660736084,-0.07520134001970291,0.04509095475077629,0.6753601431846619,-0.6113885641098022,0.635917603969574,-0.4513170123100281,1.0324459075927734,0.573323667049408,1.856902837753296,-1.272478461265564,0.9211944341659546,3.9482994079589844,-1.534735083580017,3.238712787628174,1.8200830221176147,1.7479161024093628,1.2693853378295898,2.2659120559692383,-1.3057583570480347,2.5657036304473877,2.7187023162841797,-2.3873720169067383,-0.6737300157546997,1.2202459573745728,1.4676179885864258,0.2919096350669861,-0.9611697793006897,0.40098828077316284,-0.2900013327598572,1.3706698417663574,0.22501574456691742,-1.1331697702407837,0.24086882174015045,-0.6477012038230896,1.84601628780365,-0.8735467195510864,-2.317107915878296,0.5655143857002258,-0.08854402601718903,0.18789049983024597,3.0846259593963623,-0.9176558256149292,0.7867192029953003,-0.24856436252593994,0.8225204944610596,-2.2490053176879883,0.17672321200370789,-0.1488860845565796,0.7397990822792053,1.496140956878662,-2.159219741821289,2.3360955715179443,2.0186967849731445,-1.5557292699813843,-0.40047645568847656,3.2899792194366455,-0.08267391473054886,-0.7785001397132874,-1.6802794933319092,-1.7039005756378174,1.0224995613098145,-2.343498706817627,-0.82732093334198,1.6382263898849487,-1.0527633428573608,2.720245599746704,1.58772611618042,0.608264684677124,-0.4938849210739136,1.2486947774887085,-1.100180983543396,0.558804452419281,0.20673510432243347,-1.2485014200210571,-0.4640549421310425,0.9851258993148804,-0.9211581945419312,0.6329993009567261,-1.9822618961334229,-0.0831238180398941,0.23523777723312378,0.7866199612617493,-2.794551134109497,-0.873587429523468,-1.0270839929580688,-0.6978185772895813,-0.39712637662887573,-1.3451645374298096,2.7524592876434326,0.5002276301383972,-0.8111189603805542,0.6723912358283997,-1.0196402072906494,-1.0418404340744019,-0.33841410279273987,-3.1033828258514404,3.8208088874816895,0.9378737211227417,0.19442537426948547,-3.3577826023101807,-0.04408127814531326,0.8801544904708862,0.9634641408920288,-2.738703489303589,-0.7197206616401672,0.866561770439148,1.0630594491958618,1.384697675704956,0.11947538703680038,0.35595422983169556,-2.1163878440856934,1.3767653703689575,0.6866806745529175,-2.5264289379119873,2.7096288204193115,-1.582743525505066,-0.9172229170799255,0.5458317995071411,0.07321389764547348,-0.8650140762329102,0.33377698063850403,-2.230027675628662,-2.6396067142486572,-4.429382801055908,-2.327314853668213,1.3964272737503052,5.953088760375977,6.0802717208862305,0.32823994755744934,1.014195442199707,-1.9899159669876099,-1.5045397281646729,-2.2359771728515625,1.817428469657898,1.2742999792099,-1.0022369623184204,0.9093689918518066,0.9668405055999756,0.2247205376625061,-0.7555093169212341,0.7808207869529724,-3.8887219429016113,-2.0402801036834717,-1.2662549018859863,1.4515197277069092,-2.854597330093384,0.10641153156757355,1.0366328954696655,-2.4252262115478516,1.845445156097412,-2.0346016883850098,0.10822378844022751,-1.1125999689102173,1.1807880401611328,-2.060173988342285,-0.03794443607330322,-1.6456708908081055,-5.402081489562988,1.528772234916687,1.351966381072998,-1.78562331199646,-0.8484992980957031,0.12977029383182526,-1.1797977685928345,-0.39563360810279846,1.513546109199524,-0.040494535118341446,-0.604129433631897,0.8187633752822876,0.2222747802734375,4.521407127380371,1.1696208715438843,-0.9451205730438232,0.27736181020736694,5.477173328399658,-2.015261173248291,-0.0367342047393322,0.2840608060359955,-0.6399312019348145,3.036419630050659,2.633476495742798,1.1057438850402832,1.145875334739685,-0.20376498997211456,0.40127769112586975,0.3724372088909149,-1.179988980293274,1.7248321771621704,0.4088515043258667,-0.14355222880840302,0.3571721315383911,0.32064560055732727,-2.554586410522461,1.6523690223693848,-0.5410061478614807,-1.142411708831787,-0.7420704364776611,-0.6367724537849426,0.5139707326889038,0.5836726427078247,1.6456239223480225,0.38000980019569397,1.0384052991867065,2.0388972759246826,2.554189920425415,-3.318763017654419,2.134995937347412,-1.489422082901001,0.9147873520851135,1.9772003889083862,0.24599964916706085,-0.8201868534088135,-2.3869845867156982,-4.306220531463623,-1.8684743642807007,-0.8118561506271362,1.3744462728500366,1.3976497650146484,0.6772036552429199,-0.9348171353340149,-0.47129759192466736,-1.6603509187698364,-1.714792251586914,0.3427966237068176,0.6709068417549133,-0.28350210189819336,-1.8026849031448364,0.22629961371421814,2.2075085639953613,0.007649676408618689,2.3222005367279053,-0.7198942303657532,3.7389943599700928,1.888318419456482,-0.30068835616111755,1.0093207359313965,0.24312420189380646,1.155396580696106,1.2390066385269165,1.2345356941223145,-2.795208692550659,5.0120038986206055,1.4871549606323242,1.2071934938430786,4.876242160797119,2.513890504837036,1.202675223350525,-0.23423534631729126,-0.3607429563999176,-2.2828006744384766,3.4650189876556396,-3.3154194355010986,-0.08697820454835892,-1.7295639514923096,1.21774160861969,-3.445578098297119,-1.5785104036331177,-0.12017831951379776,2.780388593673706,-2.30418062210083,1.288590908050537,0.37085747718811035,0.5333495736122131,-1.3083370923995972,1.318381428718567,-0.8678075075149536,-2.283038377761841,1.7075368165969849,4.014944553375244,0.15750692784786224,0.0989089384675026,-1.214143991470337,-2.877972364425659,0.7765004634857178,1.119444489479065,-0.19170142710208893,0.10543002188205719,2.4777286052703857,0.34842732548713684,2.6426608562469482,-0.660796046257019,-0.6212548017501831,1.0485433340072632,-1.6797353029251099,-0.5666013956069946,0.9634799957275391,2.193922996520996,1.4064176082611084,0.6771337389945984,1.8609228134155273,-5.022820949554443,3.5798940658569336,-0.7754725813865662,2.1356077194213867,1.9558907747268677,2.7661540508270264,0.4961271286010742,-1.5568299293518066,-1.6471534967422485,-1.5816279649734497,-1.3660650253295898,-3.4350216388702393,1.267930030822754,-0.06084662303328514,-0.7325910925865173,-3.0004284381866455,-3.234706163406372,1.668763518333435,0.8137902617454529,-1.6005165576934814,-0.9984498620033264,0.9235239624977112,-0.880660355091095,0.25975123047828674,0.32626280188560486,-3.0147504806518555,1.153564453125,-2.3370578289031982,0.1198536679148674,-1.4775258302688599,2.739812135696411,-1.4078211784362793,0.5684815049171448,-1.7588942050933838,1.5027426481246948,-1.2669126987457275,-0.8423548936843872,-2.811328649520874,-1.4381023645401,0.9695196151733398,-1.2117408514022827,-1.8585845232009888,-1.1315466165542603,-0.1163799911737442,0.8737204670906067,-3.1925840377807617,-0.27509215474128723,0.17779982089996338,0.3301783502101898,-0.7056891322135925,-0.38857385516166687,1.2371118068695068,-1.7958488464355469,-1.3649685382843018,1.16745126247406,-0.031390801072120667,-2.383044481277466,0.07282069325447083,2.4511518478393555,1.7494834661483765,1.5972809791564941,-2.957505941390991,-2.0513126850128174,-0.9998326301574707,-0.5306520462036133,0.938798725605011,-2.52298903465271,-0.4349929392337799,-0.669069230556488,-1.4405853748321533,2.55934476852417,1.4105242490768433,-0.7214279770851135,-1.3315104246139526,0.1402980387210846,-0.022991614416241646,0.8865883350372314,-0.18092255294322968,1.0049084424972534,-0.3450474143028259,1.0494128465652466,-2.5333940982818604,0.28161919116973877,1.0687522888183594,-0.23725253343582153,-0.382126122713089,-0.6814308166503906,-0.17530225217342377,0.8786219954490662,1.6747150421142578,-0.8411377668380737,-0.7801312208175659,0.5351242423057556,0.3281557559967041,0.6895372271537781,-3.0986669063568115,1.0523267984390259,-1.4803991317749023,3.7489397525787354,-3.1520280838012695,-1.66287362575531,0.08739136159420013,-0.9780811071395874,-0.5962759852409363,-0.006067894399166107,2.947789430618286,-0.11138849705457687,-2.2770042419433594,-0.93412184715271,1.395995020866394,-1.700791835784912,-1.0910310745239258,2.9666049480438232,-2.399057388305664,-2.495990753173828,0.6386787295341492,-0.02554280497133732,0.39706918597221375,-0.03880542516708374,1.2032181024551392,0.5432983636856079,-0.09797177463769913,-1.7616796493530273,1.9894429445266724,-0.011708839796483517,-1.7567665576934814,-0.5139545202255249,-0.3732580244541168,0.06045691296458244,3.909440040588379,-2.6157374382019043,0.4581465423107147,2.3176629543304443,0.23227481544017792,-0.979510486125946,-2.0881261825561523,-1.4467698335647583,0.8454573750495911,0.6323924660682678,-0.348606675863266,2.144932508468628,0.5197967290878296,0.7598727345466614,-2.202022075653076,-0.7590200901031494,1.1010980606079102,1.8166213035583496,0.9016823768615723,-0.8040322661399841,0.16676920652389526,-1.556836485862732,-1.2436681985855103,-0.5738687515258789,2.056225538253784,-2.665207862854004,0.4306521713733673,-3.5462517738342285,-0.3224145174026489,0.4879577159881592,-0.3079594075679779,0.4056299030780792,4.266172409057617,-1.2802238464355469,-0.6098443865776062,0.9735299944877625,2.1617941856384277,-2.4289937019348145,-1.6712816953659058,-0.1600242704153061,-0.14448772370815277,-1.132399082183838,-2.4417459964752197,-1.8103452920913696,-1.8790220022201538,0.3990117907524109,-1.2484745979309082,1.3142645359039307,-1.7350986003875732,-1.4087920188903809,-1.9275293350219727,1.5042457580566406,0.8692516684532166,-1.2231296300888062,-0.10241112858057022,-0.4823482930660248,-0.5529079437255859,-0.6162810325622559,-2.1765503883361816,1.448266625404358,-0.9319292902946472,0.6913676857948303,1.5775169134140015,4.102569103240967,0.5734233260154724,0.9343796968460083,-0.8446626663208008,-1.0101325511932373,-1.7558441162109375,1.549794316291809,0.08006293326616287,1.2313966751098633,1.4410490989685059,0.35002678632736206,0.9736703038215637,-0.7045361995697021,-1.510569453239441,-2.0211892127990723,-2.1370270252227783,0.43283045291900635,-0.1299191266298294,0.8857983350753784,3.6616432666778564,-0.2959538400173187,-1.4769017696380615,0.3365045487880707,-1.7823412418365479,0.28196948766708374,1.0877878665924072,-1.895296335220337,0.035536833107471466,0.15985040366649628,1.9695812463760376,0.7163075804710388,0.17634911835193634,1.707931637763977,-2.785966396331787,1.1584746837615967,0.10382936149835587,-0.8207085132598877,0.09556109458208084,-0.34667715430259705,0.8222861289978027,0.20084893703460693,-3.3451051712036133,-0.717939555644989,-2.741426467895508,1.3631484508514404,0.9796767830848694,-0.08450476080179214,-1.904814600944519,0.2207348793745041,1.6690162420272827,-0.5267871618270874,-1.2624022960662842,-0.41019555926322937,0.32012689113616943,-0.869193434715271,-0.6370154023170471,0.6103264093399048,1.3551244735717773,0.21688058972358704,-1.6220757961273193,-1.1019657850265503,-1.442314624786377,4.192751884460449,-2.09454607963562,1.5542327165603638,0.7664740681648254,-2.9112796783447266,-1.2079237699508667,0.8654620051383972,-0.048749491572380066,-1.3662296533584595,1.5028349161148071,-1.7375521659851074,0.4715552031993866,-0.9579769968986511,0.21961504220962524,-3.1021506786346436,2.650299549102783,-1.7743881940841675,-0.5895958542823792,-1.5585713386535645,-1.3971292972564697,0.9928468465805054,0.6129177808761597,0.1933683156967163,0.8762229681015015,-1.722150444984436,-1.7757163047790527,-0.09761247038841248,-1.1451234817504883,0.18751436471939087,0.21874026954174042,-3.3078033924102783,1.6189347505569458,0.006058080121874809,1.4409446716308594,-2.25960636138916,-0.8446547985076904,0.2698943018913269,-3.2002360820770264,2.998241424560547,1.988218903541565,-0.12638604640960693,-1.2459118366241455,1.8536628484725952,1.5504264831542969,-2.1419260501861572,-0.7633168697357178,0.2406502217054367,-2.1980645656585693,-0.8930724859237671,0.4431346356868744,0.7529476881027222,0.618070662021637,0.23356924951076508,0.3044617772102356,0.054779116064310074,1.2041646242141724,-1.1343722343444824,-1.2331104278564453,-1.2074216604232788,-1.6815024614334106,0.6577000021934509,-0.12432548403739929,-0.11998820304870605,-1.8529984951019287,0.8074962496757507,-1.9186365604400635,0.44265785813331604,1.8748875856399536,-0.8617806434631348,0.661197304725647,1.599241852760315,1.1228703260421753,0.7581806778907776,-2.1780598163604736,-3.1655445098876953,0.7680621147155762,1.662446141242981,1.1342061758041382,-0.06557894498109818,-1.1303540468215942,1.516476035118103,-0.9497874975204468,-0.5572107434272766,-0.23733381927013397,0.6131336092948914,0.4874543249607086,-2.5878281593322754,2.066699266433716,0.7302401065826416,0.9552639722824097,-0.15458717942237854,-1.5352574586868286,-1.0346975326538086,-2.5750114917755127,-1.263952612876892,0.4344070553779602,-0.05684138834476471,-0.9612072110176086,0.12481940537691116,0.40717658400535583,1.0019645690917969,1.449008584022522,-0.9673171639442444,1.2460110187530518,2.9698574542999268,1.2630647420883179,1.6209888458251953,1.0329164266586304,-1.011702060699463,4.439403057098389,2.6816225051879883,-2.1442856788635254,0.05992509797215462,-0.8413665294647217,1.4883393049240112,-1.939879059791565,1.674892544746399,0.25305846333503723,-0.11373696476221085,3.4699347019195557,-0.8023121356964111,-1.3476126194000244,-0.5488791465759277,-1.2471903562545776,0.3795725703239441,-1.3385943174362183,0.9199297428131104,0.9650498032569885,3.0486810207366943,0.7709667682647705,0.8606492280960083,0.9949331879615784,-2.666872262954712,1.721897840499878,-1.3641618490219116,0.23044665157794952,-0.9575107097625732,-0.2705998122692108,0.18995362520217896,-2.7953431606292725,1.8264145851135254,-0.9590179920196533,1.183929681777954,1.053684115409851,0.29049918055534363,1.8651460409164429,-2.0189740657806396,-1.4034477472305298,-1.1326690912246704,-0.40001413226127625,2.6744818687438965,2.126904010772705,0.18037764728069305,1.0805144309997559,0.5653812885284424,-0.6209692358970642,-0.11384199559688568,1.0850176811218262,0.8435935378074646,-0.7865886688232422,1.9313583374023438,-1.4626494646072388,-0.6594851016998291,1.3259327411651611,-0.4554828703403473,-0.13777898252010345,2.311894178390503,-0.3762266933917999,1.0062556266784668,0.8797942996025085,-0.27421700954437256,0.005169107113033533,-0.43534380197525024,0.6238533854484558,1.8607878684997559,-1.143983244895935,0.3540087342262268,-1.1739585399627686,3.6098947525024414,0.16470898687839508,-0.8245758414268494,-0.42661213874816895,-3.130873680114746,2.3896071910858154,-2.516637086868286,-0.23173214495182037,-3.014270305633545,1.6266957521438599,1.5352965593338013,0.7258388996124268,-0.5370315313339233,-1.13382089138031,0.32986995577812195,1.3116260766983032,2.0294899940490723,1.5046181678771973,-0.8755608797073364,-1.0248795747756958,-3.0781197547912598,-2.54689359664917,0.1364087164402008,2.232344388961792,0.2590862810611725,2.61556339263916,-1.3420522212982178,-1.5523850917816162,3.958921194076538,-3.8962149620056152,-0.6300016045570374,1.5922558307647705,0.022457756102085114,0.7791491746902466,-1.9629589319229126,-2.1389267444610596,1.043354868888855,0.534623920917511,-0.477586954832077,-1.4551682472229004,0.000305794645100832,-0.048509396612644196,-3.1816391944885254,1.994789481163025,-3.777552843093872,-0.7158472537994385,2.1252803802490234,-2.6237761974334717,-0.0136762959882617,1.8182299137115479,2.228994607925415,-1.4899864196777344,0.9710792899131775,-1.7065144777297974,2.0895309448242188,-0.8197617530822754,1.8118960857391357,-2.2690913677215576,-0.6477858424186707,0.6519795060157776,3.063906192779541,2.4690656661987305,-0.29360005259513855,0.06938762217760086,1.5142779350280762,-4.040929317474365,-0.727703332901001,1.6326172351837158,0.7694608569145203,-1.2879709005355835,2.031355619430542,0.7654392123222351,-0.8928655385971069,-1.4742275476455688,-1.6396896839141846,-0.3514355421066284,-0.6342124938964844,-1.44558846950531,-1.4525765180587769,0.07940671592950821,1.1038172245025635,1.3994779586791992,1.102320671081543,0.09960385411977768,-1.91715669631958,0.1716180443763733,1.090348482131958,-1.4558645486831665,1.2588005065917969,1.033118486404419,-3.0575971603393555,0.6006025075912476,-1.2703362703323364,0.4017409384250641,-0.3443926274776459,-1.2149890661239624,-0.9151826500892639,0.43974268436431885,2.4048595428466797,-2.305293560028076,-0.31847062706947327,2.1226305961608887,-1.4131050109863281,3.384146213531494,-3.430023193359375,2.888099431991577,2.9754066467285156,0.152053564786911,0.03339580073952675,1.9016815423965454,1.348022699356079,-1.2629773616790771,2.9160451889038086,-2.037627935409546,2.9170022010803223,2.8816843032836914,2.3788819313049316,-0.8046103119850159,-0.6208838820457458,-0.6059916615486145,-1.405976414680481,0.6440070867538452,0.13663193583488464,0.8119975924491882,0.9643896222114563,-0.19124741852283478,0.7633021473884583,-0.9638984203338623,-1.2953780889511108,-0.7955935001373291,0.21015425026416779,-0.3231998383998871,0.20068995654582977,3.2153797149658203,-1.06110680103302,-3.331874370574951,1.0095537900924683,2.3082756996154785,-1.2946850061416626,-0.4445807933807373,-2.363382577896118,0.8986932039260864,-1.5496277809143066,-2.256378650665283,-1.2472327947616577,-1.2710597515106201,2.0774126052856445,1.1746522188186646,0.412171870470047,0.2682420611381531,-0.4907194674015045,-1.637882113456726,4.628770351409912,-2.303279161453247,2.9766170978546143,-0.7040521502494812,0.6809085607528687,2.3674979209899902,0.6729263663291931,-0.2693977952003479,0.6329628825187683,0.8828255534172058,1.5524474382400513,-0.930142343044281,-0.7120800614356995,-0.2801670730113983,-0.03312452882528305,-1.9209752082824707,-0.8855564594268799,-2.200855255126953,2.2679121494293213,2.0323445796966553,-0.22083227336406708,-1.1244720220565796,-1.1639485359191895,-0.6582144498825073,0.9111356735229492,0.494595468044281,0.4835442304611206,-0.04665545001626015,-1.1688660383224487,2.3734045028686523,-1.5101311206817627,-1.9210002422332764,2.379140615463257,-0.40776461362838745,-0.3029565215110779,0.8054237365722656,-1.0705407857894897,0.8862686157226562,0.7596747279167175,1.076817512512207,-0.24546173214912415,2.453908920288086,0.9892554879188538,-1.112740159034729,0.7612931728363037,1.3157565593719482,-1.5055701732635498,3.089390993118286,-2.51924729347229,-1.0233701467514038,-0.5714077353477478,0.22381728887557983,0.3696461617946625,-1.0426084995269775,-1.5200438499450684,0.04253913834691048,-1.124275803565979,-0.18142926692962646,-1.1142387390136719,-0.8709468245506287,-1.4341338872909546,-0.019439280033111572,-0.23555082082748413,0.038459520787000656,1.2109483480453491,-0.44041794538497925,-0.37830227613449097,-2.7631869316101074,-1.4945508241653442,-0.9153366684913635,1.3857961893081665,1.6151740550994873,1.2607320547103882,0.11139708012342453,0.9113350510597229,-0.5839842557907104,-0.29560160636901855,1.6481237411499023,1.486889123916626,0.6223883628845215,2.90486741065979,-2.524749755859375,0.40961551666259766,-1.9432191848754883,1.7870888710021973,-1.1181259155273438,-0.7274506092071533,0.711007833480835,-1.2566306591033936,1.5090560913085938,0.22868919372558594,0.6804986596107483,0.8267645239830017,-0.8837692141532898,1.025863528251648,0.09706683456897736,5.62218713760376,0.8740003705024719,-0.7576531767845154,1.357329249382019,-0.6859749555587769,0.2253485769033432,-0.7852023243904114,3.5861656665802,0.45428335666656494,-1.6938954591751099,0.9405370354652405,1.8389488458633423,0.9499360918998718,3.0482399463653564,-1.0627644062042236,-0.4554777443408966,2.0666518211364746,1.2201132774353027,2.1440958976745605,0.6792512536048889,-0.48561936616897583,0.8901003003120422,-1.0172394514083862,0.8944131135940552,0.823524534702301,-0.8362380862236023,3.883207321166992,-1.6641300916671753,4.715698719024658,-0.8422536849975586,-0.1284434050321579,1.4247161149978638,0.23220768570899963,-0.018582407385110855,-2.2451987266540527,1.2860288619995117,3.187595844268799,0.7046951055526733,-1.0882877111434937,-0.26855140924453735,0.3539011776447296,0.3703513443470001,0.2100403904914856,-1.9580578804016113,-0.702443540096283,-0.7284195423126221,1.4488316774368286,-1.1506961584091187,0.27729332447052,-2.3012142181396484,1.691127061843872,-0.7110428810119629,-0.5684554576873779,0.7936123013496399,-0.9525131583213806,1.262230634689331,0.28259167075157166,-1.6093569993972778,-1.1594233512878418,1.6460418701171875,1.2204234600067139,0.4249038100242615,-1.80002760887146,-2.185676097869873,1.4656922817230225,-1.870235800743103,-1.0021090507507324,-2.0212016105651855,-3.40030837059021,0.3735106289386749,-0.42466193437576294,0.0964028611779213,0.773753821849823,-0.5660907626152039,-1.2622877359390259,1.7681968212127686,0.09912246465682983,0.141057088971138,-0.3621690571308136,1.2220176458358765,0.1511121243238449,-0.37605154514312744,0.6528542637825012,0.1167839989066124,-1.5086277723312378,-0.19595210254192352,-1.245290994644165,-0.6856275200843811,3.098726749420166,-0.3574686646461487,-0.9254810810089111,2.187548875808716,0.44830718636512756,0.40799054503440857,0.026325136423110962,-0.7953366637229919,-0.5790675282478333,-0.17761234939098358,2.0631210803985596,-0.24907095730304718,-2.1141085624694824,-0.5605543851852417,-0.25401201844215393,-3.1672005653381348,-0.17327389121055603,-0.7232702970504761,0.48627492785453796,-0.9808052778244019,-2.1458492279052734,0.09524737298488617,-0.4507628083229065,-2.728868246078491,-0.5889497399330139,-0.7024789452552795,0.9594987630844116,-0.4724806845188141,1.075574517250061,1.0515333414077759,-0.7131105661392212,1.5462864637374878,-0.8661496043205261,-1.7886658906936646,-1.0719140768051147,-2.36444091796875,0.7967338562011719,-0.3268187642097473,0.6344601511955261,1.2947918176651,2.10430645942688,-2.337229013442993,-0.5916625261306763,0.2394445538520813,-3.6358745098114014,1.261244535446167,-0.36393311619758606,-0.31059932708740234,2.5647497177124023,0.6824432611465454,-0.3960200548171997,-0.9244282245635986,0.7422899603843689,0.388023316860199,-0.37599775195121765,-0.7896898984909058,1.5000455379486084,0.6369845867156982,-0.6729076504707336,1.6944841146469116,1.3930199146270752,1.7145330905914307,0.7691293954849243,-2.1866092681884766,1.871594786643982,0.06102798506617546,-0.3999723792076111,1.1619657278060913,2.121922016143799,1.123817801475525,-1.2169431447982788,-2.0102055072784424,-1.0533965826034546,1.5752885341644287,-1.6168458461761475,-1.1661934852600098,-1.5164798498153687,0.9282117486000061,1.5153621435165405,-0.1130218431353569,-1.2820683717727661,0.624308705329895,-0.5023371577262878,-1.422153115272522,-0.4078645408153534,1.8792749643325806,0.7187124490737915,0.19363632798194885,0.17127838730812073,2.0311386585235596,2.058417558670044,-1.356070876121521,-1.4814373254776,-0.9060876965522766,-0.5234792232513428,-0.2869071066379547,-0.6052005887031555,-0.001141621614806354,0.6729288697242737,0.02550264075398445,-0.09910405427217484,1.200405478477478,-0.5291204452514648,-0.756772518157959,0.6552168130874634,0.7689830660820007,2.3020713329315186,0.5999464392662048,-0.173213392496109,0.3606390357017517,-0.5775481462478638,-0.9822860956192017,-1.898452639579773,-2.0267653465270996,-0.8821094036102295,0.3752644658088684,1.127273678779602,0.2792976498603821,-1.398573637008667,1.1523078680038452,0.5070971250534058,1.2437993288040161,0.2551387846469879,-1.2316473722457886,-0.3952263593673706,-0.06646374613046646,0.24671021103858948,2.1335501670837402,-1.6751426458358765,1.6566873788833618,-0.8932170271873474,0.5592601299285889,1.2850401401519775,-2.8894248008728027,-0.07456803321838379,-0.4594009220600128,0.08440758287906647,-1.7373435497283936,-0.5984676480293274,1.367246150970459,-0.682409942150116,0.11177894473075867,-0.7213486433029175,1.4915783405303955,0.9359612464904785,1.807216763496399,-2.1556501388549805,0.6400392055511475,1.4112590551376343,-0.6644511818885803,-0.15240933001041412,-1.2494858503341675,-2.038703680038452,0.9608866572380066,0.5981765985488892,-0.23476621508598328,-1.135386347770691,0.01738562621176243,-0.5101714730262756,-1.3459937572479248,-2.047992467880249,-1.2966800928115845,-0.9155795574188232,-2.9723939895629883,2.221810817718506,1.089859127998352,1.7510932683944702,1.431902289390564,-0.3238973319530487,0.419744074344635,1.849035620689392,2.8973917961120605,-3.6822564601898193,-0.3022429049015045,-1.613540530204773,0.9603878259658813,3.1366283893585205,0.3821171224117279,-0.4320465922355652,-0.8467295169830322,1.7485785484313965,0.40895456075668335,-2.2232091426849365,0.29703018069267273,-2.5783963203430176,-0.632034420967102,2.0855302810668945,2.9183261394500732,-0.9359034299850464,-1.801435947418213,-1.6128578186035156,-0.24151532351970673,-1.9019737243652344,0.35003817081451416,0.583713948726654,1.9452496767044067,-0.30881258845329285,1.7779756784439087,0.6940081715583801,-1.0425113439559937,1.522047996520996,1.1537456512451172,0.15838754177093506,-1.3101427555084229,-1.1431105136871338,-2.163661479949951,1.7030845880508423,-1.5467860698699951,1.5853817462921143,-0.46774184703826904,0.04804491996765137,1.336148977279663,-0.1366179883480072,-0.5115494728088379,-1.8805897235870361,-0.4370533525943756,0.23498675227165222,-2.290214776992798,0.3569481074810028,0.42249998450279236,0.9028319716453552,-2.156108856201172,-0.7414560914039612,2.057229518890381,-1.010084629058838,-0.5299503803253174,0.5211901068687439,-1.8340004682540894,-0.8644839525222778,0.23275262117385864,-2.4780077934265137,2.9870212078094482,-1.6522637605667114,-0.48744896054267883,1.965453863143921,2.070960283279419,-1.0033024549484253,-1.815244436264038,-1.939621925354004,-0.04321777820587158,-0.4266388714313507,-0.2019815444946289,-0.1927391141653061,-0.7326439023017883,-0.319888174533844,1.5664008855819702,0.25013473629951477,-1.1259034872055054,2.0960941314697266,0.5247259140014648,-0.7761879563331604,0.009509576484560966,-0.8196070790290833,-3.3104755878448486,2.617342472076416,0.0951617881655693,-0.743099570274353,-0.9611634612083435,2.018441915512085,0.47325679659843445,-1.5734617710113525,0.5463145971298218,1.2519683837890625,-0.18396565318107605,-2.2219507694244385,-1.103040337562561,1.665992259979248,0.46391865611076355,0.8267782330513,-0.8045491576194763,0.3501894176006317,-1.118568778038025,-0.9374991059303284,0.27839207649230957,1.3891501426696777,-2.1748979091644287,0.04935552552342415,-1.9460415840148926,-0.7309308052062988,1.7402161359786987,-0.6681438088417053,4.94487190246582,-0.48865917325019836,0.6899267435073853,1.125494360923767,-1.300299882888794,0.34585899114608765,-1.1905068159103394,-0.7991983890533447,-0.5067443251609802,-0.6126894950866699,0.490085631608963,1.27228844165802,-4.3752288818359375,1.1321053504943848,0.527539074420929,-0.6177196502685547,-1.4469554424285889,2.2082438468933105,-0.16224230825901031,-1.2185002565383911,0.9510467052459717,-2.2437329292297363,0.15053699910640717,0.5032588839530945,1.4304978847503662,-1.146451711654663,3.0663535594940186,2.4812984466552734,3.160494089126587,2.3682119846343994,1.5770270824432373,1.22266685962677,1.8450284004211426,-2.6903443336486816,1.597821593284607,-0.9872533082962036,-0.9260717630386353,-0.40980076789855957,1.06972074508667,-1.9337595701217651,-0.3246958553791046,-0.2291080206632614,-2.4315054416656494,-1.1259686946868896,-2.437255382537842,1.043349266052246,-0.18581070005893707,0.11510201543569565,0.013614792376756668,0.7644357085227966,-1.8448641300201416,-0.9158040881156921,-0.6983410120010376,0.4579009413719177,1.754209280014038,1.3444534540176392,0.3427616357803345,2.4431312084198,1.2092620134353638,0.5087617635726929,4.9037580490112305,0.6334695816040039,-0.792443573474884,-0.38551414012908936,-1.8249114751815796,1.7583636045455933,1.1916753053665161,0.24224326014518738,1.8253498077392578,0.5766257643699646,-0.35935428738594055,-0.9701933264732361,-2.1537280082702637,-0.3860926330089569,-1.1801562309265137,-0.086403988301754,-0.31377121806144714,-0.7288253307342529,0.8088745474815369,-0.872448205947876,-0.6366519927978516,-1.5094276666641235,2.283052921295166,-2.2141549587249756,0.9710937738418579,-1.3825381994247437,0.984745442867279,1.6531282663345337,0.7799997925758362,-0.24048924446105957,1.4990241527557373,-1.2230641841888428,0.008525330573320389,1.5071004629135132,-1.544676423072815,1.1368356943130493,0.7492567896842957,1.3423668146133423,-1.4372334480285645,0.9410944581031799,-1.2937284708023071,-0.1984507143497467,0.675317645072937,-0.2570488750934601,1.6449333429336548,1.1432217359542847,0.44708600640296936,3.524425983428955,-0.7669671773910522,-1.014299988746643,-0.13726703822612762,0.548498272895813,-0.5000196695327759,1.4557216167449951,0.10091697424650192,0.9089853763580322,-0.22917520999908447,2.9962148666381836,0.622509777545929,-0.13988985121250153,2.904416561126709,-0.814824104309082,2.1371312141418457,-1.607692837715149,-0.646809458732605,1.781886339187622,-2.2401208877563477,-1.2905441522598267,0.7623962759971619,-1.5420331954956055,-0.5460342764854431,1.1503313779830933,-1.3479812145233154,1.5173617601394653,-0.24895155429840088,0.4508865177631378,-1.216330885887146,0.22207961976528168,-2.2551686763763428,0.3124126195907593,1.9874058961868286,-0.863455057144165,0.2681201696395874,0.9424058198928833,0.3879992663860321,0.843933641910553,-0.46787431836128235,-0.6162389516830444,-1.2801563739776611,-0.9851386547088623,0.1379515677690506,0.29217103123664856,1.6744128465652466,-0.7716302871704102,-0.259674072265625,-0.7431042194366455,0.3434334695339203,0.43867966532707214,-1.8444647789001465,1.1306792497634888,-1.249642014503479,-1.1212139129638672,-1.1132869720458984,-0.03066207654774189,-0.8181207776069641,-1.7210181951522827,0.3863857686519623,1.2473018169403076,0.7912580966949463,0.08983267098665237,-1.118414282798767,-0.06924071907997131,1.080345630645752,0.5589858293533325,1.8759725093841553,-0.1757427603006363,-1.7966723442077637,0.2429848611354828,1.6855435371398926,2.904547929763794,-0.5414720773696899,2.332401752471924,0.8481239080429077,0.9511513113975525,-1.8625177145004272,1.3930308818817139,-1.2282764911651611,-1.6044602394104004,3.2271885871887207,-0.538360595703125,0.3642057478427887,0.298794686794281,0.15928442776203156,1.9052891731262207,0.03965850919485092,-0.908126950263977,-1.5192841291427612,-1.6012309789657593,1.6184900999069214,1.3973745107650757,-1.8673200607299805,-1.7761645317077637,-0.6926114559173584,0.9052348136901855,0.7698347568511963,-0.5227702856063843,0.6797769665718079,1.8105179071426392,-1.0135120153427124,-1.1887401342391968,-0.2103261947631836,0.5358054637908936,-0.5176880955696106,1.006279706954956,0.5545579791069031,-0.5576905608177185,-1.5626636743545532,0.8262324333190918,0.08511442691087723,0.9362539052963257,0.36178508400917053,-2.5283868312835693,0.7166692614555359,0.8404289484024048,-2.383939743041992,-0.3680618107318878,-1.6672924757003784,-1.1759575605392456,1.5338773727416992,-0.07578776776790619,0.2828315496444702,-0.056224457919597626,2.1338279247283936,1.9863789081573486,-0.4950292706489563,0.12193137407302856,-1.2641867399215698,0.9383870363235474,0.804170548915863,0.6761559247970581,-1.7707929611206055,1.2708141803741455,-0.605758011341095,0.533728301525116,-0.7237722873687744,0.2890203893184662,-1.2601732015609741,-1.1018402576446533,-1.1677056550979614,1.8536100387573242,0.4781913161277771,-2.9651196002960205,-0.31197649240493774,-0.747904360294342,-0.585823655128479,2.0510599613189697,1.0926817655563354,1.148558259010315,-1.1730941534042358,0.5386525392532349,2.1639068126678467,0.55061274766922,-0.4813799262046814,-0.7909150123596191,1.7386199235916138,-1.239645004272461,0.8838912844657898,-0.10346915572881699,-0.33716318011283875,0.8868200182914734,0.7601044774055481,-0.0395367257297039,-0.13767270743846893,0.5478672981262207,-0.3028523921966553,-0.000877420068718493,-0.332311749458313,1.2758127450942993,-2.0477359294891357,-0.692738950252533,-0.7850354909896851,0.5514727234840393,0.3140853941440582,-0.26042813062667847,1.399361252784729,-0.6034064888954163,-0.837870180606842,1.7624270915985107,0.10015296936035156,-0.020347632467746735,-0.23215945065021515,-2.2384302616119385,-0.7615578770637512,-0.7301240563392639,-0.8081589341163635,-0.5637375116348267,1.667507290840149,0.4356409013271332,1.219862937927246,-1.572012186050415,-1.8547817468643188,0.7716087698936462,-0.7849578857421875,-1.1408580541610718,0.6653455495834351,1.5795044898986816,-0.4159944951534271,-0.6952629089355469,-0.5859707593917847,1.4196102619171143,-0.6882984638214111,-1.833713412284851,1.2658346891403198,-1.5103392601013184,1.1823222637176514,0.6742900609970093,2.1466610431671143,-0.4696691632270813,2.154160499572754,2.149230718612671,0.19196176528930664,-2.064842462539673,-0.9117753505706787,-0.6955183148384094,-1.2699991464614868,-0.4035443961620331,1.5986969470977783,0.6139447093009949,0.07663457840681076,-0.11888337135314941,0.5860708951950073,-1.3273004293441772,1.3806788921356201,0.6490715742111206,0.004665367305278778,0.6098707318305969,-0.6868237853050232,-1.1420875787734985,-0.31441158056259155,1.147268533706665,2.5263636112213135,1.896634817123413,0.19187889993190765,-0.08015713840723038,-1.3263801336288452,0.2129952758550644,-0.3851626515388489,0.8541070818901062,1.0695310831069946,2.1834752559661865,-1.1471123695373535,-0.351616233587265,-0.48141881823539734,-0.24368739128112793,-0.7117186188697815,-0.5557193756103516,-0.422617107629776,0.0773334726691246,0.17964708805084229,-0.45413240790367126,1.072269082069397,0.5769773721694946,-0.3439677357673645,-0.07210596650838852,-2.2257442474365234,-0.76511150598526,1.2352242469787598,2.7915258407592773,-0.49269282817840576,-0.9408899545669556,0.9956713318824768,0.3782479465007782,-0.7280465364456177,0.3125064969062805,-0.8085683584213257,-3.29900860786438,2.0479252338409424,0.8167660236358643,1.3650531768798828,1.218716025352478,1.0062401294708252,1.0548663139343262,-1.5392969846725464,1.300004005432129,-0.5130813121795654,2.7715330123901367,-0.10498251020908356,0.3188772201538086,0.6406598091125488,1.811029076576233,-0.21573187410831451,0.4275389313697815,0.3845161199569702,-1.3281292915344238,-0.5498093366622925,2.793140172958374,-1.1881637573242188,2.3096888065338135,-0.04384982958436012,-0.07222241163253784,-1.684748649597168,-1.6992473602294922,-0.2860780656337738,1.2644997835159302,-1.8657691478729248,0.21710728108882904,2.220925807952881,-2.472778081893921,1.3533073663711548,0.43465879559516907,-0.419683039188385,0.6019997596740723,0.7170354723930359,1.6344648599624634,0.8267592191696167,1.2562823295593262,-0.24910663068294525,-0.010648784227669239,-0.9563882350921631,0.6942301392555237,-0.7465559244155884,-0.593445897102356,-0.13967782258987427,1.7445093393325806,0.18180742859840393,-1.0217771530151367,0.7431365847587585,-0.4793621003627777,-3.2651374340057373,-0.057562585920095444,-0.4604927897453308,0.38890349864959717,-0.8444531559944153,-1.4592856168746948,1.3101496696472168,0.5229049324989319,0.7354791760444641,1.431540846824646,1.8559786081314087,-1.9199467897415161,0.8138923048973083,-0.9177749156951904,1.2042990922927856,-0.13594305515289307,0.31065434217453003,-0.10974377393722534,-0.5357680916786194,0.8167482614517212,-1.3348320722579956,-0.09745151549577713,-0.17519846558570862,-0.7957960367202759,0.7258201837539673,0.5090482831001282,0.2577385902404785,1.6623923778533936,0.001808268018066883,-0.4465697705745697,1.69374680519104,-1.0191097259521484,2.8251161575317383,1.176833987236023,2.4747633934020996,0.053234923630952835,3.290184259414673,0.22629289329051971,1.1836073398590088,0.49149855971336365],"xaxis":"x","y":[0.20622408390045166,0.2868689000606537,0.14075982570648193,0.24643191695213318,0.20669521391391754,0.29349377751350403,0.10669301450252533,0.11636185646057129,0.07290462404489517,0.06935060024261475,0.24696220457553864,0.12699128687381744,0.16785572469234467,0.08248525112867355,0.03894699737429619,0.2192416936159134,0.024612154811620712,0.1686335802078247,0.15006035566329956,0.13810193538665771,0.12213236838579178,0.06504558771848679,0.08039043843746185,0.09424276649951935,0.09861108660697937,0.16771729290485382,-0.04922453686594963,-0.29446038603782654,-0.011873925104737282,0.03885207697749138,0.013354414142668247,0.13313952088356018,0.0826808363199234,-0.09480005502700806,0.15427887439727783,0.026272593066096306,-0.03423118218779564,-0.05063685402274132,0.14159511029720306,0.1547251045703888,-0.17815051972866058,-0.013900714926421642,0.04618886858224869,-0.010354437865316868,-0.0478813536465168,0.020215217024087906,0.030801961198449135,0.24461780488491058,0.12499689310789108,0.04818059131503105,-0.02641010470688343,0.026395244523882866,0.10566002875566483,0.025760972872376442,-0.02375703863799572,0.10208632051944733,-0.2918523848056793,0.07972653955221176,-0.012577950954437256,-0.0445559024810791,0.02587292715907097,0.20060569047927856,-0.06644757837057114,0.011595383286476135,0.15734055638313293,0.07241827994585037,0.17592096328735352,-0.07761023938655853,0.14732922613620758,-0.2917730510234833,0.10676330327987671,-0.34331321716308594,0.00029123565764166415,-0.0876472070813179,0.026200486347079277,0.04283888638019562,-0.06512118130922318,0.15560299158096313,0.09110339730978012,0.12933894991874695,-0.09936267882585526,0.0711798444390297,0.0014827208360657096,0.012817969545722008,-0.0045026130974292755,0.16628538072109222,0.07611145079135895,-0.051469653844833374,-0.004292261321097612,-0.14680425822734833,-0.11553086340427399,0.001740126870572567,0.12589393556118011,0.1030685156583786,0.08745454251766205,-0.21019503474235535,0.12132219225168228,-0.014568542130291462,-0.14304767549037933,0.10949856787919998,0.14043846726417542,-0.17280815541744232,-0.04497707635164261,0.09318963438272476,0.01778634451329708,0.1850021928548813,0.019461985677480698,0.06762037426233292,-0.04153526946902275,0.03335542976856232,-0.009206087328493595,-0.00324817281216383,0.07543522864580154,-0.008241608738899231,-0.12628880143165588,-0.14942707121372223,0.02822752483189106,0.1591772884130478,0.12837128341197968,0.12625743448734283,0.10713831335306168,0.08863070607185364,-0.2090262472629547,0.08932332694530487,0.03775104135274887,-0.057744670659303665,-0.060022175312042236,-0.07891758531332016,0.03678055480122566,0.10283726453781128,-0.16207757592201233,-0.012736487202346325,0.004119577817618847,-0.023510746657848358,-0.010864561423659325,-0.08571112900972366,-0.06135225296020508,0.03311355039477348,-0.0761178508400917,-0.18249838054180145,0.07316775619983673,0.034249257296323776,0.015956511721014977,0.06338381767272949,0.12008166313171387,0.1927742063999176,-0.14476633071899414,-0.006916757207363844,-0.12353432923555374,0.1589256078004837,0.10222474485635757,-0.09583042562007904,0.09416437894105911,0.06926025450229645,0.12660852074623108,0.03478092700242996,-0.17274782061576843,0.06134286895394325,0.032893527299165726,0.02347414754331112,0.0432068333029747,-0.08626005798578262,0.037727005779743195,0.03970690444111824,-0.003924389835447073,0.036095600575208664,0.12565280497074127,0.08456509560346603,-0.04799572750926018,0.08967132866382599,-0.07923642545938492,0.023491205647587776,0.051257599145174026,-0.07326807826757431,-0.0007448498508892953,0.13951651751995087,0.00683936569839716,-0.0891910120844841,-0.07147789746522903,0.04801715537905693,-0.1763850450515747,-0.30459246039390564,0.0799819752573967,-0.012226308695971966,0.05901764705777168,0.08393187075853348,0.06011814996600151,0.17707088589668274,0.22266186773777008,0.1325940191745758,0.04861915484070778,0.16691836714744568,0.1086290255188942,0.0370725616812706,-0.16750361025333405,0.04183788597583771,0.07216539978981018,-0.15587961673736572,0.001027161837555468,-0.002655188087373972,-0.1150706484913826,0.20032981038093567,-0.08461397886276245,0.10198502242565155,-0.17074796557426453,-0.15494117140769958,0.34933918714523315,0.20830988883972168,0.18522468209266663,0.12082308530807495,0.15765857696533203,-0.18749253451824188,-0.09974315017461777,-0.12962108850479126,-0.01671641506254673,0.41785532236099243,-0.20362001657485962,-0.10783498734235764,0.01860775798559189,0.1055934801697731,0.09361729770898819,-0.05267275497317314,-0.10663070529699326,0.03064444661140442,0.009047256782650948,0.08254736661911011,-0.13834220170974731,0.06273908168077469,-0.0870092585682869,-0.011953284963965416,0.17673945426940918,-0.20029376447200775,-0.26964622735977173,-0.14894337952136993,0.04171575605869293,0.036530397832393646,0.09288420528173447,-0.00892004556953907,0.13167309761047363,0.13780829310417175,-0.00788060575723648,0.02332628332078457,0.08928141742944717,0.15285459160804749,-0.1401408165693283,-0.15303952991962433,0.0524936281144619,-0.009772203862667084,-0.2729618549346924,0.15805545449256897,0.21232211589813232,0.061331864446401596,0.09554369002580643,-0.04986825957894325,-0.012541348114609718,0.01637958362698555,0.04461323097348213,0.1488998532295227,-0.11561269313097,0.008537773042917252,-0.008699470199644566,0.1259344220161438,-0.13699430227279663,-0.08376340568065643,-0.03917602822184563,0.07369494438171387,-0.07975702732801437,-0.062256939709186554,-0.03709445148706436,0.027535947039723396,-0.1083359494805336,-0.06171812862157822,-0.11206267774105072,0.06872962415218353,0.21652339398860931,-0.04841650649905205,0.181617870926857,0.2595672309398651,-0.040892988443374634,0.08143570274114609,0.05388543754816055,0.12090003490447998,0.12945882976055145,0.02225022204220295,0.19661845266819,-0.12253088504076004,0.010350273922085762,-0.1436028629541397,0.07119149714708328,-0.10135271400213242,0.14313904941082,0.15263192355632782,0.111117422580719,0.018072158098220825,-0.13418777287006378,0.013053839094936848,0.05709943547844887,0.015519589185714722,-0.02634645253419876,-0.1076636016368866,0.13326571881771088,0.02325308881700039,-0.04703357443213463,0.03781432285904884,0.14809945225715637,-0.07503677159547806,0.049939580261707306,-0.08008237928152084,0.019794318825006485,0.06112903729081154,0.18596495687961578,0.08138366788625717,-0.020440632477402687,0.040827762335538864,-0.04678250476717949,0.024396231397986412,-0.08994589745998383,-0.11676643043756485,-0.024853819981217384,-0.2905811369419098,-0.20682252943515778,-0.14085502922534943,0.049449462443590164,-0.07754803448915482,0.06679249554872513,-0.03448313847184181,-0.002978566102683544,0.04538235813379288,0.05454174429178238,0.14386044442653656,0.1547027975320816,0.15172339975833893,0.1864132285118103,-0.14098294079303741,-0.04768501594662666,0.11694056540727615,0.17199011147022247,-0.19528287649154663,-0.11178162693977356,-0.06080465763807297,-0.04693190008401871,-0.13403505086898804,-0.11117514222860336,0.17872215807437897,0.08527997881174088,0.026478754356503487,0.08039313554763794,0.051969025284051895,-0.01869971677660942,0.08867773413658142,0.09508288651704788,-0.08000557869672775,0.06444033980369568,-0.10484375804662704,-0.03620082512497902,0.004824424162507057,0.0399499237537384,-0.08345698565244675,-0.050561610609292984,-0.16576695442199707,0.13912193477153778,-0.09751427173614502,0.20386287569999695,0.087851382791996,0.14910288155078888,0.034556180238723755,-0.047404780983924866,0.6336709856987,-0.07929331809282303,-0.1356734037399292,0.03280211612582207,0.06419819593429565,-0.12202447652816772,0.17961552739143372,0.07379722595214844,0.13375841081142426,-0.103962242603302,0.07021534442901611,-0.08449900150299072,-0.06393048912286758,0.04374022036790848,-0.0959974154829979,-0.07932718843221664,-0.05300521478056908,0.029605424031615257,-0.026823366060853004,0.19076593220233917,0.04462835192680359,0.016543565317988396,0.0005286929081194103,-0.20095720887184143,0.12259258329868317,0.04107023403048515,-0.015694420784711838,0.1268211305141449,0.11570673435926437,0.14493773877620697,-0.09468728303909302,0.02108282409608364,0.01093929074704647,0.04699939489364624,-0.076822429895401,0.05318918079137802,-0.08052462339401245,-0.03559760004281998,0.06945205479860306,-0.15798451006412506,-0.044154390692710876,-0.1191331297159195,0.06022587791085243,-0.0004439740732777864,0.03709602728486061,-0.2254495769739151,0.02567662112414837,-0.004046319052577019,-0.09177841246128082,-0.030020276084542274,0.08222056180238724,-0.05126766487956047,0.07821796834468842,-0.18338246643543243,0.14009597897529602,0.1524803787469864,0.1656961888074875,-0.12740018963813782,0.31113627552986145,0.04380681365728378,-0.07150673121213913,0.0329730287194252,-0.09350193291902542,0.03548585996031761,-0.11161000281572342,-0.07512756437063217,-0.13846954703330994,-0.28487181663513184,0.08069530129432678,0.009932206012308598,-0.4337339401245117,0.1195308193564415,-0.2833714783191681,0.009186692535877228,0.034301403909921646,-0.13224683701992035,0.2491423785686493,0.0055093360133469105,-0.06660084426403046,0.060106310993433,0.005364781245589256,-0.17259560525417328,-0.02440018393099308,-0.18250508606433868,0.10570738464593887,-0.14877378940582275,-0.1429705172777176,0.09335823357105255,0.2551135718822479,0.021797195076942444,0.1517530232667923,-0.11584223806858063,-0.18605487048625946,0.05795277655124664,-0.015887204557657242,0.2594226896762848,-0.08724822103977203,-0.0028433247935026884,0.003019009018316865,0.01806536130607128,-0.1939503401517868,0.021806975826621056,-0.00029960618121549487,-0.00012285741104278713,-0.04376480355858803,-0.07399643957614899,-0.18160788714885712,-0.018898269161581993,0.04481613263487816,0.046048372983932495,0.10096688568592072,0.1467745453119278,0.14463849365711212,-0.011362574994564056,-0.18575413525104523,-0.09166838228702545,-0.5215396285057068,-0.015544847585260868,-0.03632398322224617,0.13143761456012726,-0.04329802840948105,0.13269880414009094,0.10469704866409302,0.1834997981786728,0.1381095051765442,0.08396592736244202,-0.13392409682273865,-0.010759573429822922,-0.20952996611595154,-0.05178540572524071,-0.176870658993721,-0.023921426385641098,0.011258415877819061,-0.03426799923181534,-0.14683334529399872,-0.022660067304968834,0.020730718970298767,0.1334673911333084,0.04763302579522133,0.07608652859926224,-0.004954391624778509,0.10828671604394913,-0.1346929371356964,0.1011708676815033,-0.06677815318107605,0.046315111219882965,0.26057323813438416,0.09529951214790344,0.08820141851902008,-0.034332793205976486,0.10364151746034622,-0.10110510140657425,0.053636714816093445,0.19953098893165588,-0.00504643376916647,-0.020158633589744568,0.11055020987987518,-0.11022836714982986,-0.01968054100871086,-0.027705760672688484,-0.22563092410564423,-0.02896132692694664,-0.021283447742462158,0.09686427563428879,-0.09757453203201294,-0.10868870466947556,-0.17083190381526947,0.020520420745015144,-0.021353883668780327,-0.15038996934890747,0.07754018157720566,0.20739474892616272,-0.16728544235229492,0.2529546916484833,0.05790802463889122,0.24362961947917938,-0.11586283892393112,0.032106079161167145,0.06646867096424103,-0.07881809026002884,0.08014333993196487,0.1534067690372467,0.006197270005941391,0.027050169184803963,0.1893773227930069,0.044710371643304825,-0.07030806690454483,0.13437587022781372,-0.0023908026050776243,0.14760972559452057,-0.10124702751636505,-0.017994284629821777,-0.14945897459983826,-0.16618169844150543,0.01960025727748871,-0.20411673188209534,-0.09202782064676285,-0.07576857507228851,0.04479337856173515,0.04988822713494301,0.07480305433273315,-0.12411639094352722,0.10098385810852051,0.11483855545520782,0.12077570706605911,0.19548775255680084,0.1618320792913437,-0.09101932495832443,0.04890473186969757,0.05408056452870369,-0.1653238832950592,0.0520305372774601,-0.03318559750914574,-0.06314083188772202,-0.21352693438529968,0.02266787365078926,0.18318359553813934,-0.004648705944418907,-0.058387961238622665,-0.08195760101079941,-0.01942213997244835,-0.2442723512649536,0.07176396250724792,0.10913119465112686,0.0877782329916954,0.09614983201026917,-0.001580768614076078,-0.04015535116195679,-0.46040067076683044,0.13213849067687988,0.03745271638035774,-0.1590888649225235,-0.030415939167141914,0.14710846543312073,-0.023360000923275948,0.25239208340644836,0.022188320755958557,-0.032608192414045334,0.19362454116344452,-0.09838233143091202,0.043959397822618484,-0.07786878198385239,-0.2511369287967682,-0.004288854077458382,-0.12040522694587708,-0.042073655873537064,-0.09193473309278488,-0.12575721740722656,0.1099795326590538,0.055390797555446625,-0.04146197810769081,-0.003213613061234355,-0.04721270129084587,-0.035735685378313065,0.04212728515267372,-0.06945884972810745,-0.12761595845222473,-0.161225363612175,0.0978606715798378,0.012156728655099869,-0.009528509341180325,0.11037398874759674,-0.33405792713165283,0.05615132302045822,-0.03171828016638756,-0.23865386843681335,-0.21232132613658905,0.009702544659376144,0.10411296784877777,-0.049813687801361084,0.1762062907218933,-0.42516905069351196,-0.1286817491054535,-0.039552975445985794,0.006227289326488972,-0.11865366995334625,0.26261091232299805,-0.1468980759382248,-0.11791332066059113,-0.013973131775856018,0.0011063342681154609,-0.17355592548847198,0.03747173398733139,-0.003207421861588955,-0.07352234423160553,0.10260723531246185,0.027473241090774536,0.0474039651453495,-0.053571056574583054,0.03290477395057678,0.028316054493188858,0.03245559334754944,0.12267116457223892,-0.04218314588069916,-0.018961668014526367,-0.17021474242210388,0.15724977850914001,0.08546245843172073,-0.02309972420334816,-0.012547376565635204,-0.0806785449385643,-0.042425643652677536,0.10887754708528519,0.11930253356695175,0.07038988918066025,-0.07852296531200409,0.06924312561750412,0.010858068242669106,-0.022287024185061455,-0.14476656913757324,0.18825583159923553,-0.12324906885623932,-0.08898361772298813,0.06538309901952744,0.07906720787286758,-0.03955831751227379,0.07971265912055969,0.043134137988090515,-0.3285083472728729,0.02506873570382595,-0.17293839156627655,0.18988493084907532,-0.04582590237259865,0.003258976386860013,-0.22584889829158783,0.1684144139289856,0.05255252495408058,0.13448204100131989,-0.03578130900859833,-0.10113464295864105,0.20489461719989777,-0.0934801772236824,0.0032443797681480646,-0.029161371290683746,0.055337224155664444,-0.040162768214941025,0.07429900765419006,-0.25436997413635254,0.036949750036001205,-0.06990152597427368,0.08741971105337143,0.12119878828525543,0.15504999458789825,0.09943265467882156,0.1981901079416275,-0.1689765453338623,-0.18042755126953125,0.0043627251870930195,0.05220707878470421,-0.0071584321558475494,-0.06161274015903473,-0.17847184836864471,0.004651114344596863,-0.16798800230026245,-0.01389780081808567,0.2235804796218872,-0.008708733133971691,0.27937963604927063,0.041689157485961914,0.08544516563415527,-0.02482609450817108,-0.06391403079032898,-0.039487071335315704,-0.007059883326292038,-0.053160410374403,0.051357533782720566,-0.2301119714975357,0.05253763124346733,-0.1723756641149521,0.06638593971729279,-0.1956014484167099,-0.05318760126829147,-0.010148636065423489,0.12001288682222366,-0.058164581656455994,-0.1836157590150833,-0.04310404881834984,0.07625266164541245,-0.050903480499982834,0.11487185209989548,-0.08457855880260468,0.07525674253702164,-0.1363096982240677,-0.1847960501909256,-0.2510044574737549,-0.03332969546318054,0.03852725028991699,-0.05883137881755829,0.03298364579677582,-0.012107929214835167,-0.09848862886428833,-0.16021285951137543,0.16788671910762787,0.3467557728290558,-0.005927594844251871,-0.02391335554420948,0.04861855506896973,0.03990546241402626,0.06758212298154831,0.15513914823532104,-0.056814223527908325,-0.10681062191724777,-0.05234701558947563,-0.013875948265194893,-0.09816057235002518,-0.1867005079984665,-0.02733731083571911,-0.043939486145973206,-0.1193467304110527,-0.09630068391561508,-0.006252449005842209,-0.04954333230853081,0.03442275896668434,0.1369905024766922,0.04701720178127289,0.0028483376372605562,0.007093413732945919,0.00021100328012835234,0.005112658254802227,-0.0021528296638280153,-0.09033367782831192,-0.11058227717876434,0.15197864174842834,-0.08673460781574249,-0.03681381419301033,-0.09814164787530899,-0.010191594250500202,0.07537181675434113,-0.03310992568731308,-0.10533953458070755,0.13315321505069733,0.018762459978461266,-0.08940553665161133,0.021647609770298004,0.03431818634271622,0.15921437740325928,0.014609294012188911,-0.11449427902698517,0.1008254811167717,-0.17683319747447968,0.0802057534456253,0.04736272245645523,-0.07998695969581604,-0.04770032316446304,0.10155366361141205,-0.09933249652385712,-0.11278298497200012,0.0715811476111412,0.03181944414973259,0.009930034168064594,0.09790806472301483,0.09827349334955215,0.014562922529876232,0.09624452888965607,-0.2544708847999573,-0.06462720781564713,0.036677781492471695,0.05248551070690155,0.08409576117992401,-0.06047425791621208,-0.15769162774085999,0.10977926105260849,0.03156685456633568,0.11417479068040848,0.018065188080072403,0.05454977974295616,0.16039873659610748,0.041844047605991364,-0.00943152979016304,0.13098719716072083,-0.17156240344047546,-0.09318631887435913,-0.17688657343387604,-0.10039909929037094,0.11720877140760422,0.12938927114009857,0.04156160727143288,0.09817038476467133,-0.12620927393436432,-0.08189722150564194,0.1212182268500328,-0.2540704309940338,-0.0398537740111351,-0.13545601069927216,-0.04380743205547333,-0.10132990032434464,0.021351531147956848,-0.035322416573762894,-0.22971436381340027,0.044011980295181274,0.15054644644260406,0.20418190956115723,-0.10444589704275131,0.1369553506374359,0.06190045177936554,0.03966531157493591,0.00960401352494955,-0.07068539410829544,0.03607058897614479,0.06153111532330513,-0.022017383947968483,0.18121643364429474,-0.01999116688966751,-0.01735677383840084,0.2611716091632843,0.03921539708971977,-0.15634548664093018,-0.2000880241394043,0.17963163554668427,-0.14540298283100128,0.03485190123319626,0.04272952303290367,0.08482494205236435,0.06742468476295471,0.07470406591892242,-0.0035807897802442312,0.02864936552941799,0.08268871903419495,-0.06341663002967834,0.014693872071802616,-0.17048999667167664,-0.019825531169772148,-0.0216992124915123,0.03681444376707077,0.030729351565241814,0.1704690307378769,-0.05017948895692825,-0.011252743192017078,0.2114325612783432,-0.09926657378673553,0.09455849230289459,0.1199520006775856,-0.24947993457317352,0.15014421939849854,-0.2364450842142105,-0.04235609620809555,0.0951765850186348,-0.10665184259414673,-0.21267269551753998,-0.26137569546699524,0.1940661519765854,-0.06941041350364685,0.09924878180027008,0.14291632175445557,0.012684025801718235,-0.01517255324870348,0.03603874519467354,-0.043620940297842026,0.09824611991643906,-0.10163722187280655,0.033235952258110046,0.14981059730052948,-0.037480708211660385,0.08877350389957428,0.024225225672125816,0.026745323091745377,-0.08890774846076965,0.06569132208824158,-0.11015624552965164,0.038023583590984344,0.07705255597829819,0.08211762458086014,-0.10914062708616257,0.20088954269886017,0.1293821483850479,0.17313410341739655,0.06607510149478912,-0.05852806195616722,0.1758652776479721,0.10436476022005081,0.007116106338799,0.2557401657104492,0.12706807255744934,-0.019446929916739464,-0.09527672827243805,0.02091006003320217,0.06920450925827026,-0.062417369335889816,-0.09841544181108475,-0.2474992573261261,-0.03580288589000702,-0.13842353224754333,-0.1091265082359314,0.0715750902891159,-0.13488346338272095,0.007901453413069248,-0.03850226849317551,-0.15186387300491333,-0.19731323421001434,0.003941094037145376,0.10153861343860626,-0.11561164259910583,-0.45306238532066345,0.008520709350705147,-0.04463425278663635,0.09731819480657578,0.02895519882440567,0.010222974233329296,-0.17211687564849854,0.1588384509086609,0.15833072364330292,-0.13422201573848724,-0.08540312200784683,-0.060693129897117615,0.053726404905319214,0.06894940137863159,-0.19433985650539398,-0.14056219160556793,0.09365910291671753,-0.05965862795710564,0.19223271310329437,-0.22205698490142822,-0.038962222635746,0.10580740123987198,0.16341300308704376,-0.07606790214776993,0.22235313057899475,-0.29196056723594666,0.18201464414596558,-0.060170821845531464,-0.2057800590991974,0.03086451254785061,-0.09345890581607819,0.007163787726312876,0.01727394387125969,-0.06702427566051483,0.17925582826137543,-0.10033159703016281,-0.04309548810124397,0.06569718569517136,-0.11559724062681198,0.2105574756860733,-0.20761838555335999,0.20123441517353058,-0.26332423090934753,0.13270151615142822,0.11764627695083618,0.17829683423042297,-0.2373628169298172,-0.08747292309999466,0.019637927412986755,-0.24607142806053162,-0.11386595666408539,-0.2285948544740677,-0.06967576593160629,0.04759541526436806,0.19001668691635132,0.31466275453567505,-0.0034976887982338667,-0.04405539110302925,-0.12904110550880432,0.09389962255954742,-0.21835318207740784,-0.2075716108083725,0.04391857236623764,0.16934768855571747,0.08103352785110474,-0.00852155964821577,0.08058314770460129,-0.01645684614777565,-0.09529927372932434,0.010160557925701141,0.148700550198555,0.023850567638874054,-0.21789659559726715,0.018088772892951965,0.03720172494649887,0.04492246359586716,0.04297802969813347,-0.01471399050205946,0.09741874039173126,-0.17766432464122772,-0.1072162613272667,0.002118571661412716,-0.013489285483956337,-0.026885922998189926,0.04866044223308563,-0.08439331501722336,-0.07030270993709564,-0.05969780310988426,-0.18553026020526886,0.009731228463351727,0.0994434729218483,0.3829531967639923,-0.039366286247968674,-0.03416762501001358,0.058793842792510986,0.08095292001962662,0.1735028624534607,0.013466522097587585,-0.17018519341945648,0.13462288677692413,-0.008622310124337673,0.09912286698818207,0.16157622635364532,0.1665513813495636,0.16896604001522064,-0.19679710268974304,-0.06460952013731003,-0.059761930257081985,-0.08437106013298035,-0.031095396727323532,-0.03250234201550484,0.060479775071144104,-0.036195479333400726,0.20832489430904388,-0.0890655443072319,-0.07416090369224548,-0.2258700430393219,-0.0023460066877305508,-0.23616081476211548,-0.0550653450191021,0.14540107548236847,0.08696575462818146,-0.0060701919719576836,0.040916938334703445,0.049405287951231,-0.14406470954418182,-0.16666215658187866,0.18598005175590515,-0.02343728393316269,0.02896408550441265,-0.048543237149715424,0.08053071796894073,0.04802791774272919,0.14901922643184662,-0.09429490566253662,-0.016047168523073196,-0.0531822070479393,-0.1841016709804535,0.21357937157154083,0.05081430450081825,-0.06524419039487839,0.054268162697553635,0.07653716206550598,0.05066470429301262,-0.06990683823823929,0.1965898871421814,-0.034954484552145004,0.0989985391497612,-0.14731824398040771,0.0187501423060894,-0.12646633386611938,-0.07493887841701508,0.11344529688358307,0.06614278256893158,0.036874692887067795,-0.21433450281620026,0.007502620108425617,-0.10917975753545761,0.059220071882009506,0.050802432000637054,-0.05897239223122597,0.05696455389261246,-0.03515264764428139,0.07979659736156464,-0.020990785211324692,-0.06487563252449036,0.012219054624438286,0.015901142731308937,-0.05280105397105217,0.09949854016304016,-0.08657074719667435,-0.004272754304111004,0.022889915853738785,0.09511768072843552,-0.05784355103969574,-0.16538792848587036,-0.2155919075012207,0.013212698511779308,0.16354233026504517,-0.2911086082458496,-0.08769320696592331,-0.09751297533512115,-0.05931663513183594,-0.0773795023560524,-0.01094026118516922,-0.010272834450006485,-0.03081456758081913,0.056474532932043076,0.12604235112667084,-0.003549525747075677,-0.00983255635946989,0.036740295588970184,-0.15222187340259552,0.11363431066274643,0.013431436382234097,0.08283523470163345,0.07304880023002625,-0.20355874300003052,0.09805745631456375,-0.20195621252059937,-0.02713058516383171,-0.11728163063526154,-0.2022130787372589,-0.06151966005563736,0.12129252403974533,-0.0017695857677608728,0.22971691191196442,0.17377352714538574,-0.07264239341020584,0.07556567341089249,-0.0075509194284677505,-0.22949591279029846,-0.1076090857386589,0.024010326713323593,-0.14871492981910706,0.07411054521799088,-0.07852397114038467,-0.17796115577220917,0.02804216742515564,0.04443151503801346,0.05130383372306824,-0.05934678763151169,0.019414493814110756,0.043895162642002106,0.09576897323131561,-0.10704745352268219,-0.22406846284866333,0.23643279075622559,-0.057852014899253845,-0.06002616882324219,0.28528645634651184,0.11689192056655884,0.1173662394285202,-0.07114915549755096,-0.13534817099571228,0.06250603497028351,-0.008328447118401527,-0.11933199316263199,-0.15321603417396545,0.09477655589580536,0.040078938007354736,0.023872751742601395,0.07345333695411682,0.013027466833591461,-0.10836011916399002,-0.13136549293994904,-0.10439396649599075,-0.01422432716935873,0.05141357704997063,0.053052183240652084,0.051943350583314896,0.02260476164519787,0.03273392096161842,-0.014969784766435623,-0.12675893306732178,0.16367794573307037,0.0171855129301548,0.05023762211203575,-0.015927160158753395,0.04953185096383095,-0.026525113731622696,0.0865451842546463,0.0079411081969738,0.2243531495332718,0.07345191389322281,-0.06582694500684738,0.09298320859670639,-0.07535596191883087,-0.19072550535202026,0.10421007871627808,-0.1442343145608902,0.07404452562332153,-0.06703007221221924,-0.005120757967233658,0.10916440933942795,-0.07430825382471085,0.17996738851070404,-0.14954622089862823,0.11330796778202057,0.02257109060883522,0.00983354914933443,0.11591420322656631,-0.2285361886024475,-0.1194859966635704,-0.06656478345394135,0.17994610965251923,0.0989166647195816,0.03229420632123947,0.011207914911210537,-0.006065713241696358,0.24899020791053772,0.025515321642160416,-0.002976577263325453,0.17764627933502197,0.14168894290924072,-0.08810257166624069,-0.009881707839667797,0.10152662545442581,-0.031427234411239624,-0.0072007919661700726,0.19146832823753357,-0.025366462767124176,0.03368861973285675,0.09776481986045837,0.0732804462313652,-0.138719379901886,-0.011627071537077427,0.027568668127059937,0.12537147104740143,0.05405734106898308,0.02646414004266262,0.01663830503821373,-0.14639650285243988,0.057363394647836685,0.11394274979829788,-0.06393417716026306,0.009241286665201187,0.12485582381486893,0.08475367724895477,-0.025892557576298714,-0.05676911026239395,-0.022628070786595345,-0.08502069115638733,0.05760754644870758,-0.05923338979482651,-0.15355269610881805,-0.15010708570480347,0.055579882115125656,-0.020089784637093544,-0.2951909899711609,0.11980283260345459,-0.05932474881410599,-0.23414190113544464,0.020202601328492165,-0.03838563337922096,0.057242501527071,-0.042946018278598785,-0.035387419164180756,0.07415971904993057,0.1561252474784851,-0.10793503373861313,0.10740169882774353,0.13491474092006683,-0.039586517959833145,0.01484491303563118,0.08186187595129013,0.13013271987438202,-0.004423949401825666,0.012170382775366306,-0.2427181899547577,-0.036827411502599716,-0.06003261357545853,0.02084004506468773,-0.15773813426494598,0.035402812063694,0.06265281140804291,-0.11180274933576584,-0.022602764889597893,-0.027333997189998627,0.0107250502333045,0.05251557379961014,0.14310501515865326,-0.0521131306886673,0.03650125116109848,0.07391037046909332,0.0727497935295105,-0.16619043052196503,-0.07481102645397186,-0.05468529835343361,-0.02432010881602764,0.032440368086099625,0.006488300394266844,0.013417544774711132,-0.08423268049955368,-0.15618745982646942,-0.15184690058231354,0.036674514412879944,0.016843320801854134,0.10166027396917343,0.016183555126190186,-0.05661050230264664,0.0929606631398201,-0.23241694271564484,-0.05517444387078285,0.03415468707680702,0.08898761868476868,0.07287237793207169,0.07481985539197922,0.039778031408786774,-0.10038255900144577,-0.02634790539741516,-0.12115227431058884,-0.10524877905845642,-0.08736149221658707,0.08824845403432846,-0.13714124262332916,-0.08857209980487823,0.10507689416408539,-0.11321479827165604,-0.007305588107556105,0.07578550279140472,-0.14009732007980347,0.0629081130027771,0.11557146161794662,0.06390996277332306,-0.18187446892261505,-0.1021239385008812,0.14650550484657288,-0.15830950438976288,-0.15750519931316376,0.126511350274086,-0.011608188971877098,-0.179082989692688,0.15956762433052063,-0.051607951521873474,0.1366787701845169,-0.0056149959564208984,0.04712631180882454,-0.04546766355633736,0.0011294608702883124,-0.042839352041482925,0.042464494705200195,0.011266916990280151,0.10736031830310822,0.20892982184886932,-0.05884094908833504,0.11623598635196686,0.03204444423317909,0.0012577042216435075,0.0008840282680466771,-0.18866562843322754,0.06616632640361786,0.028150929138064384,0.023365138098597527,0.056088581681251526,-0.0928570106625557,0.029600556939840317,0.17955747246742249,0.07150941342115402,-0.014206916093826294,0.16066551208496094,-0.07908038794994354,0.04253070801496506,-0.13694892823696136,-0.13129036128520966,-0.11461229622364044,-0.1224948987364769,-0.11318450421094894,0.04254103824496269,-0.0646863654255867,0.028499877080321312,-0.015786850824952126,0.102321095764637,0.0979037657380104,0.015246978960931301,0.027841733768582344,0.08661329001188278,0.16118980944156647,0.07117506116628647,-0.11715012043714523,-0.1156737208366394,0.01160932332277298,0.24005991220474243,-0.1895238310098648,-0.013630851171910763,0.047480929642915726,0.00056507159024477,0.04295817390084267,0.03227456286549568,0.01999841444194317,0.16698132455348969,0.006189191248267889,0.24343310296535492,-0.1678476631641388,0.11030109971761703,0.10798067599534988,-0.06015750393271446,0.09839628636837006,0.024902883917093277,0.03713682293891907,0.009361336007714272,-0.10288756340742111,-0.1296115517616272,-0.015000804327428341,0.12972219288349152,0.07434003055095673,0.11589165031909943,-0.011939536780118942,0.014357048086822033,-0.022427484393119812,-0.10884159058332443,-0.019282303750514984,0.002330953488126397,-0.04571789130568504,0.02202744595706463,-0.06854405254125595,-0.018168486654758453,-0.025155911222100258,0.03829285874962807,0.1389969140291214,0.03363288566470146,-0.09145203232765198,-0.11548588424921036,-0.02293834276497364,0.03803345933556557,0.03515643998980522,-0.05141110718250275,0.020411839708685875,-0.017930516973137856,-0.21620406210422516,-0.12166587263345718,-0.07335419207811356,-0.00880229938775301,0.0894785150885582,0.04694105684757233,-0.3602229356765747,-0.0031168563291430473,0.0053228395991027355,-0.12151367217302322,-0.0434698611497879,-0.047870390117168427,-0.16281192004680634,-0.10936763882637024,-0.26889002323150635,-0.1535879224538803,0.03805762156844139,0.06152858957648277,-0.09265399724245071,-0.06208343058824539,-0.12249885499477386,-0.11693202704191208,0.1122429370880127,-0.03986607491970062,0.20961801707744598,0.09341281652450562,0.18977487087249756,0.09197510778903961,-0.018266869708895683,-0.15473873913288116,-0.013114425353705883,-0.17882829904556274,-0.04006227105855942,0.10253586620092392,0.060080405324697495,0.20894287526607513,0.04405752941966057,-0.11402196437120438,-0.036186426877975464,0.06619475781917572,-0.016905877739191055,0.0028953049331903458,-0.12177105247974396,0.285025954246521,0.008671636693179607,0.15328411757946014,0.03387177363038063,0.007784021086990833,0.043076563626527786,0.09265884757041931,0.16133882105350494,-0.031850021332502365,0.03848591446876526,-0.08274669200181961,0.007205436006188393,-0.02830296941101551,0.07330780476331711,-0.11290998011827469,0.07508831471204758,0.0829785168170929,-0.1834581047296524,0.09025664627552032,0.04928230494260788,0.012372265569865704,0.12036584317684174,-0.12941543757915497,-0.015279763378202915,0.21365487575531006,0.01958191581070423,0.06826908141374588,-0.06396336853504181,-0.06872223317623138,0.023459555581212044,0.035235751420259476,-0.049008533358573914,-0.05822541564702988,-0.15058954060077667,0.038960907608270645,0.08171258866786957,0.04204948619008064,-0.13106238842010498,0.17231783270835876,0.07983110845088959,-0.16035760939121246,-0.03560204058885574,0.07776197791099548,0.10383457690477371,0.034347690641880035,0.1650765985250473,0.05366617813706398,0.054908785969018936,-0.1950230747461319,0.28770577907562256,-0.0659606009721756,-0.02097202092409134,0.0907169058918953,-0.01248857844620943,-0.053050413727760315,0.017494773492217064,-0.050916437059640884,0.04756976664066315,-0.007072948385030031,-0.10269711166620255,0.07001791149377823,0.053686607629060745,-0.2904895544052124,-0.0776834562420845,0.016075272113084793,0.0058264341205358505,0.16073089838027954,0.0868060439825058,-0.05920220911502838,0.012221684679389,-0.3149109482765198,0.04062087461352348,0.0998365730047226,-0.0021148817613720894,0.11914318054914474,0.10202565044164658,0.05161236599087715,-0.19534048438072205,-0.2810855209827423,-0.0325390063226223,0.10299021005630493,0.11943718045949936,-0.0019437925657257438,-0.01858682930469513,-0.026025965809822083,0.06773627549409866,-0.009510335512459278,-0.08522159606218338,-0.20791800320148468,-0.18706443905830383,0.07812054455280304,-0.11198913305997849,-0.01434407476335764,-0.11474119126796722,0.009755213744938374,0.03940911591053009,0.01673899032175541,-0.079794742166996,-0.05462994799017906,-0.13299866020679474,0.07179995626211166,-0.016300344839692116,-0.0655292272567749,0.11230967193841934,-0.0993875190615654,-0.009920690208673477,0.08675672113895416,-0.04369013383984566,-0.09946639835834503,0.09042448550462723,0.054595138877630234,-0.003379793604835868,0.24454352259635925,-0.02644454687833786,-0.10956234484910965,0.00033580028684809804,-0.10672364383935928,-0.09777184575796127,-0.019537363201379776,-0.2207091599702835,-0.07850753515958786,-0.16192981600761414,0.023406291380524635,-0.04089731723070145,-0.06872186064720154,-0.0003243210958316922,0.07253628224134445,-0.16504240036010742,0.13042312860488892,0.04659876227378845,0.10909382253885269,-0.1312359720468521,-0.10779011994600296,0.003505112137645483,-0.09332869201898575,0.030030101537704468,0.14337842166423798,0.1046251431107521,-0.04079278185963631,-0.024025268852710724,-0.01537211611866951,-0.1638256013393402,-0.10390996187925339,-0.03805070370435715,-0.04709610342979431,-0.03955500200390816,0.0004234779335092753,0.16374437510967255,-0.06315729022026062,-0.19272904098033905,0.040833376348018646,-0.029363052919507027,-0.019978610798716545,-0.04115670919418335,0.1053028404712677,0.0325147919356823,0.20853711664676666,-0.1132165864109993,0.002772244857624173,0.028285030275583267,0.04427814856171608,-0.181986004114151,-0.08807316422462463,-0.06593461334705353,-0.012828805483877659,-0.02627420611679554,0.07682469487190247,0.019433442503213882,0.04427945241332054,0.055836763232946396,0.10135023295879364,0.07392309606075287,-0.09660399705171585,0.19164088368415833,0.042060915380716324,0.036318764090538025,-0.08585242927074432,-0.060014527291059494,-0.026201551780104637,0.07078178226947784,-0.08128896355628967,0.26851561665534973,-0.09291081130504608,-0.14098791778087616,-0.04124103859066963,0.15180310606956482,0.15838342905044556,0.040073588490486145,0.017382102087140083,0.09906445443630219,-0.026846665889024734,0.037803031504154205,-0.03194795176386833,-0.00011510247713886201,-0.04483415558934212,0.07944189757108688,0.10437914729118347,0.029482828453183174,0.09393107146024704,-0.13827237486839294,-0.14629001915454865,0.05705595016479492,0.07435166090726852,0.03680158033967018,0.06980575621128082,-0.07275107502937317,0.024700719863176346,0.15029145777225494,-0.25546666979789734,-0.20257686078548431,0.12885133922100067,0.11620749533176422,-0.12138453125953674,-0.0640251636505127,0.04510140419006348,0.021513257175683975,-0.05988631024956703,-0.051024533808231354,-0.1369432508945465,0.21985115110874176,0.1353389024734497,-0.024472741410136223,0.13629202544689178,-0.005520340520888567,-0.07941064238548279,-0.0709863230586052,-0.12199991941452026,0.07147078216075897,0.06029444560408592,-0.06834883242845535,-0.13455145061016083,-0.11439069360494614,0.05459940433502197,0.06841415911912918,-0.30162176489830017,-0.05216412991285324,-0.060710035264492035,-0.07850836962461472,0.18083921074867249,0.17418287694454193,0.0966835618019104,-0.0558517761528492,0.022398533299565315,0.27551376819610596,0.009688557125627995,0.09652624279260635,-0.07409127056598663,-0.031417980790138245,0.0006622799555771053,-0.1699637472629547,0.021363141015172005,-0.03677855059504509,-0.3290371000766754,-0.043871473520994186,0.01702864281833172,-0.1093742698431015,-0.09538997709751129,-0.13554276525974274,-0.17594866454601288,-0.010836736299097538,-0.004014778882265091,0.06623371690511703,0.0833568349480629,-0.025959698483347893,0.006741383112967014,0.033276431262493134,0.03327667713165283,0.03149603679776192,-0.010991869494318962,-0.11232978105545044,-0.17024743556976318,-0.08294419944286346,0.010264567099511623,0.1058296486735344,-0.0648031085729599,-0.054122649133205414,0.006847306154668331,-0.1993407905101776,0.002520214766263962,-0.07734198868274689,-0.17977336049079895,-0.11639123409986496,0.06087138131260872,-0.03712872415781021,0.026222215965390205,0.1139012798666954,0.03789389505982399,-0.21251772344112396,-0.16590982675552368,-0.0749698355793953,-0.05014572665095329,0.07604340463876724,-0.018025100231170654,-0.13036933541297913,0.07980770617723465,0.071446493268013,-0.05323329567909241,0.2303597629070282,-0.11362282931804657,0.12006430327892303,0.051471810787916183,-0.09663987159729004,0.12719932198524475,-0.08340828120708466,-0.17211732268333435,-0.09702958911657333,0.17820000648498535,0.10048102587461472,0.21281267702579498,0.04994054138660431,0.013222342357039452,-0.13861364126205444,0.00915867742151022,-0.10120876133441925,0.17607460916042328,0.020225785672664642,0.03234419599175453,-0.007978345267474651,-0.10218767076730728,0.17785947024822235,-0.1301657259464264,-0.008961284533143044,-0.11734504997730255,-0.06958454102277756,-0.10259830951690674,0.01612050272524357,0.01524773146957159,0.0946447104215622,0.08948618173599243,0.09641778469085693,0.010185400024056435,-0.02002682164311409,0.05257685109972954,-0.039127640426158905,0.07586093246936798,0.0983850434422493,0.1412564069032669,-0.036876045167446136,-0.14210441708564758,-0.1932719200849533,0.02208706922829151,-0.1988779455423355,0.0490461029112339,-0.10257521271705627,-0.04421413689851761,0.05766131356358528,-0.17370720207691193,0.08084996789693832,-0.09549794346094131,0.07015296816825867,-0.16456837952136993,0.01402412261813879,-0.22630281746387482,-0.03390999138355255,-0.013069210574030876,-0.07371696829795837,0.08489350974559784,0.06238599494099617,-0.014283517375588417,0.0798809677362442,0.05036786571145058,0.024204609915614128,-0.04798908904194832,-0.18047337234020233,-0.06843502819538116,-0.11193110048770905,0.1493358165025711,0.03281363844871521,-0.10746009647846222,-0.12164701521396637,-0.12305529415607452,-0.11717720329761505,0.160767063498497,-0.11812075227499008,0.09092901647090912,-0.23307828605175018,0.05379175767302513,0.02675437554717064,-0.09234395623207092,0.00594559358432889,0.07543887943029404,-0.19119203090667725,0.2292235642671585,-0.08619386702775955,0.05476997792720795,0.03185875341296196,-0.043088097125291824,0.06327870488166809,-0.03906092792749405,0.04315371811389923,0.011110170744359493,0.16000984609127045,0.18117496371269226,0.08988288044929504,-0.12264282256364822,0.019642941653728485,-0.1664598286151886,0.1434788554906845,0.025658922269940376,0.08233106136322021,0.025366852059960365,-0.1003904640674591,0.19339559972286224,0.027226977050304413,0.13121859729290009,-0.20598460733890533,-0.042695075273513794,0.07793568819761276,0.042094577103853226,0.08468275517225266,0.07317972928285599,0.04627056047320366,0.184501051902771,-0.11441416293382645,0.02133318968117237,-0.009410740807652473,-0.16411066055297852,0.038802556693553925,0.10297345370054245,0.010321025736629963,-0.052485741674900055,-0.1391635686159134,0.12176027148962021,-0.024995550513267517,-0.1027362197637558,-0.0634075477719307,-0.07510437071323395,-0.1821986734867096,0.02927369624376297,-0.09159896522760391,-0.10865578800439835,-0.09130094945430756,-0.029784290120005608,0.1808781921863556,-0.024012381210923195,-0.017979098483920097,0.1825188845396042,0.006892987061291933,0.018661364912986755,0.03965654596686363,0.13212905824184418,0.05088154599070549,0.023871134966611862,-0.026486486196517944,-0.4044831395149231,-0.05949011072516441,0.0395117923617363,-0.1745414137840271,-0.02290963940322399,0.019817547872662544,0.002728508785367012,-0.04381091520190239,-0.019305983558297157,-0.05630485713481903,0.03089001588523388,-0.14330247044563293,-0.039693690836429596,0.16081944108009338,0.06148739531636238,-0.2661128044128418,-0.03283026069402695,-0.036560893058776855,0.05834357067942619,-0.13867321610450745,0.03559230640530586,-0.01525372825562954,-0.0050948611460626125,0.10389944911003113,-0.023035237565636635,-0.009963663294911385,0.07530810683965683,-0.10762157291173935,0.25134095549583435,0.1740463823080063,0.022361477836966515,-0.09106031805276871,0.07955872267484665,0.019117122516036034,-0.03804856166243553,0.005961011163890362,-0.003963409457355738,-0.023094259202480316,-0.11097154766321182,0.11677329242229462,-0.0097512723878026,-0.09107595682144165,0.006598066072911024,0.014787505380809307,0.13388463854789734],"yaxis":"y","type":"scattergl"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Principal Component 1"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Principal Component 2"}},"legend":{"tracegroupgap":0,"itemsizing":"constant"},"title":{"text":"Word Embeddings Visualization"}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('8498206e-34b4-4bad-80b7-dab2797befdf');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>


</div>
</div>
<p>Words in our articles are closer to each other in this visualization if they are thought to have similar meetings. This scatterplot helps further our understanding of how our model determines what is clasified as fake news. Logically, I believe our plot makes sense as words that I would anticipate to be related are plotted next to each other such as “rule” and “court”. Other words such as “blacklivesmatter”, “cnn”, and “boycotts” were also plotted near one another. In my opinion, these words are correlated as they are all associated with the democratic party. In all, this allowed me to ensure my plot was accurate.</p>
<p>Through this post, I hope you have a better understanding of how to use machine learning in the form of language processors and text classification. As fake news is a growing phenomena in the US, it is very important that we as a country find ways to combat the rampant spread of misinformation. Thank you for reading!</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>